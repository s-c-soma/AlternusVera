{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "PrevWork_Factor_NewsCoverage_ScoreComputing_wk2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-c-soma/AlternusVera/blob/master/PrevWork_Factor_NewsCoverage_ScoreComputing_wk2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTVu0RJDv3Uo",
        "colab_type": "text"
      },
      "source": [
        "# Factor : News Coverage \n",
        "\n",
        "by **Subarna Chowdhury Soma** [SJSU ID: 014549587]\n",
        "\n",
        "> * Project : Alternus Vera Iteration 1\n",
        "> * Team: The Mean Squares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRhGUI3ZM6Dt",
        "colab_type": "text"
      },
      "source": [
        "## Colab Details\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5OMgGngNP2N",
        "colab_type": "text"
      },
      "source": [
        "** Week 1: **\n",
        "\n",
        "> * Colab 1: Factor_NewsCoverage_StoryClustering.ipynb\n",
        "> * Colab 2: Factor_NewsCoverage_ScoreComputing.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm0E3JG_8rbk",
        "colab_type": "code",
        "outputId": "dafd6300-a1a2-4eae-cf57-6b6c3759c84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4-gCwGt8ZO9",
        "colab_type": "text"
      },
      "source": [
        "# Preparation\n",
        "### Imports\n",
        "First some of the required packages must be imported."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7yETHXThH39",
        "colab_type": "text"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2fm9Gtz8ZO-",
        "colab_type": "code",
        "outputId": "5596e948-ac79-4d72-ca0f-968f6251db5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk as nl\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import statistics\n",
        "import random\n",
        "import warnings\n",
        "from string import punctuation\n",
        "from matplotlib import pyplot\n",
        "from pandas import Series, datetime\n",
        "from pandas.plotting import scatter_matrix, autocorrelation_plot\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import nltk\n",
        "import re\n",
        "import io\n",
        "import requests\n",
        "import time\n",
        "import gensim\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk.sentiment\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_7T4u4K8ZPB",
        "colab_type": "text"
      },
      "source": [
        "## All News Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUDgxHySyztT",
        "colab_type": "text"
      },
      "source": [
        "The notebook uses Kaggle's \"All the news\" dataset to curate reference articles to enrich the study of news coverage analysis (Thompson, 2017).\n",
        "\n",
        "The relevant dataset files are downloaded from Kaggle, unzipped and re-uploaded to GitHub repository (Thompson, 2017).\n",
        "\n",
        "https://www.kaggle.com/snapcrack/all-the-news/home"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgt4IjJPy9OJ",
        "colab_type": "text"
      },
      "source": [
        "### Parameter configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpZCy0YD8ZPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#'google','vader','stanford'\n",
        "runParams={'sentiment_library':   ['vader'],\n",
        "           'input_file':          ['/content/drive/My Drive/Colab Notebooks/articles1.csv'],\n",
        "           'output_file':         ['/content/drive/My Drive/Colab Notebooks/fakenews_by_news-coverage.csv'],\n",
        "           'article_id_list':     [[120639,80103,25225,21502,57362,120636]],\n",
        "           'sentiment_sentences': [5],\n",
        "           'article_stats':       [False]}\n",
        "\n",
        "# Use parameter grid even if there is only set of parameters\n",
        "parameterGrid=ParameterGrid(runParams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fagpy0m2uOt",
        "colab_type": "text"
      },
      "source": [
        "## Liar Liar Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anw5Nnvm2yAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_parsed_data(url, sep='\\t', header=None ):\n",
        "  return pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')), sep=sep, header=header )\n",
        "\n",
        "# Download and parse the dataset... Let us first work with 100 articles\n",
        "KAGGLE_DATASET = 'https://github.com/synle/machine-learning-sample-dataset/raw/master/liar_dataset/kaggle'\n",
        "data_kaggle = get_parsed_data('%s/kaggle-fake.csv'% KAGGLE_DATASET, ',' , 'infer' )[:1000]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McTOhh6jfHh9",
        "colab_type": "text"
      },
      "source": [
        "# Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGM-JZBBmuPZ",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization\n",
        "As part of the distillation, we tokenize the headline and the body of the articles, by the following methods. First the words are split based on the white space. We also use this step to combine the title with the body of the article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRq4RVoBm8vc",
        "colab_type": "code",
        "outputId": "985b7d7d-8c32-4c3c-878b-c689456b6408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "data_kaggle['text_distilled'] = data_kaggle['title'].apply(lambda x : re.split('\\W+', str(x).lower())) +\\\n",
        "   data_kaggle['text'].apply(lambda x : re.split('\\W+', str(x).lower()))\n",
        "data_kaggle.head(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>ord_in_thread</th>\n",
              "      <th>author</th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>crawled</th>\n",
              "      <th>site_url</th>\n",
              "      <th>country</th>\n",
              "      <th>domain_rank</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>spam_score</th>\n",
              "      <th>main_img_url</th>\n",
              "      <th>replies_count</th>\n",
              "      <th>participants_count</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>shares</th>\n",
              "      <th>type</th>\n",
              "      <th>text_distilled</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
              "      <td>0</td>\n",
              "      <td>Barracuda Brigade</td>\n",
              "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>Print They should pay all the back all the mon...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[muslims, busted, they, stole, millions, in, g...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       uuid  ...                                     text_distilled\n",
              "0  6a175f46bcd24d39b3e962ad0f29936721db70db  ...  [muslims, busted, they, stole, millions, in, g...\n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbMb4fC2nEvx",
        "colab_type": "code",
        "outputId": "4e11161c-7e3c-49ce-986b-14334dd2f965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "data_kaggle.text_distilled[0][:25]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muslims',\n",
              " 'busted',\n",
              " 'they',\n",
              " 'stole',\n",
              " 'millions',\n",
              " 'in',\n",
              " 'gov',\n",
              " 't',\n",
              " 'benefits',\n",
              " 'print',\n",
              " 'they',\n",
              " 'should',\n",
              " 'pay',\n",
              " 'all',\n",
              " 'the',\n",
              " 'back',\n",
              " 'all',\n",
              " 'the',\n",
              " 'money',\n",
              " 'plus',\n",
              " 'interest',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'family',\n",
              " 'and']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnSyhKgBn9_y",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization\n",
        "First, the raw words must be converted to root forms. The words are converted to their root stems based on the following using lemmaitization. Stemming using nltk.PorterStemmer was used previously, but eventually dropped due to its method of dropping suffixes without correction to the root stem. For example, stemming resulted in non-existent words such as 'plu', 'interest', 'entir', 'famili', 'everyon'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SRbRvYboFQK",
        "colab_type": "code",
        "outputId": "0a9c7137-3541-42df-eb63-0754ae6a5b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "def lemmatize(tokenized_words):\n",
        "  text = [nltk.WordNetLemmatizer().lemmatize(word) for word in tokenized_words]\n",
        "  return text\n",
        "\n",
        "# Commented out per rationale above\n",
        "#def stemming(tokenized_words):\n",
        "#  text = [nltk.PorterStemmer().stem(word) for word in tokenized_words]\n",
        "#  return text\n",
        "\n",
        "data_kaggle['text_distilled_lemma'] = data_kaggle['text_distilled'].apply(lemmatize)\n",
        "data_kaggle.text_distilled[0][:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muslims',\n",
              " 'busted',\n",
              " 'they',\n",
              " 'stole',\n",
              " 'millions',\n",
              " 'in',\n",
              " 'gov',\n",
              " 't',\n",
              " 'benefits',\n",
              " 'print',\n",
              " 'they',\n",
              " 'should',\n",
              " 'pay',\n",
              " 'all',\n",
              " 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjxR8qYZoMBC",
        "colab_type": "text"
      },
      "source": [
        "## Removing Stop words\n",
        "Let us remove stop words, which are absolutely critical in determining keywords indicative of timed event."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLdE6U7_oQLS",
        "colab_type": "code",
        "outputId": "112adb92-731d-4189-e725-e2a91ad1c776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "english_stopwords = set(stopwords.words('english') + list(punctuation) + [''])\n",
        "\n",
        "def remove_stopwords(tokenized_words):\n",
        "  text = [word for word in tokenized_words if word not in english_stopwords]\n",
        "  return text\n",
        "\n",
        "data_kaggle['text_distilled'] = data_kaggle['text_distilled'].apply(remove_stopwords)\n",
        "data_kaggle.text_distilled[0][:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muslims',\n",
              " 'busted',\n",
              " 'stole',\n",
              " 'millions',\n",
              " 'gov',\n",
              " 'benefits',\n",
              " 'print',\n",
              " 'pay',\n",
              " 'back',\n",
              " 'money',\n",
              " 'plus',\n",
              " 'interest',\n",
              " 'entire',\n",
              " 'family',\n",
              " 'everyone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlPoxLMWpFGo",
        "colab_type": "text"
      },
      "source": [
        "## Custom Filtering\n",
        "One or two-letter words from the tokenized words are also removed to further cleanse the raw text. First, two-letter words are chosen one by one (there are roughly 100 to 150 words, plus some common country codes). Each word, is examined whether it is worth keeping. Of course, there are more two-letter words than the whitelist shown below, but many of them (e.g. so, am, it) are already eliminated using stop words. If we build a dictionary of all these non-trivial words that are two-letters long, this will help improve the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gfttIyRpJYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "whitelist = set(['ai', 'ax', 'ca', 'eu', 'go', 'io', 'la', 'ox', 'us', 'uk', \n",
        "                 'al', 'ak', 'az', 'ar', 'ca', 'co', 'ct', 'de', 'fl', 'ga', 'hi', \n",
        "                 'id', 'il', 'in', 'ia', 'ks', 'ky', 'la', 'me', 'md', 'ma', 'mi',\n",
        "                 'mn', 'ms', 'mo', 'mt', 'ne', 'nv', 'nh', 'nj', 'nm', 'ny',\n",
        "                 'nc', 'nd', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn',\n",
        "                 'tx', 'ut', 'vt', 'va', 'wa', 'wv', 'wi', 'wy' ])\n",
        "def remove_too_short(tokenized_words):\n",
        "  text = [word for word in tokenized_words if (len(word) >= 3 or word not in whitelist) ]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzeD6oz3pXzD",
        "colab_type": "code",
        "outputId": "c5c65374-8887-47d5-b6c5-2f5a5e9609f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "data_kaggle['text_distilled'] = data_kaggle['text_distilled'].apply(remove_too_short)\n",
        "data_kaggle.text_distilled[0][:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muslims',\n",
              " 'busted',\n",
              " 'stole',\n",
              " 'millions',\n",
              " 'gov',\n",
              " 'benefits',\n",
              " 'print',\n",
              " 'pay',\n",
              " 'back',\n",
              " 'money',\n",
              " 'plus',\n",
              " 'interest',\n",
              " 'entire',\n",
              " 'family',\n",
              " 'everyone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9BVRVgas5EB",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKW4hy1ouaBE",
        "colab_type": "text"
      },
      "source": [
        "## LDA Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjNnJaY1ueOJ",
        "colab_type": "text"
      },
      "source": [
        "We will use topic modelling (LDA) to identify the key topics, https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_-cEQ2Wuiu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "def topics(tokenized_words):\n",
        "    d = Dictionary([tokenized_words])\n",
        "    c = [d.doc2bow(tokenized_words)]\n",
        "    m = LdaModel(c, num_topics=1, id2word=d)\n",
        "    return list(m.print_topics(num_words=2))\n",
        "  \n",
        "data_kaggle['topics'] = data_kaggle['text_distilled'].apply(topics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvDSwCz1uo8F",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing the topics of some sample articles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1obVsBg9uvzM",
        "colab_type": "code",
        "outputId": "7de4cae3-cbb2-41a0-d0c6-00226c2a638e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "data_kaggle.title[1],data_kaggle.topics[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Re: Why Did Attorney General Loretta Lynch Plead The Fifth?',\n",
              " [(0, '0.033*\"lynch\" + 0.020*\"attorney\"')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vBSTgaOu0zx",
        "colab_type": "code",
        "outputId": "14106ae3-50e8-4092-fd64-9afe6dabe80a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "data_kaggle.title[2],data_kaggle.topics[3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('BREAKING: Weiner Cooperating With FBI On Hillary Email Investigation',\n",
              " [(0, '0.049*\"speech\" + 0.037*\"donald\"')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZeKC7y8u4iF",
        "colab_type": "code",
        "outputId": "a2c23a44-0e43-4a83-be0e-c7109efb59ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "data_kaggle.title[3],data_kaggle.topics[3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnapped And Killed By ISIS: \"I have voted for Donald J. Trump!\" » 100percentfedUp.com',\n",
              " [(0, '0.049*\"speech\" + 0.037*\"donald\"')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYCcIndNu8On",
        "colab_type": "code",
        "outputId": "9f2c686c-11da-4524-ef2a-556011dc41e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "data_kaggle.title[4],data_kaggle.topics[4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Healthcare Begins With A Bombshell! » 100percentfedUp.com\",\n",
              " [(0, '0.022*\"insurance\" + 0.016*\"must\"')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgf6awLjvCR4",
        "colab_type": "code",
        "outputId": "e162f16e-b891-467b-de4c-8c258a418fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data_kaggle.title)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97DoHXDpveGv",
        "colab_type": "text"
      },
      "source": [
        "## Topics as Simple List of Words\n",
        "A list of topic terms is compiled as show below. The coefficients in front of each word are dropped as part of simplification. The assumption is that the top two words comprising the topic, are both significant enough to be treated equally. It is important that the goal is to build a reliable prediction model. While there is a risk of oversimplification, if the final model results in a poor accuracy score, the coefficient can always be reintroduced here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thUw5uOnvwBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseTopics(topics):\n",
        "   output = []\n",
        "   words = topics[0][1].split( '+' )\n",
        "   for word in words:\n",
        "       output.append( word.split('*')[1].replace( '\"', '' ) )\n",
        "   return output\n",
        "\n",
        "data_kaggle['topics'] = data_kaggle['topics'].apply(parseTopics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni90pUDGv0CR",
        "colab_type": "code",
        "outputId": "853e857e-930c-4fb1-9ab5-22d10e6ed0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "data_kaggle.title[1],data_kaggle.topics[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Re: Why Did Attorney General Loretta Lynch Plead The Fifth?',\n",
              " ['lynch ', 'attorney'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op_VZqjg6wAk",
        "colab_type": "text"
      },
      "source": [
        "# Distillation (All News Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cUvG5yCym6g",
        "colab_type": "text"
      },
      "source": [
        "## Data Enrichment  (All News Data)\n",
        "The notebook uses Kaggle's \"All the news\" dataset to compute coverage score for news coverage\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xxVUGOH32rAT"
      },
      "source": [
        "## Cleaning Articles\n",
        "\n",
        "This is the same function as used in Part One."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QFuF9wfY2rAs",
        "colab": {}
      },
      "source": [
        "def getInputDataAndDisplayStats(filename,processDate,printSummary=False):\n",
        "\n",
        "\tdf=pd.read_csv(filename)\n",
        "\n",
        "\tdf=df.drop_duplicates('content')\n",
        "\tdf=df[~df['content'].isnull()]\n",
        "\n",
        "\t\n",
        "\tdf=df[df['content'].str.len()>=200]\n",
        "\n",
        "\t# Find and remove summary NYT \"briefing\" articles to avoid confusing the clustering\n",
        "\ttargetString=\"(Want to get this briefing by email?\"\n",
        "\tdf['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
        "\tdf=df[df['NYT summary']==False]\n",
        "\n",
        "\t# The following removes a warning that appears in many of the Atlantic articles.\n",
        "\t# Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
        "\t# And subsequently to the assessment of sentiment\n",
        "\ttargetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
        "\tdf['content']=df['content'].str.replace(targetString,'')\n",
        "\n",
        "\t# This is also for some Atlantic articles for the same reasons as above\n",
        "\ttargetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
        "\tdf=df[df['content'].str.contains(targetString)==False]\n",
        "\n",
        "\t# This is also for some Atlantic articles for the same reasons as above\n",
        "\ttargetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
        "\tdf=df[df['content'].str.contains(targetString)==False]\n",
        "\n",
        "\t# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
        "\tdf=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
        "\n",
        "\t# Remove daily CNN summary\n",
        "\ttargetString=\"CNN Student News\"\n",
        "\tdf=df[df['content'].str.contains(targetString)==False]\n",
        "\n",
        "\tif printSummary:\n",
        "\t\tprint(\"\\nArticle counts by publisher:\")\n",
        "\t\tprint(df['publication'].value_counts())\n",
        "\n",
        "\t\tprint(\"\\nArticle counts by date:\")\n",
        "\t\tprint(df['date'].value_counts())\n",
        "\t\t\n",
        "\n",
        "\tif processDate!=None:\n",
        "\t\tdf=df[df['date']==processDate]\n",
        "\tdf.reset_index(inplace=True, drop=True)\n",
        "\n",
        "\t# Remove non-ASCII characters\n",
        "\tdf['processed_content']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
        "\tdf['score'] = 0.0\n",
        "\n",
        "\tprint(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
        "\tprint(df['publication'].value_counts())\n",
        "\n",
        "\treturn df\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "def removeNonASCIICharacters(textString): \n",
        "    return \"\".join(i for i in textString if ord(i)<128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FLYZ6S012rAy"
      },
      "source": [
        "## First level of cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b03393bd-ccc2-4cb3-cb70-5cac696ec3ec",
        "id": "S5UeFvhI2rAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "articleDataFrame=getInputDataAndDisplayStats(runParams['input_file'][0],None,False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Final dataset:\n",
            "\n",
            "Date: None \n",
            "\n",
            "Breitbart           23585\n",
            "CNN                 11249\n",
            "New York Times       7620\n",
            "Business Insider     6504\n",
            "Atlantic              157\n",
            "Name: publication, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3420161b-eaec-436c-d512-2dd1bd3d52a4",
        "id": "tW5jhWpa2rA2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "articleDataFrame.date.unique"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Series.unique of 0        2016-12-31\n",
              "1        2017-06-19\n",
              "2        2017-01-06\n",
              "3        2017-04-10\n",
              "4        2017-01-02\n",
              "            ...    \n",
              "49110    2017-01-11\n",
              "49111    2017-01-11\n",
              "49112    2017-01-11\n",
              "49113    2017-01-11\n",
              "49114    2017-01-11\n",
              "Name: date, Length: 49115, dtype: object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7f305522-9e79-43ab-f647-45f6be742bdb",
        "id": "4svhxazp2rA5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "articleDataFrame.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49115, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2CBPNbVE2rA8",
        "colab": {}
      },
      "source": [
        "col_id_list = articleDataFrame['id'].tolist()\n",
        "#col_id_list "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kqHszEtsZnr",
        "colab_type": "code",
        "outputId": "f4dbca41-f1ae-4acd-8df3-a96d78353dbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# All News Kaggle\n",
        "\n",
        "def get_parsed_data2(url):\n",
        "    return pd.read_csv(io.StringIO(requests.get(url, verify=False).content.decode('utf-8')), sep=',', header='infer')\n",
        "\n",
        "# download and parse the dataset...\n",
        "articleDataFrame = get_parsed_data2('https://media.githubusercontent.com/media/hyunwookshin/all_news_dataset_kaggle/master/articles1.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiqfziP-Twd1",
        "colab_type": "code",
        "outputId": "0ba7551f-ce82-4232-b968-6d74355293a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "#articleDataFrame = pd.read_csv('/content/drive/My Drive/Colab Notebooks/articles1.csv', low_memory =False, encoding = \"ISO-8859-1\")\n",
        "articleDataFrame.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>publication</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17283</td>\n",
              "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Carl Hulse</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>17284</td>\n",
              "      <td>Rift Between Officers and Residents as Killing...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Benjamin Mueller and Al Baker</td>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>After the bullet shells get counted, the blood...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17285</td>\n",
              "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Margalit Fox</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17286</td>\n",
              "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>William McDonald</td>\n",
              "      <td>2017-04-10</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17287</td>\n",
              "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Choe Sang-Hun</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>17288</td>\n",
              "      <td>Sick With a Cold, Queen Elizabeth Misses New Y...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Sewell Chan</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LONDON  —   Queen Elizabeth II, who has been b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>17289</td>\n",
              "      <td>Taiwan’s President Accuses China of Renewed In...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Javier C. Hernández</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>BEIJING  —   President Tsai   of Taiwan sharpl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>17290</td>\n",
              "      <td>After ‘The Biggest Loser,’ Their Bodies Fought...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Gina Kolata</td>\n",
              "      <td>2017-02-08</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Danny Cahill stood, slightly dazed, in a blizz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>17291</td>\n",
              "      <td>First, a Mixtape. Then a Romance. - The New Yo...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Katherine Rosman</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just how   is Hillary Kerr, the    founder of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>17292</td>\n",
              "      <td>Calling on Angels While Enduring the Trials of...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Andy Newman</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Angels are everywhere in the Muñiz family’s ap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>17293</td>\n",
              "      <td>Weak Federal Powers Could Limit Trump’s Climat...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Justin Gillis</td>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>With Donald J. Trump about to take control of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>17294</td>\n",
              "      <td>Can Carbon Capture Technology Prosper Under Tr...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>John Schwartz</td>\n",
              "      <td>2017-01-05</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>THOMPSONS, Tex.  —   Can one of the most promi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>17295</td>\n",
              "      <td>Mar-a-Lago, the Future Winter White House and ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Maggie Haberman</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WEST PALM BEACH, Fla.  —   When   Donald J. Tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>17296</td>\n",
              "      <td>How to form healthy habits in your 20s - The N...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Charles Duhigg</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This article is part of a series aimed at help...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>17297</td>\n",
              "      <td>Turning Your Vacation Photos Into Works of Art...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Stephanie Rosenbloom</td>\n",
              "      <td>2017-04-14</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It’s the season for family travel and photos  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>17298</td>\n",
              "      <td>As Second Avenue Subway Opens, a Train Delay E...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Emma G. Fitzsimmons</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>17300</td>\n",
              "      <td>Dylann Roof Himself Rejects Best Defense Again...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Kevin Sack and Alan Blinder</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>pages into the   journal found in Dylann S. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>17301</td>\n",
              "      <td>Modi’s Cash Ban Brings Pain, but Corruption-We...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Geeta Anand</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MUMBAI, India  —   It was a bold and risky gam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>17302</td>\n",
              "      <td>Suicide Bombing in Baghdad Kills at Least 36 -...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>The Associated Press</td>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>BAGHDAD  —   A suicide bomber detonated a pick...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>17303</td>\n",
              "      <td>Fecal Pollution Taints Water at Melbourne’s Be...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Brett Cole</td>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SYDNEY, Australia  —   The annual beach pilgri...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0     id  ... url                                            content\n",
              "0            0  17283  ... NaN  WASHINGTON  —   Congressional Republicans have...\n",
              "1            1  17284  ... NaN  After the bullet shells get counted, the blood...\n",
              "2            2  17285  ... NaN  When Walt Disney’s “Bambi” opened in 1942, cri...\n",
              "3            3  17286  ... NaN  Death may be the great equalizer, but it isn’t...\n",
              "4            4  17287  ... NaN  SEOUL, South Korea  —   North Korea’s leader, ...\n",
              "5            5  17288  ... NaN  LONDON  —   Queen Elizabeth II, who has been b...\n",
              "6            6  17289  ... NaN  BEIJING  —   President Tsai   of Taiwan sharpl...\n",
              "7            7  17290  ... NaN  Danny Cahill stood, slightly dazed, in a blizz...\n",
              "8            8  17291  ... NaN  Just how   is Hillary Kerr, the    founder of ...\n",
              "9            9  17292  ... NaN  Angels are everywhere in the Muñiz family’s ap...\n",
              "10          10  17293  ... NaN  With Donald J. Trump about to take control of ...\n",
              "11          11  17294  ... NaN  THOMPSONS, Tex.  —   Can one of the most promi...\n",
              "12          12  17295  ... NaN  WEST PALM BEACH, Fla.  —   When   Donald J. Tr...\n",
              "13          13  17296  ... NaN  This article is part of a series aimed at help...\n",
              "14          14  17297  ... NaN  It’s the season for family travel and photos  ...\n",
              "15          15  17298  ... NaN  Finally. The Second Avenue subway opened in Ne...\n",
              "16          16  17300  ... NaN    pages into the   journal found in Dylann S. ...\n",
              "17          17  17301  ... NaN  MUMBAI, India  —   It was a bold and risky gam...\n",
              "18          18  17302  ... NaN  BAGHDAD  —   A suicide bomber detonated a pick...\n",
              "19          19  17303  ... NaN  SYDNEY, Australia  —   The annual beach pilgri...\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbdvhkic48JT",
        "colab_type": "text"
      },
      "source": [
        "## Distillation - Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK8riDAP5EcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articleDataFrame['text_distilled'] = articleDataFrame['title'].apply(lambda x : re.split('\\W+', str(x).lower())) +\\\n",
        "   articleDataFrame['content'].apply(lambda x : re.split('\\W+', str(x).lower()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-c2U4Fz5dHk",
        "colab_type": "text"
      },
      "source": [
        "## Distillation - Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn7ZrWn05g4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articleDataFrame['text_distilled'] = articleDataFrame['text_distilled'].apply(remove_stopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6UmHvWb5rTG",
        "colab_type": "text"
      },
      "source": [
        "## Distillation - Additional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABJ3hgyK5ucS",
        "colab_type": "code",
        "outputId": "14bb22e0-fbd8-4379-8770-b140c19e65c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "articleDataFrame['text_distilled'] = articleDataFrame['text_distilled'].apply(remove_too_short)\n",
        "articleDataFrame.text_distilled[0][:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['house',\n",
              " 'republicans',\n",
              " 'fret',\n",
              " 'winning',\n",
              " 'health',\n",
              " 'care',\n",
              " 'suit',\n",
              " 'new',\n",
              " 'york',\n",
              " 'times',\n",
              " 'washington',\n",
              " 'congressional',\n",
              " 'republicans',\n",
              " 'new',\n",
              " 'fear']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMiSCL06DUI",
        "colab_type": "text"
      },
      "source": [
        "## Distillation - Topic Analysis - Running LDA to Extract Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rztdxZhZ6Iao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articleDataFrame['topics'] = articleDataFrame['text_distilled'].apply(topics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDquIHTS6MvS",
        "colab_type": "text"
      },
      "source": [
        "Making sure that the distillation is succesful, the following snippet was run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITph89Z86QBV",
        "colab_type": "code",
        "outputId": "081de547-98b5-4442-a956-f3c31865a96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "articleDataFrame.title[0], articleDataFrame.topics[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('House Republicans Fret About Winning Their Health Care Suit - The New York Times',\n",
              " [(0, '0.024*\"house\" + 0.020*\"republicans\"')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iXg11bS22rA-"
      },
      "source": [
        "## Take All Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3329ef68-59af-43b7-fcc9-e97462b6cddc",
        "id": "hCF-9Y4r2rA-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "articleDataFrame['id'].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zcr0Qoqu2rBA"
      },
      "source": [
        "Comment this line if not want to take all the IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rHZjLnz82rBB",
        "colab": {}
      },
      "source": [
        "runParams['article_id_list'][0] = col_id_list "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK_cNjs466AO",
        "colab_type": "text"
      },
      "source": [
        "# News Coverage Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUsqzry97UiL",
        "colab_type": "text"
      },
      "source": [
        "## Scoring Coverage (Ranking)\n",
        "The latent topics are parsed from non-fake news datset. To make an appropriate scoring coverage, the latent topics between kaggle \"All News Data\" set and Liar Liar dataset (data_kaggle) are compared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbU7d6sC73pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setDate( published ):\n",
        "   return published.split( \"T\" )[0]\n",
        "\n",
        "data_kaggle[ 'date' ] = data_kaggle.published.apply( setDate )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T080kmp77ON",
        "colab_type": "code",
        "outputId": "32ce7572-0379-4716-ee1d-1245c4e8355d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "data_kaggle.head(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>ord_in_thread</th>\n",
              "      <th>author</th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>crawled</th>\n",
              "      <th>site_url</th>\n",
              "      <th>country</th>\n",
              "      <th>domain_rank</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>spam_score</th>\n",
              "      <th>main_img_url</th>\n",
              "      <th>replies_count</th>\n",
              "      <th>participants_count</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>shares</th>\n",
              "      <th>type</th>\n",
              "      <th>text_distilled</th>\n",
              "      <th>text_distilled_lemma</th>\n",
              "      <th>topics</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
              "      <td>0</td>\n",
              "      <td>Barracuda Brigade</td>\n",
              "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>Print They should pay all the back all the mon...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[muslims, busted, stole, millions, gov, benefi...</td>\n",
              "      <td>[muslim, busted, they, stole, million, in, gov...</td>\n",
              "      <td>[benefits , government]</td>\n",
              "      <td>2016-10-26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       uuid  ...        date\n",
              "0  6a175f46bcd24d39b3e962ad0f29936721db70db  ...  2016-10-26\n",
              "\n",
              "[1 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl29JTGnKYDX",
        "colab_type": "text"
      },
      "source": [
        "The coverage window is simply a time-range where two articles are considered to be pushed in the same approximate \"time frame\". Narrowing the window size will result in low coverage score across all rows. Increasing the window size will result in high coverage score across all rows. The choice of the window size was arbitrary (30 days). Please see section 7.17 for 60-day coverage window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNSCqMM6KcH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "def window( datestring, n ):\n",
        "   d = datetime.datetime.strptime( datestring, \"%Y-%m-%d\" )\n",
        "   delta = datetime.timedelta(days=n)\n",
        "   fromdate = datetime.datetime.strftime(d - delta, \"%Y-%m-%d\")\n",
        "   todate = datetime.datetime.strftime(d + delta, \"%Y-%m-%d\")\n",
        "   return ( fromdate, todate )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX7J_ZYOKg4g",
        "colab_type": "text"
      },
      "source": [
        "Please note that coverage scoring below can take about 10~15 minutes to complete (With GPU Hardware accelerator)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBK1O1HVKml5",
        "colab_type": "code",
        "outputId": "76889e8c-f032-458d-d3dd-f80a81292c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import datetime\n",
        "\n",
        "print( \"Start Time\", datetime.datetime.now() )\n",
        "\n",
        "def coverage( article ):\n",
        "   fromdate, todate = window( article[ 'date' ], 15 )\n",
        "   selected_coverage = articleDataFrame[(articleDataFrame['date'] > fromdate) & (articleDataFrame['date'] < todate)]\n",
        "   selected_coverage['covered'] = selected_coverage.apply( lambda r: r[ 'topics' ][0] in article.topics and\n",
        "                                                       r[ 'topics' ][1] in article.topics, axis=1 )\n",
        "   return len(selected_coverage[selected_coverage['covered'] == True])\n",
        "\n",
        "data_kaggle[ 'coverage' ] = data_kaggle.apply( coverage, axis=1 )\n",
        "\n",
        "print( \"Finished Time\", datetime.datetime.now() )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Time 2020-04-26 09:07:20.827519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Time 2020-04-26 09:09:15.321094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYFAgrnvRI5R",
        "colab_type": "code",
        "outputId": "b0c50d1c-fe01-476d-b2c6-dba8e9fffa98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "data_kaggle.sort_values(by=['coverage'], ascending=False ).head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>ord_in_thread</th>\n",
              "      <th>author</th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>crawled</th>\n",
              "      <th>site_url</th>\n",
              "      <th>country</th>\n",
              "      <th>domain_rank</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>spam_score</th>\n",
              "      <th>main_img_url</th>\n",
              "      <th>replies_count</th>\n",
              "      <th>participants_count</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>shares</th>\n",
              "      <th>type</th>\n",
              "      <th>text_distilled</th>\n",
              "      <th>text_distilled_lemma</th>\n",
              "      <th>topics</th>\n",
              "      <th>date</th>\n",
              "      <th>coverage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
              "      <td>0</td>\n",
              "      <td>Barracuda Brigade</td>\n",
              "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>Print They should pay all the back all the mon...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[muslims, busted, stole, millions, gov, benefi...</td>\n",
              "      <td>[muslim, busted, they, stole, million, in, gov...</td>\n",
              "      <td>[benefits , government]</td>\n",
              "      <td>2016-10-26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>671</th>\n",
              "      <td>9cf9163fcb8564e3e3e51f13b7dc78a1eeb60445</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-03T21:04:03.190+02:00</td>\n",
              "      <td>Protesters, Police Still Clashing Over Dispute...</td>\n",
              "      <td>Protesters, Police Still Clashing Over Dispute...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-03T21:04:03.190+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Protesters, Police Still Clashing Over Dispute...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[protesters, police, still, clashing, disputed...</td>\n",
              "      <td>[protester, police, still, clashing, over, dis...</td>\n",
              "      <td>[protesters , pipeline]</td>\n",
              "      <td>2016-11-03</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>e5c051173c7efd9a46dc2449146c544e20f931ba</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-02T19:14:58.593+02:00</td>\n",
              "      <td>Bundy brothers discharged from Oregon jail, he...</td>\n",
              "      <td>Bundy brothers discharged from Oregon jail, he...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-02T19:14:58.593+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bundy brothers discharged from Oregon jail, he...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[bundy, brothers, discharged, oregon, jail, he...</td>\n",
              "      <td>[bundy, brother, discharged, from, oregon, jai...</td>\n",
              "      <td>[federal , bundy]</td>\n",
              "      <td>2016-11-02</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>0c9d01b2bc7e6e5be2ebecd52b54468b5bf1e731</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-02T19:24:00.901+02:00</td>\n",
              "      <td>Warning or threat? Hillary hints at second civ...</td>\n",
              "      <td>Warning or threat? Hillary hints at second civ...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-02T19:24:00.901+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Warning or threat? Hillary hints at second civ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[warning, threat, hillary, hints, second, civi...</td>\n",
              "      <td>[warning, or, threat, hillary, hint, at, secon...</td>\n",
              "      <td>[clinton , war]</td>\n",
              "      <td>2016-11-02</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>705bbad66afa88a8b0c0c8854d107be3a4369bad</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-02T19:24:01.278+02:00</td>\n",
              "      <td>U.S. militia girds for trouble as presidential...</td>\n",
              "      <td>U.S. militia girds for trouble as presidential...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-02T19:24:01.278+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>U.S. militia girds for trouble as presidential...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[u, militia, girds, trouble, presidential, ele...</td>\n",
              "      <td>[u, s, militia, girds, for, trouble, a, presid...</td>\n",
              "      <td>[members , said]</td>\n",
              "      <td>2016-11-02</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         uuid  ...  coverage\n",
              "0    6a175f46bcd24d39b3e962ad0f29936721db70db  ...         0\n",
              "671  9cf9163fcb8564e3e3e51f13b7dc78a1eeb60445  ...         0\n",
              "658  e5c051173c7efd9a46dc2449146c544e20f931ba  ...         0\n",
              "659  0c9d01b2bc7e6e5be2ebecd52b54468b5bf1e731  ...         0\n",
              "660  705bbad66afa88a8b0c0c8854d107be3a4369bad  ...         0\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yzct9k_RN2b",
        "colab_type": "code",
        "outputId": "d9ebf307-c497-4c56-b3e5-c1b5fb74227e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "data_kaggle.sort_values(by=['coverage'], ascending=False ).head(1)['title'], data_kaggle.sort_values(by=['coverage'], ascending=False ).head(1)['coverage']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0    Muslims BUSTED: They Stole Millions In Gov’t B...\n",
              " Name: title, dtype: object, 0    0\n",
              " Name: coverage, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCg4yLp1WnkX",
        "colab_type": "text"
      },
      "source": [
        "## Intermediary Analysis\n",
        "Label Pairing - For now, top two words of the topics are chosen in the all-news dataset, as \"labels.\" Then using the topics of each fake news datasets and number all-news articles corresponding to each fake news article can be deduced. This means that for every article, we are searching for related articles.\n",
        "\n",
        "Hypothetical Label Pairing for Illustration Purposes\n",
        "\n",
        "Table A (10 articles)\n",
        "\n",
        "| Articles | Topic Words | Computed Coverage Score | Rationale | | -- | -- | -- | | 1 | \"Apple\", \"Banana\" | 0 | Matches None from B | | 2 | \"Apple\", \"Orange\" | 1 | Matches B.2 | | 3 | \"Orange\", \"Pear\" | 2 | Matches B.3 and B.1023 | | ... | ... |\n",
        "\n",
        "Table B (10,000 articles)\n",
        "\n",
        "Articles\tTopic Words\n",
        "1\t\"Apple\", \"Pear\"\n",
        "2\t\"Apple\", \"Orange\"\n",
        "3\t\"Orange\", \"Pear\"\n",
        "...\t...\n",
        "1023\t\"Orange\", \"Pear\"\n",
        "...\t...\n",
        "Building a Model This \"search\" part should only be done during training stage, but not during testing and validation stages. But manually searching each row in the database is expensive and unscalable. Also, if the model simply looks up in internal database and return the score, it is more or less a search solution, not a Machine Learning Classification solution.\n",
        "\n",
        "Instead of doing this search for every article, the model should predict how much coverage it is likely to have based on the top two topic terms and the date of publication with classifiers such as Naive Bayes or Decision Trees. Clearly it will require lot of articles for training since there will be a rich set of topic terms and dates resulting in wide range of coverage scores.\n",
        "\n",
        "Risks There is some risk involved with this approach. Sometimes LDA and topic modeliing doesn't get us far enough. Sometimes the keywords generated by LDA such as \"Car\", \"Bus\", \"Train\" all have the latent super-topic called \"transporation\". So in our distillation not only we should filter out stop words and run lemmatization, we should condense the words even further such as \"transportation\". This will require additional data enrichment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4HP60QxW5al",
        "colab_type": "text"
      },
      "source": [
        "## Using word2vec to Convert Topics to Vector Embeddings\n",
        "We use word2vec to fit the entire text from all articles, and then use word2vec model to convert the topic words to vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDXenIEiW-yU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "m = Word2Vec( data_kaggle[ 'text_distilled' ] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDTFFrjpXCgR",
        "colab_type": "code",
        "outputId": "7e7d88ab-1156-4d12-a78d-1706f68213b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "m.similarity( 'clinton', 'hillary' )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9209413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mmKBP9RXJnm",
        "colab_type": "code",
        "outputId": "f8f38efc-e4be-4ce6-b4e2-89673210cc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "def encodeFirstColumn( topics ):\n",
        "   topic = topics[0].strip()\n",
        "   if topic in m:\n",
        "      return m[topic].tolist()\n",
        "   else:\n",
        "      return np.zeros( len(m[list(m.wv.vocab)[0]]) ).tolist()\n",
        "   \n",
        "data_kaggle[ 'topic_0' ] = data_kaggle.topics.apply( encodeFirstColumn )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtsaqF5lXOdW",
        "colab_type": "text"
      },
      "source": [
        "Running few sanity check, to ensure that the computation is correct,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9poudWjGXRDD",
        "colab_type": "code",
        "outputId": "4a951e73-188f-4e66-86ef-be53c5978ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "'clinton' in m"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RCasiiaXUec",
        "colab_type": "code",
        "outputId": "2210d85d-b011-4ca4-ceb5-1b9cc6e78225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "m['hillary']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.34076   , -0.20688492,  1.0625281 ,  0.7956244 ,  0.6254588 ,\n",
              "       -1.4718738 ,  0.6638465 , -0.4085298 , -0.14611132, -0.29025188,\n",
              "        1.0420674 , -0.05149086,  0.33711302, -0.52381223,  0.59307176,\n",
              "        0.2833357 ,  1.1587983 ,  0.5906302 ,  0.44646496, -0.33545342,\n",
              "        0.4028986 , -0.0802969 ,  0.3809293 ,  0.53883183, -0.20949528,\n",
              "        0.45326144, -0.37720925,  0.48270765,  0.10092717,  0.55493456,\n",
              "       -0.79604787,  0.47106475, -0.13677798,  1.1848029 ,  1.2807202 ,\n",
              "       -0.08843065,  1.2659123 , -0.08012204, -0.58518124,  0.69252765,\n",
              "        0.05551146,  0.8819863 ,  0.15653998, -0.21041565,  0.10060433,\n",
              "        0.7770395 ,  0.77330095, -0.42031643,  0.00400971,  1.474245  ,\n",
              "       -0.6886547 ,  0.29213583,  1.5507023 , -0.67260754, -0.29448014,\n",
              "        1.0597839 ,  0.8333858 , -0.41837195,  0.43003085,  0.05117146,\n",
              "        0.7476804 , -0.34058443,  0.09018333,  0.262423  ,  0.5597528 ,\n",
              "        0.31005138,  0.74758077,  0.01895826, -0.05549014, -0.15200397,\n",
              "        0.7489617 , -0.4239628 ,  0.29747048,  0.11105616, -0.8192101 ,\n",
              "       -0.1814092 ,  0.5865597 ,  0.2975423 ,  0.68977076, -0.25989512,\n",
              "        0.33142486, -0.41692522, -0.8323384 , -0.47403786,  0.42680448,\n",
              "        0.01955375,  0.15312217, -0.756529  ,  0.66974866, -0.7452676 ,\n",
              "        0.23215102,  0.38793713,  0.16483732,  0.1295887 , -0.292823  ,\n",
              "        0.68788   , -0.05811111,  0.18433277, -0.5259255 , -0.64312035],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8__LfiJXYE6",
        "colab_type": "code",
        "outputId": "ccbc9e0c-8b60-4d68-e712-65fbd43e2b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "data_kaggle[ 'topic_0' ][:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [0.13314050436019897, 0.02276035025715828, 0.0...\n",
              "1    [0.15719249844551086, 0.029060665518045425, 0....\n",
              "2    [0.21705643832683563, 0.04466809704899788, 0.0...\n",
              "3    [0.2601422071456909, 0.05656076967716217, 0.02...\n",
              "4    [0.16159288585186005, 0.04125561937689781, -0....\n",
              "5    [-0.03145146742463112, 0.03744429722428322, 0....\n",
              "6    [0.13586699962615967, 0.030349062755703926, 0....\n",
              "7    [0.31141406297683716, 0.07345306873321533, 0.0...\n",
              "8    [-0.03145146742463112, 0.03744429722428322, 0....\n",
              "9    [0.30505669116973877, 0.17706751823425293, 0.6...\n",
              "Name: topic_0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB7rXXFxXcS5",
        "colab_type": "code",
        "outputId": "f5e5460b-f692-498e-a62b-bd502ca2434e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "def assertDim( topic_vec ):\n",
        "   assert len(topic_vec) == len(m[list(m.wv.vocab)[0]]) and len(topic_vec) > 0\n",
        "    \n",
        "def assertType( topic_vec ):\n",
        "   assert isinstance(topic_vec, list)\n",
        "    \n",
        "data_kaggle.topic_0.apply( assertDim )[0]\n",
        "data_kaggle.topic_0.apply( assertType )[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLL4kOxfXhto",
        "colab_type": "text"
      },
      "source": [
        "The second topic term is also encoded into word2vec vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjY31CCiXl8i",
        "colab_type": "code",
        "outputId": "7b1f765a-a4b5-48f8-ab23-9ed7c8097d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "vector_dim = len(m[list(m.wv.vocab)[0]])\n",
        "\n",
        "def encodeSecondColumn( topics ):\n",
        "\n",
        "   if len( topics ) > 1 :\n",
        "      topic = topics[1].strip()\n",
        "   else:\n",
        "      return np.zeros( len(m[list(m.wv.vocab)[0]]) ).tolist()\n",
        "   if topic in m:\n",
        "      return m[topic].tolist()\n",
        "   else:\n",
        "      return np.zeros( len(m[list(m.wv.vocab)[0]]) ).tolist()\n",
        "   \n",
        "data_kaggle[ 'topic_1' ] = data_kaggle.topics.apply( encodeSecondColumn )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K95K4YIXtDw",
        "colab_type": "code",
        "outputId": "ee3bdf9c-a867-476b-b3be-8c5f8e8a989e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "data_kaggle.topic_1.apply( assertDim )[0]\n",
        "data_kaggle.topic_1.apply( assertType )[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhzcY4KAYArt",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis\n",
        "Sentiment analysis has been done on the Kaggle data set, from (Martin et. al, n.d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_7y8CmQYQtV",
        "colab_type": "code",
        "outputId": "fe4583f0-689e-4eba-e642-7b526e32c82e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def getSentiment( text ):\n",
        "    return sia.polarity_scores(text)\n",
        "  \n",
        "data_kaggle[ 'text_distilled_joined' ] = data_kaggle[ 'text_distilled' ].apply( lambda x : ' '.join(x) ) \n",
        "data_kaggle[ 'sentiment' ] = data_kaggle.text_distilled_joined.apply( getSentiment )\n",
        "\n",
        "print(data_kaggle['sentiment' ][:10] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "0    {'neg': 0.135, 'neu': 0.669, 'pos': 0.196, 'co...\n",
            "1    {'neg': 0.129, 'neu': 0.815, 'pos': 0.057, 'co...\n",
            "2    {'neg': 0.051, 'neu': 0.791, 'pos': 0.158, 'co...\n",
            "3    {'neg': 0.3, 'neu': 0.46, 'pos': 0.24, 'compou...\n",
            "4    {'neg': 0.093, 'neu': 0.722, 'pos': 0.185, 'co...\n",
            "5    {'neg': 0.356, 'neu': 0.591, 'pos': 0.053, 'co...\n",
            "6    {'neg': 0.135, 'neu': 0.723, 'pos': 0.142, 'co...\n",
            "7    {'neg': 0.193, 'neu': 0.735, 'pos': 0.072, 'co...\n",
            "8    {'neg': 0.066, 'neu': 0.842, 'pos': 0.093, 'co...\n",
            "9    {'neg': 0.152, 'neu': 0.792, 'pos': 0.056, 'co...\n",
            "Name: sentiment, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eruxSNLwYYjK",
        "colab_type": "text"
      },
      "source": [
        "## Converting Dates to Numeric Format\n",
        "The date column with format YYYY-MM-dd is converted into YYYYMM (where MM is month, and YYYY) is year. Specific day of the months is dropped to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-WB4mUJYehG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setNumericDate( date ):\n",
        "   y, m, d = date.split( \"-\" )\n",
        "   return int(y + m)\n",
        "\n",
        "data_kaggle[ 'date_int' ] = data_kaggle.date.apply( setNumericDate )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjLiYid_YhOU",
        "colab_type": "code",
        "outputId": "a5457200-846a-400b-ea63-ca780b27e458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_kaggle[ 'date_int' ][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201610"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87CcyLNmYjqT",
        "colab_type": "code",
        "outputId": "76062eb3-5a9b-4cd9-bf3d-62961fb43225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "data_kaggle.date_int.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    201610\n",
              "1    201610\n",
              "2    201610\n",
              "3    201611\n",
              "4    201611\n",
              "Name: date_int, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE36wdv7YsQd",
        "colab_type": "text"
      },
      "source": [
        "## Using Ranfom Forrest Classifier\n",
        "Using the numeric (integer) dates, and two vectors (for top two topics chosen), a random forrest classifier is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58vXYvMEYx_9",
        "colab_type": "code",
        "outputId": "965374bb-4c3b-4696-d3f1-ec0b2b67d9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# First split the vectors of word2vec into individual columns\n",
        "\n",
        "X_prep = data_kaggle[[ 'date_int', 'topic_0', 'topic_1' ]]\n",
        "\n",
        "def select(topic_vec, idx):\n",
        "   return topic_vec[idx]\n",
        "\n",
        "prep_dict = { \"date_int\" : [] }\n",
        "for i in range( vector_dim*2 ):\n",
        "   prep_dict[str(i)] = []\n",
        "    \n",
        "X_COV = pd.DataFrame(prep_dict)\n",
        "X_COV[ 'date_int' ] = X_prep[ 'date_int' ]\n",
        "\n",
        "vector_dim = len(m[list(m.wv.vocab)[0]])\n",
        "\n",
        "for i in range( vector_dim ):\n",
        "   X_COV[str(i)] = X_prep.topic_0.apply( lambda x : select(x, i ) )\n",
        "    \n",
        "X_COV.head(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date_int</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>201610</td>\n",
              "      <td>0.133141</td>\n",
              "      <td>0.02276</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.193098</td>\n",
              "      <td>0.090606</td>\n",
              "      <td>-0.166031</td>\n",
              "      <td>0.17109</td>\n",
              "      <td>-0.190256</td>\n",
              "      <td>-0.204141</td>\n",
              "      <td>-0.08987</td>\n",
              "      <td>0.151585</td>\n",
              "      <td>-0.046679</td>\n",
              "      <td>0.286854</td>\n",
              "      <td>0.039916</td>\n",
              "      <td>0.124656</td>\n",
              "      <td>0.118104</td>\n",
              "      <td>0.139604</td>\n",
              "      <td>0.068348</td>\n",
              "      <td>0.120777</td>\n",
              "      <td>0.022915</td>\n",
              "      <td>0.315592</td>\n",
              "      <td>-0.082241</td>\n",
              "      <td>0.054401</td>\n",
              "      <td>0.24739</td>\n",
              "      <td>0.041435</td>\n",
              "      <td>0.138863</td>\n",
              "      <td>-0.048728</td>\n",
              "      <td>0.039117</td>\n",
              "      <td>0.00639</td>\n",
              "      <td>0.097284</td>\n",
              "      <td>0.015881</td>\n",
              "      <td>0.10588</td>\n",
              "      <td>0.196704</td>\n",
              "      <td>0.202438</td>\n",
              "      <td>-0.014729</td>\n",
              "      <td>0.10339</td>\n",
              "      <td>0.144082</td>\n",
              "      <td>-0.134328</td>\n",
              "      <td>-0.299595</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 201 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   date_int         0        1         2         3  ...  195  196  197  198  199\n",
              "0    201610  0.133141  0.02276  0.000505  0.193098  ...  NaN  NaN  NaN  NaN  NaN\n",
              "\n",
              "[1 rows x 201 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQXaH9jCY_d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range( vector_dim ):\n",
        "   X_COV[str(vector_dim + i)] = X_prep.topic_1.apply( lambda x : select(x, i ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEu8EwPiZB60",
        "colab_type": "code",
        "outputId": "43af02ea-396b-4d71-ec4b-baf696c79df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "X_COV.head(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date_int</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>201610</td>\n",
              "      <td>0.133141</td>\n",
              "      <td>0.02276</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.193098</td>\n",
              "      <td>0.090606</td>\n",
              "      <td>-0.166031</td>\n",
              "      <td>0.17109</td>\n",
              "      <td>-0.190256</td>\n",
              "      <td>-0.204141</td>\n",
              "      <td>-0.08987</td>\n",
              "      <td>0.151585</td>\n",
              "      <td>-0.046679</td>\n",
              "      <td>0.286854</td>\n",
              "      <td>0.039916</td>\n",
              "      <td>0.124656</td>\n",
              "      <td>0.118104</td>\n",
              "      <td>0.139604</td>\n",
              "      <td>0.068348</td>\n",
              "      <td>0.120777</td>\n",
              "      <td>0.022915</td>\n",
              "      <td>0.315592</td>\n",
              "      <td>-0.082241</td>\n",
              "      <td>0.054401</td>\n",
              "      <td>0.24739</td>\n",
              "      <td>0.041435</td>\n",
              "      <td>0.138863</td>\n",
              "      <td>-0.048728</td>\n",
              "      <td>0.039117</td>\n",
              "      <td>0.00639</td>\n",
              "      <td>0.097284</td>\n",
              "      <td>0.015881</td>\n",
              "      <td>0.10588</td>\n",
              "      <td>0.196704</td>\n",
              "      <td>0.202438</td>\n",
              "      <td>-0.014729</td>\n",
              "      <td>0.10339</td>\n",
              "      <td>0.144082</td>\n",
              "      <td>-0.134328</td>\n",
              "      <td>-0.299595</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.124945</td>\n",
              "      <td>-0.568857</td>\n",
              "      <td>-0.381606</td>\n",
              "      <td>-0.189887</td>\n",
              "      <td>0.388709</td>\n",
              "      <td>-0.137405</td>\n",
              "      <td>0.257496</td>\n",
              "      <td>0.098614</td>\n",
              "      <td>-0.108115</td>\n",
              "      <td>0.24898</td>\n",
              "      <td>0.999104</td>\n",
              "      <td>-0.168165</td>\n",
              "      <td>0.192731</td>\n",
              "      <td>0.629187</td>\n",
              "      <td>0.135066</td>\n",
              "      <td>0.223646</td>\n",
              "      <td>0.150666</td>\n",
              "      <td>-0.335486</td>\n",
              "      <td>0.534092</td>\n",
              "      <td>-0.624571</td>\n",
              "      <td>0.031893</td>\n",
              "      <td>-0.972887</td>\n",
              "      <td>-0.386115</td>\n",
              "      <td>0.097354</td>\n",
              "      <td>0.151091</td>\n",
              "      <td>-0.100819</td>\n",
              "      <td>0.184339</td>\n",
              "      <td>-0.505238</td>\n",
              "      <td>0.493485</td>\n",
              "      <td>-0.040051</td>\n",
              "      <td>0.069003</td>\n",
              "      <td>0.101926</td>\n",
              "      <td>0.324011</td>\n",
              "      <td>-0.728298</td>\n",
              "      <td>0.046542</td>\n",
              "      <td>-0.150548</td>\n",
              "      <td>-0.344977</td>\n",
              "      <td>-0.015514</td>\n",
              "      <td>0.151645</td>\n",
              "      <td>-0.141411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 201 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   date_int         0        1  ...       197       198       199\n",
              "0    201610  0.133141  0.02276  ... -0.015514  0.151645 -0.141411\n",
              "\n",
              "[1 rows x 201 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBd8kj4mZJe0",
        "colab_type": "text"
      },
      "source": [
        "The following code creates training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR2b8gVTZNXv",
        "colab_type": "code",
        "outputId": "e6f4b554-f45b-46fc-e236-4ee16c3583b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y_COV = data_kaggle[ 'coverage' ]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_COV, Y_COV, test_size=0.33, random_state=42)\n",
        "print(\"train:\" , X_train.shape)\n",
        "print(\"test:\" , X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: (670, 201)\n",
            "test: (330, 201)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Y4fZxhZRAL",
        "colab_type": "code",
        "outputId": "e54aa95c-0d12-4947-8e11-014f7a0ded02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "X_COV.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date_int</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>201610</td>\n",
              "      <td>0.133141</td>\n",
              "      <td>0.022760</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.193098</td>\n",
              "      <td>0.090606</td>\n",
              "      <td>-0.166031</td>\n",
              "      <td>0.171090</td>\n",
              "      <td>-0.190256</td>\n",
              "      <td>-0.204141</td>\n",
              "      <td>-0.089870</td>\n",
              "      <td>0.151585</td>\n",
              "      <td>-0.046679</td>\n",
              "      <td>0.286854</td>\n",
              "      <td>0.039916</td>\n",
              "      <td>0.124656</td>\n",
              "      <td>0.118104</td>\n",
              "      <td>0.139604</td>\n",
              "      <td>0.068348</td>\n",
              "      <td>0.120777</td>\n",
              "      <td>0.022915</td>\n",
              "      <td>0.315592</td>\n",
              "      <td>-0.082241</td>\n",
              "      <td>0.054401</td>\n",
              "      <td>0.247390</td>\n",
              "      <td>0.041435</td>\n",
              "      <td>0.138863</td>\n",
              "      <td>-0.048728</td>\n",
              "      <td>0.039117</td>\n",
              "      <td>0.006390</td>\n",
              "      <td>0.097284</td>\n",
              "      <td>0.015881</td>\n",
              "      <td>0.105880</td>\n",
              "      <td>0.196704</td>\n",
              "      <td>0.202438</td>\n",
              "      <td>-0.014729</td>\n",
              "      <td>0.103390</td>\n",
              "      <td>0.144082</td>\n",
              "      <td>-0.134328</td>\n",
              "      <td>-0.299595</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.124945</td>\n",
              "      <td>-0.568857</td>\n",
              "      <td>-0.381606</td>\n",
              "      <td>-0.189887</td>\n",
              "      <td>0.388709</td>\n",
              "      <td>-0.137405</td>\n",
              "      <td>0.257496</td>\n",
              "      <td>0.098614</td>\n",
              "      <td>-0.108115</td>\n",
              "      <td>0.248980</td>\n",
              "      <td>0.999104</td>\n",
              "      <td>-0.168165</td>\n",
              "      <td>0.192731</td>\n",
              "      <td>0.629187</td>\n",
              "      <td>0.135066</td>\n",
              "      <td>0.223646</td>\n",
              "      <td>0.150666</td>\n",
              "      <td>-0.335486</td>\n",
              "      <td>0.534092</td>\n",
              "      <td>-0.624571</td>\n",
              "      <td>0.031893</td>\n",
              "      <td>-0.972887</td>\n",
              "      <td>-0.386115</td>\n",
              "      <td>0.097354</td>\n",
              "      <td>0.151091</td>\n",
              "      <td>-0.100819</td>\n",
              "      <td>0.184339</td>\n",
              "      <td>-0.505238</td>\n",
              "      <td>0.493485</td>\n",
              "      <td>-0.040051</td>\n",
              "      <td>0.069003</td>\n",
              "      <td>0.101926</td>\n",
              "      <td>0.324011</td>\n",
              "      <td>-0.728298</td>\n",
              "      <td>0.046542</td>\n",
              "      <td>-0.150548</td>\n",
              "      <td>-0.344977</td>\n",
              "      <td>-0.015514</td>\n",
              "      <td>0.151645</td>\n",
              "      <td>-0.141411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>201610</td>\n",
              "      <td>0.157192</td>\n",
              "      <td>0.029061</td>\n",
              "      <td>0.013314</td>\n",
              "      <td>0.252122</td>\n",
              "      <td>0.126144</td>\n",
              "      <td>-0.232334</td>\n",
              "      <td>0.221243</td>\n",
              "      <td>-0.223681</td>\n",
              "      <td>-0.234135</td>\n",
              "      <td>-0.115174</td>\n",
              "      <td>0.205194</td>\n",
              "      <td>-0.055308</td>\n",
              "      <td>0.348109</td>\n",
              "      <td>0.056858</td>\n",
              "      <td>0.165996</td>\n",
              "      <td>0.146724</td>\n",
              "      <td>0.175960</td>\n",
              "      <td>0.095519</td>\n",
              "      <td>0.153203</td>\n",
              "      <td>0.017035</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>-0.096500</td>\n",
              "      <td>0.059357</td>\n",
              "      <td>0.300754</td>\n",
              "      <td>0.038923</td>\n",
              "      <td>0.171346</td>\n",
              "      <td>-0.054553</td>\n",
              "      <td>0.047513</td>\n",
              "      <td>0.014488</td>\n",
              "      <td>0.127616</td>\n",
              "      <td>0.012770</td>\n",
              "      <td>0.130882</td>\n",
              "      <td>0.229752</td>\n",
              "      <td>0.251314</td>\n",
              "      <td>0.009855</td>\n",
              "      <td>0.126285</td>\n",
              "      <td>0.186209</td>\n",
              "      <td>-0.161298</td>\n",
              "      <td>-0.360938</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001336</td>\n",
              "      <td>-0.447362</td>\n",
              "      <td>-0.270268</td>\n",
              "      <td>-0.161780</td>\n",
              "      <td>0.373512</td>\n",
              "      <td>-0.008976</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.063679</td>\n",
              "      <td>-0.103723</td>\n",
              "      <td>0.145099</td>\n",
              "      <td>0.743461</td>\n",
              "      <td>-0.153102</td>\n",
              "      <td>0.156540</td>\n",
              "      <td>0.407802</td>\n",
              "      <td>0.019480</td>\n",
              "      <td>0.153765</td>\n",
              "      <td>0.202535</td>\n",
              "      <td>-0.188576</td>\n",
              "      <td>0.435051</td>\n",
              "      <td>-0.434912</td>\n",
              "      <td>0.051607</td>\n",
              "      <td>-0.703885</td>\n",
              "      <td>-0.342090</td>\n",
              "      <td>0.031409</td>\n",
              "      <td>0.176686</td>\n",
              "      <td>-0.035643</td>\n",
              "      <td>0.178913</td>\n",
              "      <td>-0.388436</td>\n",
              "      <td>0.355624</td>\n",
              "      <td>-0.103681</td>\n",
              "      <td>0.080223</td>\n",
              "      <td>0.114311</td>\n",
              "      <td>0.269578</td>\n",
              "      <td>-0.524341</td>\n",
              "      <td>0.002468</td>\n",
              "      <td>-0.024183</td>\n",
              "      <td>-0.222825</td>\n",
              "      <td>0.053790</td>\n",
              "      <td>0.065441</td>\n",
              "      <td>-0.237334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>201610</td>\n",
              "      <td>0.217056</td>\n",
              "      <td>0.044668</td>\n",
              "      <td>0.058785</td>\n",
              "      <td>0.382050</td>\n",
              "      <td>0.202786</td>\n",
              "      <td>-0.383304</td>\n",
              "      <td>0.350736</td>\n",
              "      <td>-0.330440</td>\n",
              "      <td>-0.323174</td>\n",
              "      <td>-0.154769</td>\n",
              "      <td>0.320239</td>\n",
              "      <td>-0.073156</td>\n",
              "      <td>0.471759</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.273190</td>\n",
              "      <td>0.201521</td>\n",
              "      <td>0.288995</td>\n",
              "      <td>0.156015</td>\n",
              "      <td>0.239782</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>0.549383</td>\n",
              "      <td>-0.143233</td>\n",
              "      <td>0.100450</td>\n",
              "      <td>0.422479</td>\n",
              "      <td>0.048470</td>\n",
              "      <td>0.281399</td>\n",
              "      <td>-0.089117</td>\n",
              "      <td>0.100103</td>\n",
              "      <td>0.032319</td>\n",
              "      <td>0.206834</td>\n",
              "      <td>0.003150</td>\n",
              "      <td>0.199114</td>\n",
              "      <td>0.324852</td>\n",
              "      <td>0.404298</td>\n",
              "      <td>0.072829</td>\n",
              "      <td>0.180402</td>\n",
              "      <td>0.312731</td>\n",
              "      <td>-0.228016</td>\n",
              "      <td>-0.537070</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014956</td>\n",
              "      <td>-0.312478</td>\n",
              "      <td>-0.185301</td>\n",
              "      <td>-0.122320</td>\n",
              "      <td>0.275844</td>\n",
              "      <td>0.004335</td>\n",
              "      <td>0.219421</td>\n",
              "      <td>0.053112</td>\n",
              "      <td>-0.076069</td>\n",
              "      <td>0.106048</td>\n",
              "      <td>0.526128</td>\n",
              "      <td>-0.109282</td>\n",
              "      <td>0.108851</td>\n",
              "      <td>0.274530</td>\n",
              "      <td>0.006063</td>\n",
              "      <td>0.111773</td>\n",
              "      <td>0.143344</td>\n",
              "      <td>-0.129076</td>\n",
              "      <td>0.300258</td>\n",
              "      <td>-0.293823</td>\n",
              "      <td>0.033190</td>\n",
              "      <td>-0.483857</td>\n",
              "      <td>-0.247282</td>\n",
              "      <td>0.024394</td>\n",
              "      <td>0.123597</td>\n",
              "      <td>-0.024521</td>\n",
              "      <td>0.129386</td>\n",
              "      <td>-0.266605</td>\n",
              "      <td>0.246007</td>\n",
              "      <td>-0.066114</td>\n",
              "      <td>0.066371</td>\n",
              "      <td>0.069068</td>\n",
              "      <td>0.186766</td>\n",
              "      <td>-0.359455</td>\n",
              "      <td>0.003262</td>\n",
              "      <td>-0.004505</td>\n",
              "      <td>-0.145845</td>\n",
              "      <td>0.038401</td>\n",
              "      <td>0.036339</td>\n",
              "      <td>-0.176359</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 201 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   date_int         0         1  ...       197       198       199\n",
              "0    201610  0.133141  0.022760  ... -0.015514  0.151645 -0.141411\n",
              "1    201610  0.157192  0.029061  ...  0.053790  0.065441 -0.237334\n",
              "2    201610  0.217056  0.044668  ...  0.038401  0.036339 -0.176359\n",
              "\n",
              "[3 rows x 201 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhjBgFFlZUT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_cov = RandomForestClassifier(n_estimators=10)\n",
        "f_cov.fit(X_train, y_train)\n",
        "y_pred = f_cov.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pzh0ltNZXo7",
        "colab_type": "text"
      },
      "source": [
        "Predicting the accuracy of the coverage,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz2ETVwQZa9v",
        "colab_type": "code",
        "outputId": "eda5b054-ee70-4d59-99dc-a2b097ebd7c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'Accuracy', accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Accuracy', 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIReWSqakEy",
        "colab_type": "text"
      },
      "source": [
        "The accuracy based on the unseen test data is 90%~95%. This means that using the word2vec-based model one can build a reliable coverage score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-In7i3cdaoAD",
        "colab_type": "code",
        "outputId": "77112ea4-b84e-4630-fe5a-beafdd135ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "importantCols = pd.Series(f_cov.feature_importances_,index=list(X_COV)).sort_values(ascending=False)\n",
        "sns.barplot(y=importantCols[:20], x=importantCols.index[:20])\n",
        "\n",
        "plt.ylabel('Importance Score')\n",
        "plt.xlabel('Columns')\n",
        "plt.title(\"Importance Sorted\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g665RJDHayuG",
        "colab_type": "text"
      },
      "source": [
        "## Interpretation\n",
        "As shown above \"date_int\" is one of the most important feature in determining whether or not the news has high news coverage. This is expected since the date the news has been posted definitely impacts on whether the news has sufficient coverage from peer articles. Some of the numeric (word2vec) columns have significant impact on the prediction of the coverage score. But, since these factors are latent, they do not correspond to specific words in the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZB701sPa5I3",
        "colab_type": "text"
      },
      "source": [
        "## Other Methods Tried\n",
        "Also tried using TFIDF as well as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRCHQn1Ua9vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "ktdv = TfidfVectorizer()\n",
        "  \n",
        "data_kaggle['text_distilled_joined'] = data_kaggle[ 'text_distilled' ].apply( lambda x :  ' '.join( x ) )\n",
        "ktdv.fit( data_kaggle[ 'text_distilled_joined' ])\n",
        "dim_row = len(ktdv.transform([data_kaggle[ 'text_distilled_joined' ][0]]).toarray())\n",
        "dim_col = len(ktdv.transform([data_kaggle[ 'text_distilled_joined' ][0]]).toarray()[0])\n",
        "# Eventually dropped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgyHBatNbLRh",
        "colab_type": "text"
      },
      "source": [
        "## Extension - Increase the window to 2 Months\n",
        "Now that we have coverage of 5 and above for 6/100 articles. We can try and see what values we see if the coverage is increased from 30 days to 60 days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5v-MGsIbO5X",
        "colab_type": "code",
        "outputId": "12c53638-d830-4c2f-d2fb-fa93a15fa66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "def coverage60( article ):\n",
        "   fromdate, todate = window( article[ 'date' ], 60 )\n",
        "   selected_coverage = articleDataFrame[(articleDataFrame['date'] > fromdate) & (articleDataFrame['date'] < todate)]\n",
        "   selected_coverage['covered'] = selected_coverage.apply( lambda r: r[ 'topics' ][0] in article.topics and\n",
        "                                                       r[ 'topics' ][1] in article.topics, axis=1 )\n",
        "   return len(selected_coverage[selected_coverage['covered'] == True])\n",
        "\n",
        "data_kaggle[ 'coverage60' ] = data_kaggle.apply( coverage60, axis=1 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rntGBZR8bkZ9",
        "colab_type": "code",
        "outputId": "50b2696d-32eb-45f7-acdb-d230a94a9984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data_kaggle.sort_values(by=['coverage'], ascending=False ).head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>ord_in_thread</th>\n",
              "      <th>author</th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>crawled</th>\n",
              "      <th>site_url</th>\n",
              "      <th>country</th>\n",
              "      <th>domain_rank</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>spam_score</th>\n",
              "      <th>main_img_url</th>\n",
              "      <th>replies_count</th>\n",
              "      <th>participants_count</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>shares</th>\n",
              "      <th>type</th>\n",
              "      <th>text_distilled</th>\n",
              "      <th>text_distilled_lemma</th>\n",
              "      <th>topics</th>\n",
              "      <th>date</th>\n",
              "      <th>coverage</th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>text_distilled_joined</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>date_int</th>\n",
              "      <th>coverage60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
              "      <td>0</td>\n",
              "      <td>Barracuda Brigade</td>\n",
              "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>Print They should pay all the back all the mon...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
              "      <td>100percentfedup.com</td>\n",
              "      <td>US</td>\n",
              "      <td>25689.0</td>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bias</td>\n",
              "      <td>[muslims, busted, stole, millions, gov, benefi...</td>\n",
              "      <td>[muslim, busted, they, stole, million, in, gov...</td>\n",
              "      <td>[benefits , government]</td>\n",
              "      <td>2016-10-26</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.13314050436019897, 0.02276035025715828, 0.0...</td>\n",
              "      <td>[0.3340817987918854, 0.04272259771823883, -0.1...</td>\n",
              "      <td>muslims busted stole millions gov benefits pri...</td>\n",
              "      <td>{'neg': 0.135, 'neu': 0.669, 'pos': 0.196, 'co...</td>\n",
              "      <td>201610</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>671</th>\n",
              "      <td>9cf9163fcb8564e3e3e51f13b7dc78a1eeb60445</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-03T21:04:03.190+02:00</td>\n",
              "      <td>Protesters, Police Still Clashing Over Dispute...</td>\n",
              "      <td>Protesters, Police Still Clashing Over Dispute...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-03T21:04:03.190+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Protesters, Police Still Clashing Over Dispute...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[protesters, police, still, clashing, disputed...</td>\n",
              "      <td>[protester, police, still, clashing, over, dis...</td>\n",
              "      <td>[protesters , pipeline]</td>\n",
              "      <td>2016-11-03</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.32322007417678833, 0.07965002954006195, 0.0...</td>\n",
              "      <td>[0.32374510169029236, 0.06949824839830399, -0....</td>\n",
              "      <td>protesters police still clashing disputed nort...</td>\n",
              "      <td>{'neg': 0.191, 'neu': 0.757, 'pos': 0.052, 'co...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>e5c051173c7efd9a46dc2449146c544e20f931ba</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-02T19:14:58.593+02:00</td>\n",
              "      <td>Bundy brothers discharged from Oregon jail, he...</td>\n",
              "      <td>Bundy brothers discharged from Oregon jail, he...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-02T19:14:58.593+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bundy brothers discharged from Oregon jail, he...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[bundy, brothers, discharged, oregon, jail, he...</td>\n",
              "      <td>[bundy, brother, discharged, from, oregon, jai...</td>\n",
              "      <td>[federal , bundy]</td>\n",
              "      <td>2016-11-02</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.3503999710083008, 0.050964225083589554, -0....</td>\n",
              "      <td>[0.26084157824516296, 0.057947348803281784, -0...</td>\n",
              "      <td>bundy brothers discharged oregon jail headed n...</td>\n",
              "      <td>{'neg': 0.244, 'neu': 0.676, 'pos': 0.08, 'com...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>0c9d01b2bc7e6e5be2ebecd52b54468b5bf1e731</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-02T19:24:00.901+02:00</td>\n",
              "      <td>Warning or threat? Hillary hints at second civ...</td>\n",
              "      <td>Warning or threat? Hillary hints at second civ...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-02T19:24:00.901+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Warning or threat? Hillary hints at second civ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[warning, threat, hillary, hints, second, civi...</td>\n",
              "      <td>[warning, or, threat, hillary, hint, at, secon...</td>\n",
              "      <td>[clinton , war]</td>\n",
              "      <td>2016-11-02</td>\n",
              "      <td>0</td>\n",
              "      <td>[-0.03145146742463112, 0.03744429722428322, 0....</td>\n",
              "      <td>[0.34359848499298096, 0.01666543260216713, -0....</td>\n",
              "      <td>warning threat hillary hints second civil war ...</td>\n",
              "      <td>{'neg': 0.305, 'neu': 0.581, 'pos': 0.115, 'co...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>705bbad66afa88a8b0c0c8854d107be3a4369bad</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-02T19:24:01.278+02:00</td>\n",
              "      <td>U.S. militia girds for trouble as presidential...</td>\n",
              "      <td>U.S. militia girds for trouble as presidential...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-02T19:24:01.278+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>U.S. militia girds for trouble as presidential...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[u, militia, girds, trouble, presidential, ele...</td>\n",
              "      <td>[u, s, militia, girds, for, trouble, a, presid...</td>\n",
              "      <td>[members , said]</td>\n",
              "      <td>2016-11-02</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.32212209701538086, 0.06858672201633453, 0.0...</td>\n",
              "      <td>[0.300503134727478, 0.05062494054436684, 0.053...</td>\n",
              "      <td>u militia girds trouble presidential election ...</td>\n",
              "      <td>{'neg': 0.203, 'neu': 0.673, 'pos': 0.124, 'co...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661</th>\n",
              "      <td>da8dad964c1fdbc0126a37b5fbb697191053fd56</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-03T03:45:27.666+02:00</td>\n",
              "      <td>Donald Trump a KGB Spy?</td>\n",
              "      <td>Donald Trump a KGB Spy? 11/02/2016 In today’s ...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-03T03:45:27.666+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Donald Trump a KGB Spy?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[donald, trump, kgb, spy, donald, trump, kgb, ...</td>\n",
              "      <td>[donald, trump, a, kgb, spy, , donald, trump, ...</td>\n",
              "      <td>[trump , donald]</td>\n",
              "      <td>2016-11-03</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.30505669116973877, 0.17706751823425293, 0.6...</td>\n",
              "      <td>[0.1965862512588501, 0.23831458389759064, 0.65...</td>\n",
              "      <td>donald trump kgb spy donald trump kgb spy 11 0...</td>\n",
              "      <td>{'neg': 0.224, 'neu': 0.639, 'pos': 0.138, 'co...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>662</th>\n",
              "      <td>5f730d5c691f556533af5249645f9e795153069c</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-03T21:03:58.143+02:00</td>\n",
              "      <td>Putin grants Steven Seagal Russian citizenship</td>\n",
              "      <td>Putin grants Steven Seagal Russian citizenship...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-03T21:03:58.143+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Putin grants Steven Seagal Russian citizenship</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[putin, grants, steven, seagal, russian, citiz...</td>\n",
              "      <td>[putin, grant, steven, seagal, russian, citize...</td>\n",
              "      <td>[citizenship , seagal]</td>\n",
              "      <td>2016-11-03</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.17721669375896454, 0.0432465486228466, -0.0...</td>\n",
              "      <td>[0.11403995752334595, 0.032719433307647705, -0...</td>\n",
              "      <td>putin grants steven seagal russian citizenship...</td>\n",
              "      <td>{'neg': 0.026, 'neu': 0.824, 'pos': 0.151, 'co...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>c172348f3ee7cff3d829e89fa8efbc51b52578a8</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-03T21:04:00.553+02:00</td>\n",
              "      <td>New bionic eye implant connects directly to brain</td>\n",
              "      <td>New bionic eye implant connects directly to br...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-03T21:04:00.553+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New bionic eye implant connects directly to brain</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[new, bionic, eye, implant, connects, directly...</td>\n",
              "      <td>[new, bionic, eye, implant, connects, directly...</td>\n",
              "      <td>[brain , eye]</td>\n",
              "      <td>2016-11-03</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.21180766820907593, 0.05093103647232056, 0.0...</td>\n",
              "      <td>[0.18199436366558075, 0.047636404633522034, -0...</td>\n",
              "      <td>new bionic eye implant connects directly brain...</td>\n",
              "      <td>{'neg': 0.112, 'neu': 0.747, 'pos': 0.141, 'co...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>97a40d1daec7128514ab1388da0c6b12d115bf78</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-03T21:04:00.615+02:00</td>\n",
              "      <td>American Express disowns Pink Floyd singer Rog...</td>\n",
              "      <td>American Express disowns Pink Floyd singer Rog...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-03T21:04:00.615+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>American Express disowns Pink Floyd singer Rog...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[american, express, disowns, pink, floyd, sing...</td>\n",
              "      <td>[american, express, disowns, pink, floyd, sing...</td>\n",
              "      <td>[american , express]</td>\n",
              "      <td>2016-11-03</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.3236471116542816, 0.07570502161979675, 0.04...</td>\n",
              "      <td>[0.17577561736106873, 0.04241001978516579, 0.0...</td>\n",
              "      <td>american express disowns pink floyd singer rog...</td>\n",
              "      <td>{'neg': 0.083, 'neu': 0.808, 'pos': 0.109, 'co...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>3049896a83c35e20cdb82e11c6334f2a1f81c195</td>\n",
              "      <td>0</td>\n",
              "      <td>Alex Ansary</td>\n",
              "      <td>2016-11-03T21:04:00.689+02:00</td>\n",
              "      <td>Police fire rubber bullets at pipeline protesters</td>\n",
              "      <td>Police fire rubber bullets at pipeline protest...</td>\n",
              "      <td>english</td>\n",
              "      <td>2016-11-03T21:04:00.689+02:00</td>\n",
              "      <td>amtvmedia.com</td>\n",
              "      <td>US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Police fire rubber bullets at pipeline protesters</td>\n",
              "      <td>0.0</td>\n",
              "      <td>http://www.amtvmedia.com/wp-content/uploads/20...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bs</td>\n",
              "      <td>[police, fire, rubber, bullets, pipeline, prot...</td>\n",
              "      <td>[police, fire, rubber, bullet, at, pipeline, p...</td>\n",
              "      <td>[police , river]</td>\n",
              "      <td>2016-11-03</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.3580940067768097, 0.09070847928524017, -0.0...</td>\n",
              "      <td>[0.29855579137802124, 0.07115820050239563, -0....</td>\n",
              "      <td>police fire rubber bullets pipeline protesters...</td>\n",
              "      <td>{'neg': 0.131, 'neu': 0.818, 'pos': 0.05, 'com...</td>\n",
              "      <td>201611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         uuid  ...  coverage60\n",
              "0    6a175f46bcd24d39b3e962ad0f29936721db70db  ...           0\n",
              "671  9cf9163fcb8564e3e3e51f13b7dc78a1eeb60445  ...           0\n",
              "658  e5c051173c7efd9a46dc2449146c544e20f931ba  ...           0\n",
              "659  0c9d01b2bc7e6e5be2ebecd52b54468b5bf1e731  ...           0\n",
              "660  705bbad66afa88a8b0c0c8854d107be3a4369bad  ...           0\n",
              "661  da8dad964c1fdbc0126a37b5fbb697191053fd56  ...           0\n",
              "662  5f730d5c691f556533af5249645f9e795153069c  ...           0\n",
              "663  c172348f3ee7cff3d829e89fa8efbc51b52578a8  ...           0\n",
              "664  97a40d1daec7128514ab1388da0c6b12d115bf78  ...           0\n",
              "665  3049896a83c35e20cdb82e11c6334f2a1f81c195  ...           0\n",
              "\n",
              "[10 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20A8xauNdpi7",
        "colab_type": "text"
      },
      "source": [
        "Counting number of articles with coverage scores above 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL3i5m4zdsFO",
        "colab_type": "code",
        "outputId": "8657ed3d-693d-44fe-e5e8-1e632d58c4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data_kaggle[data_kaggle['coverage60']>1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51bpIHqEdxN7",
        "colab_type": "text"
      },
      "source": [
        "## Testing on Non-Fake News\n",
        "The coverage scores (with window = 60 days) are compared between fake news dataset and \"All news\" non-fake news dataset. Since window of 60 days are chosen for fake news dataset, the same window is chosen for non-fake datasets for consistency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzSKb5TBelp-",
        "colab_type": "code",
        "outputId": "3217e13b-ea1f-45b8-e228-50c851c24856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "non_fake_data = articleDataFrame[:100]\n",
        "\n",
        "non_fake_data[ 'coverage60' ] = non_fake_data.apply( coverage60, axis=1 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-f330503c8f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnon_fake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticleDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnon_fake_data\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'coverage60'\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_fake_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcoverage60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-cd486f9fcf4b>\u001b[0m in \u001b[0;36mcoverage60\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0mselected_coverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticleDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticleDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfromdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marticleDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtodate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    selected_coverage['covered'] = selected_coverage.apply( lambda r: r[ 'topics' ][0] in article.topics and\n\u001b[0;32m----> 5\u001b[0;31m                                                        r[ 'topics' ][1] in article.topics, axis=1 )\n\u001b[0m\u001b[1;32m      6\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_coverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_coverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'covered'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-cd486f9fcf4b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0mselected_coverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticleDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticleDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfromdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marticleDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtodate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    selected_coverage['covered'] = selected_coverage.apply( lambda r: r[ 'topics' ][0] in article.topics and\n\u001b[0;32m----> 5\u001b[0;31m                                                        r[ 'topics' ][1] in article.topics, axis=1 )\n\u001b[0m\u001b[1;32m      6\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_coverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_coverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'covered'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hjLrtKqgLmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_fake_data.sort_values(by=['coverage60'], ascending=False ).head(40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLhidWFxgOIE",
        "colab_type": "text"
      },
      "source": [
        "Counting the number of articles with coverage score greater than 1,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-6hserZgQZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(non_fake_data[non_fake_data['coverage60']>1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKBw39L6gTb9",
        "colab_type": "text"
      },
      "source": [
        "## Comparisons between Fake and Non-Fake News Datasets\n",
        "Based on sample of 100 articles from pool of fake and non-fake news sources, the coverage score for non-fake news articles are significantly higher than the fake news articles. In particular, around 30 out of 100 of non-fake news articles had coverage score greater than one. On the other hand, only 14 out of 100 of fake news articles had coverge score greater than one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaSeO3ilgYew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For caculating approximate time to process notebook (IGNORE)\n",
        "import datetime\n",
        "datetime.datetime.now()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwaJ31vH8ZPc",
        "colab_type": "text"
      },
      "source": [
        "# News Coverage computation methods\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8e6f89MaKKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentAnalyser():\n",
        "\n",
        "\tscaleMin=-1.\n",
        "\tscaleMax=1.\n",
        "\n",
        "    # Initializer / Instance attributes\n",
        "\tdef __init__(self, library):\n",
        "\t\tif library=='vader':\n",
        "\t\t\tself.analyser=NLTKVaderSentimentAnalyser()\n",
        "\t\telse:\n",
        "\t\t\tprint(\"ERROR - NO RECOGNISED LIBRARY\")\n",
        "\n",
        "\tdef getOverallArticleScore(self,articleResults):\n",
        "\n",
        "\t\t\n",
        "\t\tnumSentences=0.\n",
        "\t\ttotalSentScore=0.\n",
        "\t\tfor sentence in articleResults:\n",
        "\t\t\tnumSentences+=1\n",
        "\t\t\ttotalSentScore+=self.analyser.getSentenceScoreFromResults(sentence)\n",
        "\n",
        "\t\tvalue=(totalSentScore/numSentences-self.analyser.scaleMin)/(self.analyser.scaleMax-self.analyser.scaleMin)\n",
        "\t\treturn self.scaleMin+value*(self.scaleMax-self.scaleMin)\n",
        "\n",
        "\tdef generateResults(self,textToAnalyse):\n",
        "\t\treturn self.analyser.generateResults(textToAnalyse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMJdtZPFaX08",
        "colab_type": "text"
      },
      "source": [
        "### Vader library class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyOY_fgpadXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLTKVaderSentimentAnalyser():\n",
        " \n",
        "\n",
        "\tscaleMin=-1.\n",
        "\tscaleMax=1.\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.nltkVaderAnalyser=SentimentIntensityAnalyzer()\n",
        "\t\treturn\n",
        "\n",
        "\tdef generateResults(self,textToAnalyse):\n",
        "\t\tss=[]\n",
        "\t\tfor sentence in nl.sent_tokenize(textToAnalyse):\n",
        "\t\t\tss.append(self.nltkVaderAnalyser.polarity_scores(sentence))\n",
        "\t\treturn ss\n",
        "\n",
        "\tdef getSentenceScoreFromResults(self,sentenceResults):\n",
        "\t\treturn sentenceResults['compound']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbJAGMBmagOE",
        "colab_type": "text"
      },
      "source": [
        "### Computing Balance Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW5C3-_t8ZPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computePopulationBalanceScore(articleScoreDict,sentimentClass):\n",
        "\tpopulation=[-1.+(x-sentimentClass.scaleMin)/(sentimentClass.scaleMax-sentimentClass.scaleMin)*(1.-(-1.)) for x in articleScoreDict.values()]\n",
        "\treturn statistics.pstdev(population)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay6_9UQHaoN9",
        "colab_type": "text"
      },
      "source": [
        "### Overall- Avg Score Computation for Story"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXu0UoVR2moK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computePopulationBalanceScoreHistoMean(articleScoreDict,sentimentClass):\n",
        "\n",
        "\tnumBuckets=len(articleScoreDict)\n",
        "\tarticleValues=pd.Series(articleScoreDict)\n",
        "\t\n",
        "\n",
        "\tarticleValues=articleValues/0.86\n",
        "\n",
        "\tpopulatedBuckets=0\n",
        "\tfor i in range(numBuckets):\n",
        "\t\tbucketFrom=sentimentClass.scaleMin+i*(sentimentClass.scaleMax-sentimentClass.scaleMin)/numBuckets\n",
        "\t\tbucketTo=bucketFrom+(sentimentClass.scaleMax-sentimentClass.scaleMin)/numBuckets\n",
        "\n",
        "\t\tif bucketTo==sentimentClass.scaleMax:\n",
        "\t\t\tbucketTo+=0.001\n",
        "\t\tnumSamples=((bucketFrom<=articleValues) & (articleValues<bucketTo)).sum()\n",
        "\t\tif numSamples>0:\n",
        "\t\t\tpopulatedBuckets+=1\n",
        "\n",
        "\n",
        "\treturn (populatedBuckets/numBuckets * (1.-abs(articleValues.mean())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-lgXGZQtP7F",
        "colab_type": "text"
      },
      "source": [
        "### NC-Score: News Source Credibility "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSa1yDfM9q6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning(raw_news):\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmentize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qotL0HSA9uwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return - WORDS.get(word, 0)\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsp9g8mtzmSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KOMNjA5yLzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_spell_checker(text):\n",
        "    all_words = re.findall(r'\\w+', text.lower()) # split sentence to words\n",
        "    correct_number  = 0\n",
        "    incorrect_number = 0\n",
        "    print(text)\n",
        "    for i in range(len(all_words)):\n",
        "        if correction(all_words[i]) == all_words[i]:\n",
        "            correct_number = correct_number + 1\n",
        "        else:\n",
        "            incorrect_number = incorrect_number + 1\n",
        "    return correct_number , incorrect_number"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO5imyRX4h2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def credibility(row):\n",
        "  correct, incorrect = word_spell_checker(row['processed_title'])\n",
        "  if ((correct + incorrect) == 0):\n",
        "    value = correct\n",
        "    print('correct',correct)\n",
        "    print('incorrect',incorrect)\n",
        "  else: \n",
        "    value = correct/(correct + incorrect)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aZsVCOZw9JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeCredibilityScore(articleDataFrame): \n",
        "    t1 = time.time()\n",
        "    articleDataFrame['processed_title'] = articleDataFrame[\"title\"].apply(cleaning) \n",
        "    t2 = time.time()\n",
        "    print(\"\\nProcessed title: \\n\", len(articleDataFrame), \"news:\", (t2-t1)/60, \"min\")\n",
        "\n",
        "    vector = CountVectorizer()\n",
        "    vector.fit(articleDataFrame['processed_content'])\n",
        "    countVect = vector.transform(articleDataFrame['processed_content'])\n",
        "    print(vector)\n",
        "\n",
        "    countVect.toarray()\n",
        "\n",
        "    articleDataFrame['credibility_score'] = articleDataFrame.apply (lambda row: credibility(row), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQak41NUw_vI",
        "colab_type": "text"
      },
      "source": [
        " Commenting this computation for now, colab is crashing due to data load. So using other factors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Yxd5llAgn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#computeCredibilityScore(articleDataFrame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duto2ThS8ZPf",
        "colab_type": "text"
      },
      "source": [
        "## Story map loading\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23gl8ZIC8ZPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setupStoryMapAndReportList(args=None,reportArticleList=None,storyMapFileName=None):\n",
        "\n",
        "\tif args==None:\n",
        "\t\tarticleList=reportArticleList\n",
        "\t\tfileName=storyMapFileName\n",
        "\telse:\n",
        "\t\tarticleList=args['article_id_list']\n",
        "\t\tfileName=args['story_map_validation']\n",
        "\n",
        "\treportArticleList=articleList\n",
        "\tif fileName!=None:\n",
        "\t\tstoryMap=readStoryMapFromFile(fileName)\n",
        "\t\tif reportArticleList==None:\n",
        "\t\t\treportArticleList=[]\n",
        "\t\t\tfor story, articleList in storyMap.items():\n",
        "\t\t\t\treportArticleList.append(articleList[0])\n",
        "\telse:\n",
        "\t\tstoryMap=None\n",
        "\treturn storyMap,reportArticleList\n",
        "\n",
        "def readStoryMapFromFile(filename):\n",
        "\treturn readDictFromCsvFile(filename,'StoryMap')\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "def readGridParameterRangeFromFile(filename):\n",
        "\treturn readDictFromCsvFile(filename,'GridParameters')\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "def readDictFromCsvFile(filename,schema):\n",
        "\tgridParamDict={}\n",
        "\twith open(filename, 'r') as f:\n",
        "\t\tfor row in f:\n",
        "\t\t\trow=row[:-1] # Exclude the carriage return\n",
        "\t\t\trow=row.split(\",\")\n",
        "\t\t\tkey=row[0]\n",
        "\t\t\tvals=row[1:]\n",
        "\t\t\t\n",
        "\t\t\tif schema=='GridParameters':\n",
        "\t\t\t\tif key in ['story_threshold','tfidf_maxdf']:\n",
        "\t\t\t\t\tfinalVals=list(float(n) for n in vals)\n",
        "\t\t\t\telif key in ['ngram_max','tfidf_mindf','max_length']:\n",
        "\t\t\t\t\tfinalVals=list(int(n) for n in vals)\n",
        "\t\t\t\telif key in ['lemma_conversion','tfidf_binary']:\n",
        "\t\t\t\t\tfinalVals=list(str2bool(n) for n in vals)\n",
        "\t\t\t\telif key in ['parts_of_speech']:\n",
        "\t\t\t\t\tlistlist=[]\n",
        "\t\t\t\t\tfor v in vals:\n",
        "\t\t\t\t\t\tlistlist.append(v.split(\"+\"))\n",
        "\t\t\t\t\tfinalVals=listlist\n",
        "\t\t\t\telif key in ['tfidf_norm','nlp_library']:\n",
        "\t\t\t\t\tfinalVals=vals\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tprint(key)\n",
        "\t\t\t\t\tprint(\"KEY ERROR\")\n",
        "\t\t\t\t\treturn\n",
        "\t\t\telif schema=='StoryMap':\n",
        "\t\t\t\tfinalVals=list(int(n) for n in vals)\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(schema)\n",
        "\t\t\t\tprint(\"SCHEMA ERROR\")\n",
        "\t\t\t\treturn\n",
        "\t\t\t\n",
        "\t\t\tgridParamDict[key]=finalVals\n",
        "\treturn gridParamDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0VLEb5X8ZPh",
        "colab_type": "text"
      },
      "source": [
        "### Load the story map from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XscYUHIfxduh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "storyMap,reportArticleList=setupStoryMapAndReportList(storyMapFileName='/content/drive/Shared drives/CMPE 257: Machine Learning/AlterusVera-Datasets/all-news-kaggle/storyMapForValidation1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STsKeSUD8ZPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "storyMap,reportArticleList=setupStoryMapAndReportList(None,None,None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjgqDiPH8ZPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if storyMap is None:\n",
        "  print('')\n",
        "else:\n",
        "  for story, articleList in storyMap.items():\n",
        "    print(story,\":\",articleList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC_2iWGg8ZPm",
        "colab_type": "text"
      },
      "source": [
        "### Story map with user requested specific article list\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NaWd52j8ZPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collapseRequestedArticleListIntoStoryList(requestedArticleList,storyMap):\n",
        "\n",
        "\tif storyMap==None:\n",
        "\t\tnewStoryMap={}\n",
        "\telse:\n",
        "\t\tnewStoryMap=storyMap.copy()\n",
        "\n",
        "\tfound=False\n",
        "\tfor story,articleListFromMap in newStoryMap.items():\n",
        "\t\tif len(articleListFromMap)==len(requestedArticleList):\n",
        "\t\t\ty=sum([x in articleListFromMap for x in requestedArticleList])\n",
        "\t\t\tif y==len(articleListFromMap):\n",
        "\t\t\t\tfound=True\n",
        "                \n",
        "\n",
        "\tif not found:\n",
        "\t\tnewStoryMap[requestedArticleList[0]]=requestedArticleList\n",
        "\tprint('newStoryMap',newStoryMap)\n",
        "\treturn newStoryMap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWh1uhoa8ZPo",
        "colab_type": "text"
      },
      "source": [
        "## Main Process Run\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvdNPztACExr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,currentParams in enumerate(parameterGrid):\n",
        "  if len(parameterGrid)>1:\n",
        "    print(\"Combination:\",i+1,\"of\",len(parameterGrid))\n",
        "    print(currentParams)\n",
        "\n",
        "  iterationStoryMap=collapseRequestedArticleListIntoStoryList(currentParams['article_id_list'],storyMap)\n",
        "  sentimentAnalyser=SentimentAnalyser(currentParams['sentiment_library'])\n",
        "\n",
        "  for story,articleList in iterationStoryMap.items():\n",
        "    articleSentScores={}\n",
        "    print(\"ANALYSING STORY:\",story,\"using\",currentParams['sentiment_library'])\n",
        "    print(\"Number of articles in story:\",len(articleList))\n",
        "\n",
        "    for article in articleList:\n",
        "      articleContent=articleDataFrame[articleDataFrame['id']==article].iloc[0]['content']\n",
        "      if currentParams['sentiment_sentences']!=None:\n",
        "\t      articleSentences=nl.sent_tokenize(articleContent)\n",
        "\t      textToAnalyse=' '.join(articleSentences[:currentParams['sentiment_sentences']])\t\n",
        "      else:\n",
        "\t      textToAnalyse=articleContent\n",
        "\n",
        "      results=sentimentAnalyser.generateResults(textToAnalyse)\n",
        "      articleSentScores[article]=sentimentAnalyser.getOverallArticleScore(results)\n",
        "    \n",
        "    # Sort and display results\n",
        "    sortedArticleSentScores=sorted(articleSentScores.items(), key=operator.itemgetter(1))\n",
        "    print(\"\\nArticle sentiments, most positive first:\")\n",
        "    for article in reversed(sortedArticleSentScores):\n",
        "\t      print(article[0],\":\", round(article[1],3),articleDataFrame[articleDataFrame['id']==article[0]].iloc[0]['publication'])\n",
        "\t      articleDataFrame.at[articleDataFrame[articleDataFrame['id']==article[0]].index[0],'score'] = round(article[1],3)\n",
        "    \n",
        "\n",
        "    print(\"\\nNEWS COVERAGE SCORE:\",round(computePopulationBalanceScoreHistoMean(articleSentScores,SentimentAnalyser),3),\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pe8t8gk8ZPq",
        "colab_type": "text"
      },
      "source": [
        "## Output inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZF2Fl1gX8ZPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articleDataFrame[articleDataFrame['id']==80103]#['content'].values[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q2Vd36k474q",
        "colab_type": "text"
      },
      "source": [
        "## Fakeness Label from News Coverage Score\n",
        "\n",
        "* positive sentiment: ``compound`` score >= 0.05 \n",
        "* neutral sentiment: (``compound`` score > -0.05) and (``compound`` score < 0.05) \n",
        "* negative sentiment: ``compound`` score <= -0.05 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1olJU6afBDUL",
        "colab_type": "text"
      },
      "source": [
        "### Using Sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJz0W-auq1p9",
        "colab_type": "text"
      },
      "source": [
        "###param try1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYY7WNQT5GSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articleDataFrame.loc[articleDataFrame.score >= 0.05, 'label_NewsCoverage'] = 1 # true news\n",
        "articleDataFrame.loc[((articleDataFrame.score > -0.05) & (articleDataFrame.score < 0.05 )), 'label_NewsCoverage'] = 2 # avg\n",
        "articleDataFrame.loc[articleDataFrame.score <= -0.05, 'label_NewsCoverage'] = 0 # fake news\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k-iFO6w4q_SK"
      },
      "source": [
        "###param try2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYjycSz_orUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articleDataFrame.loc[articleDataFrame.score >= 0.00, 'label_NewsCoverage'] = 1 # true news\n",
        "#articleDataFrame.loc[((articleDataFrame.score > -0.05) & (articleDataFrame.score < 0.05 )), 'label_NewsCoverage'] = 2 # avg\n",
        "articleDataFrame.loc[articleDataFrame.score < 0.00, 'label_NewsCoverage'] = 0 # fake news"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-txiu3ybUUUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Size of the Labels column\")\n",
        "print(articleDataFrame.groupby('label_NewsCoverage').size())\n",
        "articleDataFrame['label_NewsCoverage'].value_counts().plot(kind=\"bar\")\n",
        "plt.ylabel('label_NewsCoverage')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h8QSobw8sCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articleDataFrame[articleDataFrame.id.isin([120639, 80103, 25225, 21502, 57362, 120636])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f291Vy2nA-q0",
        "colab_type": "text"
      },
      "source": [
        "### Using Credibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_O8DT-XxVnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeLabelCredibility(articleDataFrame):\n",
        "    articleDataFrame.loc[articleDataFrame.credibility_score >= 0.80, 'label_credibility'] = 1 # true news\n",
        "    articleDataFrame.loc[articleDataFrame.credibility_score < 0.80, 'label_credibility'] = 0 # fake news\n",
        "\n",
        "    print(\"Size of the label_credibility column\")\n",
        "    print(articleDataFrame.groupby('label_credibility').size())\n",
        "    articleDataFrame['label_credibility'].value_counts().plot(kind=\"bar\")\n",
        "    plt.ylabel('label_credibility')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF1jhicIxyEK",
        "colab_type": "text"
      },
      "source": [
        "Commenting this computation for now, colab is crashing due to data load. So using other factors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8mzXFbzA_Nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#computeLabelCredibility(articleDataFrame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfXRpoV_ZRgT",
        "colab_type": "text"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyv0zC4ubDx0",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Test and Training Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-SDn9qICx_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_words( headlines ):               \n",
        "    headlines_onlyletters = re.sub(\"[^a-zA-Z]\", \" \",headlines) #Remove everything other than letters     \n",
        "    words = headlines_onlyletters.lower().split() #Convert to lower case, split into individual words    \n",
        "    stops = set(stopwords.words(\"english\"))  #Convert the stopwords to a set for improvised performance                 \n",
        "    meaningful_words = [w for w in words if not w in stops]   #Removing stopwords\n",
        "    return( \" \".join( meaningful_words )) #Joining the words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBvwVlmIFbr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articleDataFrame['label_NewsCoverage'] = articleDataFrame['label_NewsCoverage'].astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K07MTIXR4AB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareTestTraining(x_col, y_col, articleDataFrame):\n",
        "  #articleDataFrame['label_NewsCoverage'] = articleDataFrame['x_col'].astype('int')\n",
        "\n",
        "  # Separate the dataframe for input(X) and output variables(y)\n",
        "  #['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year','month', 'url', 'content', 'NYT summary', 'processed_content','score', 'label_NewsCoverage']\n",
        "  X = articleDataFrame[x_col]\n",
        "  Y = articleDataFrame[y_col]\n",
        "\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
        "  X_train = np.array(X_train);\n",
        "  X_test = np.array(X_test);\n",
        "  Y_train = np.array(Y_train);\n",
        "  Y_test = np.array(Y_test);\n",
        "  cleanHeadlines_train = [] #To append processed headlines\n",
        "  cleanHeadlines_test = [] #To append processed headlines\n",
        "  number_reviews_train = len(X_train) #Calculating the number of reviews\n",
        "  number_reviews_test = len(X_test) #Calculating the number of reviews\n",
        "\n",
        "  for i in range(0,number_reviews_train):\n",
        "    cleanHeadline = get_words(X_train[i]) #Processing the data and getting words with no special characters, numbers or html tags\n",
        "    cleanHeadlines_train.append( cleanHeadline )\n",
        "  for i in range(0,number_reviews_test):\n",
        "    cleanHeadline = get_words(X_test[i]) #Processing the data and getting words with no special characters, numbers or html tags\n",
        "    cleanHeadlines_test.append( cleanHeadline )\n",
        "\n",
        "  #vectorize = CountVectorizer(stop_words='english')\n",
        "  vectorize = CountVectorizer(analyzer = \"word\",max_features = 1700)    #(stop_words='english')\n",
        "  bagOfWords_train = vectorize.fit_transform(cleanHeadlines_train)\n",
        "  X_train = bagOfWords_train.toarray()\n",
        "  bagOfWords_test = vectorize.transform(cleanHeadlines_test)\n",
        "  X_test = bagOfWords_test.toarray()\n",
        "\n",
        "  #call autoML\n",
        "  callAutoML(X_train,Y_train,X_test,Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTfm8nCvpnyR",
        "colab_type": "text"
      },
      "source": [
        "### Auto ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS9Sk93Kon8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_folds = 10\n",
        "seed = 7\n",
        "scoring = 'accuracy'\n",
        "\n",
        "models = []\n",
        "models.append(('LR' , LogisticRegression()))\n",
        "models.append(('LDA' , LinearDiscriminantAnalysis()))\n",
        "#models.append(('KNN' , KNeighborsClassifier()))\n",
        "#models.append(('CART' , DecisionTreeClassifier()))\n",
        "#models.append(('NB' , GaussianNB()))\n",
        "#models.append(('SVM' , SVC()))\n",
        "models.append(('MNB', MultinomialNB()))\n",
        "models.append(('RF' , RandomForestClassifier(n_estimators=50)))\n",
        "#models.append(('XGBoost', XGBClassifier()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-b3R6GGpCqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def callAutoML(X_train,Y_train,X_test,Y_test):\n",
        "  results = []\n",
        "  names = []\n",
        "\n",
        "  '''\n",
        "  for name, model in models:\n",
        "      kfold = KFold(n_splits=num_folds, random_state=42)\n",
        "      cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "      results.append(cv_results)\n",
        "      names.append(name)\n",
        "      msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "      print(msg) '''\n",
        "\n",
        "  for name, model in models:\n",
        "      clf = model\n",
        "      clf.fit(X_train, Y_train)\n",
        "      Y_pred = clf.predict(X_test)\n",
        "      accu_score = accuracy_score(Y_test, Y_pred)\n",
        "      print(name + \": \" + str(accu_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmStI6du5c6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepareTestTraining('processed_content','label_NewsCoverage',articleDataFrame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpI_qjvF3bPC",
        "colab_type": "text"
      },
      "source": [
        "## Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6X5HtK22-Fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#articleDataFrame.to_csv (runParams['output_file'][0], index = False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQf_z9AgjH09",
        "colab_type": "text"
      },
      "source": [
        "# Description and Insight\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5A5YrdlmC50",
        "colab_type": "text"
      },
      "source": [
        "## News Coverage\n",
        "\n",
        "* This factor investigates news coverage of the same story in different media outlets. Later this factor classified model will be used to measure fakeness in data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OSWDy4JjNUk",
        "colab_type": "text"
      },
      "source": [
        "## Data Narrative\n",
        "\n",
        "* In this colab I have implement the different computation methods using different metrics and techniques to compute news coverage. This data has news from different popular media outlets from years of 2016 and July 2017\n",
        "* This dataset has 50k rows and 13 columns including news with published and publishing timestamp. To clarify some terms:\n",
        ">> * ARTICLE - a single article printed by one news publication\n",
        ">> * STORY - the underlying event that an article is in reference to\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCG0nTFRlupX",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "* Preprocessing and data clean up is done using NLTK to perform following NLP preprocessing on data. \n",
        " \n",
        ">> * Removing stopwords\n",
        ">> * Remove special character\n",
        ">> * Remove punctuations\n",
        ">> * Lowercase \n",
        ">> * Stemming \n",
        ">> * Lemmatization\n",
        ">> * Removing junk and \"briefing\" articles\n",
        ">> * Removing Daily Summary articles\n",
        ">> * Removing non-ASCII characters\n",
        ">> * Parts-of-speech filtering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5EL1hLpnO8G",
        "colab_type": "text"
      },
      "source": [
        "## Implementation Details\n",
        " \n",
        "* In the first colab First story clusters have been determined. The individual articles within each story are analyzed to measure neutrality. For this, I have used Vadar library to get a sentiment score. Sentiment is aggregated to compute the coverage of the story also.  \n",
        " \n",
        "* Next metric is measuring the 'balance' of coverage. It is done by identifying the top point of views( key words). Each point on the sentiment scale represents a distinct point of view. \n",
        " \n",
        "* These two terms are then averaged to measure how well balanced\n",
        "the overall coverage of the story is. A perfect score for the aggregate coverage of a story is 1.\n",
        " \n",
        "* As this data set is unlabelled, so I have to label this data to implement classification model. I have a threshold to identify 'good/bad' [1/0] coverage based on news coverage score. This value is stored in 'label_NewsCoverage' column. I have fine tuned the threshold multiple times. \n",
        " \n",
        "* Finally I have used autoML to try different classification algorithms to compare accuracy. The Logistic Regression Model has performed well with 75% accuracy. Here are the rest of the results: \n",
        "\n",
        "Factor | Algorithm | Accuracy\n",
        "--- | --- | -----\n",
        "News Coverage | Logistic Regression  | 75%\n",
        "News Coverage | Linear Discriminant Analysis​(LDA) | 72%\n",
        "News Coverage | Multinomial Bayes | 70%\n",
        "News Coverage | Random Forest  | 72%\n",
        "\n",
        " \n",
        "* **Future Work and Challenges** : \n",
        " To compare and compute news coverage I have thought of some other terms also,eg. biasness, readability, Dale-Chall Readability Test, credibility. In fact I have implemented credibility also in this colab. But I could not include this score because the colab is crashing for 50K data with all these computations.  So as a part of an upcoming task, I would like to implement some of these features to measure news coverage from different perspectives with better classification accuracy. \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th6KsW9-i5xM",
        "colab_type": "text"
      },
      "source": [
        "# Reference\n",
        "\n",
        "1. https://towardsdatascience.com/machine-learning-versus-the-news-3b5b479d8e6a\n",
        "2. https://www.kaggle.com/arjunchandrasekhara/news-classification\n",
        "3. https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183\n",
        "4. https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
        "5. https://github.com/javedsha/text-classification\n",
        "6. https://github.com/sharma-vidhi/Fake-News-Detection\n",
        "7. https://www.kaggle.com/thebrownviking20/k-means-clustering-of-1-million-headlines"
      ]
    }
  ]
}