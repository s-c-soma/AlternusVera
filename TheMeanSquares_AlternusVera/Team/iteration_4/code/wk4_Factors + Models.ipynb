{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Factors + Models.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"B92JlvLCEZ_k","colab_type":"text"},"source":["# 10 Factors on Liar-Liar Dataset"]},{"cell_type":"code","metadata":{"id":"iClfDWdoZx1S","colab_type":"code","outputId":"1c1c2725-acfa-4624-a7eb-4db24f2093c3","executionInfo":{"status":"ok","timestamp":1589225735376,"user_tz":420,"elapsed":2496,"user":{"displayName":"Wasae Qureshi","photoUrl":"","userId":"03410788767210486856"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["import pandas as pd\n","import numpy as np\n","import csv\n","import gensim\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import StandardScaler\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import confusion_matrix\n","from nltk.stem.porter import PorterStemmer\n","from sklearn import metrics\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.pipeline import Pipeline\n","from nltk.corpus import stopwords\n","from string import punctuation\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","import nltk\n","import requests\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","import matplotlib.pyplot as plt\n","from scipy import sparse\n","# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n","# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6qS9zrvbYrJ4","colab_type":"code","outputId":"0e167994-2eda-4678-f64b-63e8b6ebe319","executionInfo":{"status":"error","timestamp":1589225751614,"user_tz":420,"elapsed":18720,"user":{"displayName":"Wasae Qureshi","photoUrl":"","userId":"03410788767210486856"}},"colab":{"base_uri":"https://localhost:8080/","height":548}},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"-U-E5cFKRsPB","colab_type":"text"},"source":["### Factor 1: Sentiment Analysis"]},{"cell_type":"code","metadata":{"id":"zeTQ8H8VR8mN","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem.porter import *\n","from sklearn import metrics\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class SentimentAnalysis():\n","\n","    def __init__(self):        \n","\n","        columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\"]\n","        dataTrain = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/sentiment/train_sentiment.csv', sep=',', header=None, names = columnNames)\n","        dataTest = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/sentiment/test_sentiment.csv', sep=',', header=None, names = columnNames)\n","\n","        #dropping columns\n","        columnsToRemove = ['jsonid', 'label', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector']\n","        dataTrain = dataTrain.drop(columns=columnsToRemove)\n","        dataTest = dataTest.drop(columns=columnsToRemove)\n","        #dataTrain = dataTrain.loc[1:] \n","        #dataTest = dataTest.loc[1:]\n","    \n","    \n","        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n","\n","        self.logR_pipeline = Pipeline([\n","                ('LogRCV', tfidfV),\n","                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n","                ])\n","\n","        self.logR_pipeline.fit(dataTrain['headline_text'],dataTrain['vader_polarity'])\n","        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n","        score = metrics.accuracy_score(dataTest['vader_polarity'], predicted_LogR)\n","        print(\"Sentiment Analysis Model Trained - accuracy:   %0.6f\" % score)\n","        \n","\n","    def predict(self, text):\n","        predicted = self.logR_pipeline.predict([text])\n","        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n","        return bool(predicted), float(predicedProb)\n","    \n","    \n","sa = SentimentAnalysis()\n","sa.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pK11E9x8SX2R","colab_type":"code","colab":{}},"source":["SentimentAnalysis = SentimentAnalysis()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDx-o_m3SZ_M","colab_type":"code","colab":{}},"source":["def DATAMINERS_getSentimentAnalysisScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n","    #print(clickBait.predict(\"Should You bring the money now\"))\n","    binaryValue, probValue = SentimentAnalysis.predict(text)\n","    return (float(probValue))\n","\n","print(DATAMINERS_getSentimentAnalysisScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fsI9bF7Rvfc","colab_type":"text"},"source":["### Factor 2: LDA Topic Modelling"]},{"cell_type":"code","metadata":{"id":"Xo-fm7duSc08","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem.porter import *\n","from sklearn import metrics\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class LDATopicModelling():\n","\n","    def __init__(self):        \n","\n","        columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\", \"index\", \"topic_number\", \"lda_score\", \"topic_top_words\"]\n","        dataTrain = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/lda/train_lda.csv', sep=',', header=None, names = columnNames)\n","        dataTest = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/lda/test_lda.csv', sep=',', header=None, names = columnNames)\n","\n","        #dropping columns\n","        columnsToRemove = ['jsonid', 'label', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector', 'sentiment_vector','vader_polarity', 'sentiment_score', 'index']\n","        dataTrain = dataTrain.drop(columns=columnsToRemove)\n","        dataTest = dataTest.drop(columns=columnsToRemove)\n","        #dataTrain = dataTrain.loc[1:] \n","        #dataTest = dataTest.loc[1:]\n","    \n","    \n","        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n","\n","        self.logR_pipeline = Pipeline([\n","                ('LogRCV', tfidfV),\n","                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n","                ])\n","\n","        self.logR_pipeline.fit(dataTrain['headline_text'],dataTrain['topic_number'])\n","        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n","        score = metrics.accuracy_score(dataTest['topic_number'], predicted_LogR)\n","        print(\"LDA Topic Model Trained - accuracy:   %0.6f\" % score)\n","        \n","\n","    def predict(self, text):\n","        predicted = self.logR_pipeline.predict([text])\n","        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n","        return bool(predicted), float(predicedProb)\n","    \n","    \n","# lda = LDATopicModelling()\n","# lda.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fv6hIScRSkBc","colab_type":"code","colab":{}},"source":["ldaTopicModelling = LDATopicModelling()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxMzUsY8Snef","colab_type":"code","colab":{}},"source":["def DATAMINERS_getLDATopicModellingScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n","    #print(clickBait.predict(\"Should You bring the money now\"))\n","    binaryValue, probValue = ldaTopicModelling.predict(text)\n","    return (float(probValue))\n","\n","print(DATAMINERS_getLDATopicModellingScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kU8raVjRy7l","colab_type":"text"},"source":["### Factor 3: Sensationalism"]},{"cell_type":"code","metadata":{"id":"ft8oyQ06SooK","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem.porter import *\n","from sklearn import metrics\n","import warnings\n","warnings.filterwarnings('ignore')\n","#from sklearn.preprocessing import Imputer\n","from sklearn.impute import SimpleImputer\n","\n","\n","class SensationalismFeature():\n","\n","    def __init__(self):        \n","\n","        columnNames = [\"jsonid\", \"headline_text\", \"clean\", \"sensational_score\"]\n","        dataTrain = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/sensationalism/train_sensationalism.csv', sep=',', header=None, names = columnNames)\n","        dataTest = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/sensationalism/test_sensationalism.csv', sep=',', header=None, names = columnNames)\n","        #dataTrain = dataTrain.loc[1:]\n","        #dataTest = dataTest.loc[1:]\n","            \n","        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df = 30, use_idf = True, smooth_idf = True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n","\n","        self.logR_pipeline = Pipeline([\n","                ('LogRCV', tfidfV),\n","                ('LogR_clf', LogisticRegression(solver='liblinear', C = 32/100))\n","                ])\n","\n","        self.logR_pipeline.fit(dataTrain['headline_text'], dataTrain['sensational_score'].astype(str))\n","        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n","        score = metrics.accuracy_score(dataTest['sensational_score'].astype(str), predicted_LogR)\n","        print(\"Sensationalism Model Trained - accuracy:   %0.6f\" % score)\n","        \n","\n","    def predict(self, text):\n","        predicted = self.logR_pipeline.predict([text])\n","        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n","        return bool(predicted), float(predicedProb)\n","    \n","    \n","sf = SensationalismFeature()\n","# sf.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZ6qPiuuSvGe","colab_type":"code","colab":{}},"source":["def DATAMINERS_getSensationalismScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n","    #print(clickBait.predict(\"Should You bring the money now\"))\n","    binaryValue, probValue = sf.predict(text)\n","    return (float(probValue*100))\n","\n","print(DATAMINERS_getSensationalismScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jyxQ6UJOR1c9","colab_type":"text"},"source":["### Factor 4: Political Affiliation"]},{"cell_type":"code","metadata":{"id":"hOwwwsnNS1WM","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem.porter import *\n","from sklearn import metrics\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier\n","#from sklearn.cross_validation import KFold\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix, f1_score, classification_report\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import learning_curve\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import average_precision_score\n","import matplotlib.pyplot as plt\n","import itertools\n","\n","class PartyAffiliation():\n","    \n","    # API to check whether the subject(Headline) is present in the \n","    # - democrats most used words if the party affiliation is democrat\n","    # - republicans most used words if the part affiliation is republican\n","    def partyAffiliationFromHeadline(self, r):\n","        v = r['subject_str']\n","        p = r['party_str']\n","        if (p =='democrat'):\n","            s2 = set(self.countDemV.get_feature_names())\n","        if (p =='republican'):\n","            s2 = set(self.countRepV.get_feature_names())\n","        if (p != 'democract' and p !='republican'):\n","            return 1 #'true'        \n","        if set(v).intersection(s2):\n","            return 1 #'true'\n","        else:\n","            return 0 #'false'\n","\n","    #API to convert true, mostly-true and half-true to true\n","    # false, barely-true and pants-fire to false\n","    def convertMulticlassToBinaryclass(self, r):\n","        v = r['label']\n","        if (v == 'true'):\n","            return 1 #'true'\n","        if (v == 'mostly-true'):\n","            return 1 #'true'\n","        if (v == 'half-true'):\n","            return 1 #'true'\n","        if (v == 'barely-true'):\n","            return 0 #'false'\n","        if (v == 'false'):\n","            return 0 #'false'\n","        if (v == 'pants-fire'):\n","            return 0 #'false'\n","            \n","            \n","            \n","    def plot_confusion_matrix(self, cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","\n","        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","        plt.title(title)\n","        plt.colorbar()\n","        tick_marks = np.arange(len(classes))\n","        plt.xticks(tick_marks, classes, rotation=45)\n","        plt.yticks(tick_marks, classes)\n","\n","        if normalize:\n","            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","            print(\"Normalized confusion matrix\")\n","        else:\n","            print('Confusion matrix, without normalization')\n","\n","        thresh = cm.max() / 2.\n","        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","            plt.text(j, i, cm[i, j],\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","        plt.tight_layout()\n","        plt.ylabel('True label')\n","        plt.xlabel('Predicted label')       \n","            \n","    \n","    def __init__(self):        \n","\n","        columnNamesPar = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n","        dataTrainPar = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/liar-liar dataset/train.tsv', sep='\\t', header=None, names = columnNamesPar)\n","        dataValidatePar = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/liar-liar dataset/valid.tsv', sep='\\t', header=None, names = columnNamesPar)\n","        dataTestPar = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/liar-liar dataset/test.tsv', sep='\\t', header=None, names = columnNamesPar)\n","        \n","    \n","        # Remove unwanted columns in the dataset\n","        columnsToRemovePar = ['id', 'speaker', 'context','speaker_job_title', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']\n","        dataTrainPar = dataTrainPar.drop(columns=columnsToRemovePar)\n","        dataValidatePar = dataValidatePar.drop(columns=columnsToRemovePar)\n","        dataTestPar = dataTestPar.drop(columns=columnsToRemovePar)\n","        \n","        # convert the labels to true and false only\n","        dataTrainPar['label'] = dataTrainPar.apply(self.convertMulticlassToBinaryclass, axis=1)\n","        dataValidatePar['label'] = dataValidatePar.apply(self.convertMulticlassToBinaryclass, axis=1)\n","        dataTestPar['label'] = dataTestPar.apply(self.convertMulticlassToBinaryclass, axis=1)\n","        \n","        # display all the party affiliations and show the count of each party \n","#         dataTrainPar.groupby('party_affiliation').count()[['state_info']].rename(\n","#         columns={'state_info': 'count'}).sort_values(\n","#         'count', ascending=False).reset_index().plot.bar(\n","#         x='party_affiliation', y='count', figsize=(16, 10), fontsize=18);\n","        \n","        # As we are considering only democrat, republican and none (top 3 party affiliations),\n","        # ignoring other party affiliations\n","        rowsToRemove = ['Moderate', 'activist', 'business-leader', 'columnist', 'constitution-party', 'democratic-farmer-labor', 'education-official', 'government-body', 'green', 'independent', 'journalist', 'labor-leader', 'liberal-party-canada', 'libertarian', 'nan', 'newsmaker', 'ocean-state-tea-party-action', 'organization', 'state-official', 'talk-show-host', 'tea-party-member']\n","\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'Moderate']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'activist']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'business-leader']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'columnist']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'constitution-party']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'democratic-farmer-labor']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'education-official']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'government-body']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'green']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'independent']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'journalist']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'labor-leader']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'liberal-party-canada']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'libertarian']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'nan']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'newsmaker']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'ocean-state-tea-party-action']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'organization']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'state-official']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'talk-show-host']\n","        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'tea-party-member']\n","\n","        # As we are considering only democrat, republican and none (top 3 party affiliations),\n","        # ignoring other party affiliations\n","\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'Moderate']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'activist']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'business-leader']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'columnist']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'constitution-party']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'democratic-farmer-labor']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'education-official']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'government-body']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'green']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'independent']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'journalist']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'labor-leader']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'liberal-party-canada']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'libertarian']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'nan']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'newsmaker']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'ocean-state-tea-party-action']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'organization']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'state-official']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'talk-show-host']\n","        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'tea-party-member']\n","\n","        \n","        dataTrainPar['party_str'] = dataTrainPar['party_affiliation'].astype(str)\n","        dataTestPar['party_str'] = dataTestPar['party_affiliation'].astype(str)\n","        \n","\n","        #predicting truth level\n","#        dataTrainPar.groupby('label').count()[['party_affiliation']].reset_index().plot.bar(x='label', y='party_affiliation')\n","        \n","        # get the most used democrat words\n","        self.countDemV = CountVectorizer(stop_words='english', min_df=40, max_df=80, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n","\n","        dataTrainDem= dataTrainPar\n","        dataTrainDem = dataTrainPar.loc[dataTrainPar['party_str'] == 'democrat']\n","        dem_count = self.countDemV.fit_transform(dataTrainDem['statement'].values)\n","        \n","        #get the republican most used words\n","        \n","        self.countRepV = CountVectorizer(stop_words='english', min_df=20, max_df=40, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n","        dataTrainRep= dataTrainPar\n","        dataTrainRep = dataTrainPar.loc[dataTrainPar['party_str'] == 'republican']\n","        rep_count = self.countRepV.fit_transform(dataTrainRep['statement'].values)\n","\n","        dataTestDem= dataTestPar\n","        dataTestDem = dataTestPar.loc[dataTestPar['party_str'] == 'democrat']\n","        \n","        dataTrainPar['subject_str'] = dataTrainPar['subject'].astype(str).str.split() \n","        dataTrainPar['label_str'] = dataTrainPar.apply(self.partyAffiliationFromHeadline, axis=1)\n","\n","        dataTestPar['subject_str'] = dataTestPar['subject'].astype(str).str.split() \n","        dataTestPar['label_str'] = dataTestPar.apply(self.partyAffiliationFromHeadline, axis=1)\n","\n","        dataTrainDem['subject_str'] = dataTrainDem['subject'].astype(str).str.split() \n","        dataTrainDem['label_str'] = dataTrainDem.apply(self.partyAffiliationFromHeadline, axis=1)\n","    \n","        dataTestDem['subject_str'] = dataTestDem['subject'].astype(str).str.split() \n","        dataTestDem['label_str'] = dataTestDem.apply(self.partyAffiliationFromHeadline, axis=1)\n","        \n","        \n","        self.model = LogisticRegression()\n","        self.model = self.model.fit(dataTrainPar['label_str'].values.reshape(-1, 1), dataTrainPar['label'].values)\n","        predicted_LogR = self.model.predict(dataTestPar['label_str'].values.reshape(-1, 1))\n","        score = metrics.accuracy_score(dataTestPar['label'], predicted_LogR)\n","        print(\"Party Affiliation Model Trained - accuracy:   %0.6f\" % score)\n","\n","    \n","    def predict(self, headline, party):\n","                \n","        #creating the dataframe with our text so we can leverage the existing code\n","        dfrme = pd.DataFrame(index=[0], columns=['subject', 'party_str'])\n","        dfrme['subject_str'] = headline\n","        dfrme['party_str'] = party        \n","\n","        dfrme['subject'] = headline\n","        dfrme['subject_str'] = dfrme['subject'].astype(str).str.split() \n","        dfrme['label_str'] = dfrme.apply(self.partyAffiliationFromHeadline, axis=1)\n","        \n","        x = dfrme['label_str'].values.reshape(-1, 1)\n","        predicted = self.model.predict(x)\n","        predicedProb = self.model.predict_proba(x)[:,1]\n","        return predicted, predicedProb\n","                    \n","    \n","##testing code\n","f = PartyAffiliation()\n","#pf.predict(\"Says the Annies List political group supports third-trimester abortions on demand\", \"republican\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_R3bvAvbS6bd","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","from sklearn.exceptions import DataConversionWarning\n","warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","def loadJsonFiles(directory, veracity):    \n","    shouldAppend = False\n","    for filename in os.listdir(directory):\n","        df2 = pd.read_json(directory + filename, lines=True)\n","        if (shouldAppend):\n","            df = df.append(df2, ignore_index=True, sort=True)      \n","        else:\n","            df = df2\n","        df['veracity'] = veracity\n","        shouldAppend = True\n","        \n","            \n","    # removing nan values    \n","    df['source'].fillna(\"\", inplace=True)\n","    for index, row in df.iterrows():\n","        if (type(row['authors']) == float):\n","            df.at[index, 'authors'] = []\n","\n","            \n","    #removing unnecessary columns\n","    df = df.drop(columns=['keywords','meta_data','movies', 'keywords', 'summary', 'publish_date','top_img'])\n","    return df\n","\n","def loadDataset():\n","    dataFake = loadJsonFiles('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/author-credibility/FakeNewsContent/', 0)\n","    dataReal = loadJsonFiles('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/author-credibility/RealNewsContent/', 1)\n","    return dataReal, dataFake\n","\n","dataFake, dataReal = loadDataset()\n","\n","dataTrainFake = dataFake[:100]\n","dataTrainReal = dataReal[:100]\n","dataTestFake = dataFake[101:]\n","dataTestReal = dataReal[101:]\n","\n","dataTest = dataTestFake.append(dataTestReal,ignore_index=True, sort=True)      \n","dataTrain = dataTrainFake.append(dataTrainReal,ignore_index=True, sort=True)    \n","dataAll = dataFake.append(dataReal, ignore_index=True, sort=True)      \n","dataAll.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6tqFwVniojo","colab_type":"code","colab":{}},"source":["partyAffiliation = PartyAffiliation()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRXCz_tRS-Wr","colab_type":"code","colab":{}},"source":["def DATAMINERS_getPartyAffiliationScore(headline, partyName): # return between 0 and 1, being 0 = True,  1 = Fake\n","    if ( (headline == \"\") | (partyName == \"\") ):\n","        return 0\n","    binaryValue, probValue = partyAffiliation.predict(headline, partyName)\n","    return (1 - float(probValue))\n","\n","print(DATAMINERS_getPartyAffiliationScore(\"Says the Annies List political group supports third-trimester abortions on demand\", \"republican\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_dv2ELAR44k","colab_type":"text"},"source":["### Factor 5: Clickbait"]},{"cell_type":"code","metadata":{"id":"7ItVCXBlTCbi","colab_type":"code","colab":{}},"source":["class Clickbait():\n","    \n","    question_words = ['who', 'whos', 'whose', 'what', 'whats', 'whatre', 'when', 'whenre', 'whens', 'couldnt',\n","            'where', 'wheres', 'whered', 'why', 'whys', 'can', 'cant', 'could', 'will', 'would', 'is',\n","            'isnt', 'should', 'shouldnt', 'you', 'your', 'youre', 'youll', 'youd', 'here', 'heres',\n","            'how', 'hows', 'howd', 'this', 'are', 'arent', 'which', 'does', 'doesnt']\n","\n","    contractions = ['tis', 'aint', 'amnt', 'arent', 'cant', 'couldve', 'couldnt', 'couldntve',\n","                    'didnt', 'doesnt', 'dont', 'gonna', 'gotta', 'hadnt', 'hadntve', 'hasnt',\n","                    'havent', 'hed', 'hednt', 'hedve', 'hell', 'hes', 'hesnt', 'howd', 'howll',\n","                    'hows', 'id', 'idnt', 'idntve', 'idve', 'ill', 'im', 'ive', 'ivent', 'isnt',\n","                    'itd', 'itdnt', 'itdntve', 'itdve', 'itll', 'its', 'itsnt', 'mightnt',\n","                    'mightve', 'mustnt', 'mustntve', 'mustve', 'neednt', 'oclock', 'ol', 'oughtnt',\n","                    'shant', 'shed', 'shednt', 'shedntve', 'shedve', 'shell', 'shes', 'shouldve',\n","                    'shouldnt', 'shouldntve', 'somebodydve', 'somebodydntve', 'somebodys',\n","                    'someoned', 'someonednt', 'someonedntve', 'someonedve', 'someonell', 'someones',\n","                    'somethingd', 'somethingdnt', 'somethingdntve', 'somethingdve', 'somethingll',\n","                    'somethings', 'thatll', 'thats', 'thatd', 'thered', 'therednt', 'theredntve',\n","                    'theredve', 'therere', 'theres', 'theyd', 'theydnt', 'theydntve', 'theydve',\n","                    'theydvent', 'theyll', 'theyontve', 'theyre', 'theyve', 'theyvent', 'wasnt',\n","                    'wed', 'wedve', 'wednt', 'wedntve', 'well', 'wontve', 'were', 'weve', 'werent',\n","                    'whatd', 'whatll', 'whatre', 'whats', 'whatve', 'whens', 'whered', 'wheres',\n","                    'whereve', 'whod', 'whodve', 'wholl', 'whore', 'whos', 'whove', 'whyd', 'whyre',\n","                    'whys', 'wont', 'wontve', 'wouldve', 'wouldnt', 'wouldntve', 'yall', 'yalldve',\n","                    'yalldntve', 'yallll', 'yallont', 'yallllve', 'yallre', 'yallllvent', 'yaint',\n","                    'youd', 'youdve', 'youll', 'youre', 'yourent', 'youve', 'youvent']\n","    \n","    def process_text(self, text):\n","        result = text.replace('/', '').replace('\\n', '')\n","        result = re.sub(r'[1-9]+', 'number', result)\n","        result = re.sub(r'(\\w)(\\1{2,})', r'\\1', result)\n","        result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n","        result = ''.join(t for t in result if t not in punctuation)\n","        result = re.sub(r' +', ' ', result).lower().strip()\n","        return result\n","    \n","    def cnt_stop_words(self, text):\n","        s = text.split()\n","        num = len([word for word in s if word in self.stop])\n","        return num\n","\n","    def num_contract(self, text):\n","        s = text.split()\n","        num = len([word for word in s if word in self.contractions])\n","        return num\n","\n","    def question_word(self, text):\n","        s = text.split()\n","        if s[0] in self.question_words:\n","            return 1\n","        else:\n","            return 0\n","\n","    def part_of_speech(self, text):\n","        s = text.split()\n","        nonstop = [word for word in s if word not in self.stop]\n","        pos = [part[1] for part in nltk.pos_tag(nonstop)]\n","        pos = ' '.join(pos)\n","        return pos\n","\n","\n","    def __init__(self):        \n","        df_ycb = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/clickbait/clickbait_data.txt', sep=\"\\n\", header=None, names=['text'])\n","        df_ycb['clickbait'] = 1\n","\n","        df_ncb = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/clickbait/non_clickbait_data.txt', sep=\"\\n\", header=None, names=['text'])\n","        df_ncb['clickbait'] = 0\n","\n","        df = df_ycb.append(df_ncb, ignore_index=True).reset_index(drop=True)\n","\n","        \n","\n","       \n","        self.stop = stopwords.words('english')\n","       \n","        # Creating some latent variables from the data\n","        df['text']     = df['text'].apply(self.process_text)\n","        df['question'] = df['text'].apply(self.question_word)\n","\n","        df['num_words']       = df['text'].apply(lambda x: len(x.split()))\n","        df['part_speech']     = df['text'].apply(self.part_of_speech)\n","        df['num_contract']    = df['text'].apply(self.num_contract)\n","        df['num_stop_words']  = df['text'].apply(self.cnt_stop_words)\n","        df['stop_word_ratio'] = df['num_stop_words']/df['num_words']\n","        df['contract_ratio']  = df['num_contract']/df['num_words']\n","\n","        \n","        df.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n","\n","        df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)\n","\n","        self.tfidf = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n","                                   analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,5),\n","                                   use_idf=1, smooth_idf=1, sublinear_tf=1)\n","\n","        X_train_text = self.tfidf.fit_transform(df_train['text'])\n","        X_test_text  = self.tfidf.transform(df_test['text'])\n","\n","        self.cvec = CountVectorizer()\n","\n","        X_train_pos = self.cvec.fit_transform(df_train['part_speech'])\n","        X_test_pos  = self.cvec.transform(df_test['part_speech'])\n","\n","        self.scNoMean = StandardScaler(with_mean=False)  # we pass with_mean=False to preserve the sparse matrix\n","        X_train_pos_sc = self.scNoMean.fit_transform(X_train_pos)\n","        X_test_pos_sc  = self.scNoMean.transform(X_test_pos)\n","\n","        X_train_val = df_train.drop(['clickbait','text','part_speech'], axis=1).values\n","        X_test_val  = df_test.drop(['clickbait','text','part_speech'], axis=1).values\n","\n","        self.sc = StandardScaler()\n","        X_train_val_sc = self.sc.fit(X_train_val).transform(X_train_val)\n","        X_test_val_sc  = self.sc.transform(X_test_val)\n","\n","        y_train = df_train['clickbait'].values\n","        y_test  = df_test['clickbait'].values\n","\n","\n","\n","        X_train = sparse.hstack([X_train_val_sc, X_train_text, X_train_pos_sc]).tocsr()\n","        X_test  = sparse.hstack([X_test_val_sc, X_test_text, X_test_pos_sc]).tocsr()\n","\n","        self.model = LogisticRegression(penalty='l2', C=98.94736842105263)\n","        self.model = self.model.fit(X_train, y_train)\n","        \n","        predicted_LogR = self.model.predict(X_test)\n","        score = metrics.accuracy_score(y_test, predicted_LogR)\n","        print(\"Clickbait Model Trained - accuracy:   %0.6f\" % score)\n","\n","#     predict = model.predict(X_test)\n","#     print(classification_report(y_test, predict))\n","\n","\n","    def predict(self, text):\n","        #creating the dataframe with our text so we can leverage the existing code\n","        dfrme = pd.DataFrame(index=[0], columns=['text'])\n","        dfrme['text'] = text\n","\n","        #processing text\n","        dfrme['text']     = dfrme['text'].apply(self.process_text)\n","\n","        #adding latent variables\n","        dfrme['question'] = dfrme['text'].apply(self.question_word)\n","        dfrme['num_words']       = dfrme['text'].apply(lambda x: len(x.split()))\n","        dfrme['part_speech']     = dfrme['text'].apply(self.part_of_speech)\n","        dfrme['num_contract']    = dfrme['text'].apply(self.num_contract)\n","        dfrme['num_stop_words']  = dfrme['text'].apply(self.cnt_stop_words)\n","        dfrme['stop_word_ratio'] = dfrme['num_stop_words']/dfrme['num_words']\n","        dfrme['contract_ratio']  = dfrme['num_contract']/dfrme['num_words']\n","\n","        #removing latent variables that have high colinearity with other features\n","        dfrme.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n","\n","\n","        Xtxt_val  = dfrme.drop(['text','part_speech'], axis=1).values\n","        Xtxt_val_sc  = self.sc.transform(Xtxt_val)\n","\n","        Xtxt_text  = self.tfidf.transform(dfrme['text'])\n","\n","        Xtxt_pos  = self.cvec.transform(dfrme['part_speech'])\n","        Xtxt_pos_sc  = self.scNoMean.transform(Xtxt_pos)\n","        Xtxt  = sparse.hstack([Xtxt_val_sc, Xtxt_text, Xtxt_pos_sc]).tocsr()\n","\n","        predicted = self.model.predict(Xtxt)\n","        predicedProb = self.model.predict_proba(Xtxt)[:,1]\n","        return predicted, predicedProb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WO0s-JnmTHNK","colab_type":"code","colab":{}},"source":["# from ipynb.fs.full.m_clickbait import Clickbait\n","clickBait = Clickbait()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9juCMuyUTJ2k","colab_type":"code","colab":{}},"source":["def DATAMINERS_getClickbaitScore(headline): # return between 0 and 1, being 0 = True,  1 = Fake\n","    if (headline == \"\"):\n","        return 0\n","    binaryValue, probValue = clickBait.predict(headline)\n","    return float(probValue)\n","\n","print(DATAMINERS_getClickbaitScore(\"Should You bring the money now\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_h8ZkOCTU6Ep","colab_type":"text"},"source":["### Feature 6: Spam Score"]},{"cell_type":"code","metadata":{"id":"0u8tOLMDUxSX","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","class SpamScoreFeature():\n","    def __init__(self): \n","        #load the dataset\n","        columnNames = [\"encoded_label\", \"headline_text\", \"sensational_vector\"]\n","        dataTrain = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/spam/train_sensational_feature.csv', sep=',', header=None, names = columnNames)\n","        dataTest = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/spam/test_sensational_feature.csv', sep=',', header=None, names = columnNames)\n","        dataTrain = dataTrain.loc[1:]\n","        dataTest = dataTest.loc[1:]\n","        \n","        #load the spam dictionary\n","        spam_dict = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/spam/spam_dict.csv', usecols= [1], names = ['spamword'], encoding='latin-1', error_bad_lines=False)\n","        spam_dict = spam_dict.fillna(0)\n","        spam_dict = spam_dict.iloc[1:]\n","        spam_dict = spam_dict.drop_duplicates()\n","\n","        # spam_dict.head(5)\n","        #Count vector for train data\n","        spamcountV = CountVectorizer(vocabulary=list(set(spam_dict['spamword'])))\n","        train_count = spamcountV.fit_transform(dataTrain['headline_text'])\n","       \n","   \n","        self.logR_pipeline = Pipeline([\n","            ('NBCV',spamcountV),\n","            ('nb_clf',MultinomialNB())])\n","\n","        self.logR_pipeline.fit(dataTrain['headline_text'], dataTrain['encoded_label'])\n","        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n","        score = metrics.accuracy_score(dataTest['encoded_label'], predicted_LogR)\n","        print(\"Spam Score Model Trained - accuracy:   %0.6f\" % score)\n","        \n","\n","    def predict(self, text):\n","        predicted = self.logR_pipeline.predict([text])\n","        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n","        return bool(predicted), float(predicedProb)\n","    \n","    \n","spamscore = SpamScoreFeature()\n","spamscore.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xD5bKNrlVMM5","colab_type":"code","colab":{}},"source":["def DATAMINERS_getSpamScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n","    #print(clickBait.predict(\"Should You bring the money now\"))\n","    binaryValue, probValue = spamscore.predict(text)\n","    return (float(probValue))\n","\n","print(DATAMINERS_getSpamScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQyC5MFhVM-U","colab_type":"text"},"source":["### Feature 7: Author Credibility"]},{"cell_type":"code","metadata":{"id":"qvSevATwVOut","colab_type":"code","colab":{}},"source":["dataAllAuthorsVeracity = dataAll.copy()\n","\n","fakeZero = 0\n","fakeOne = 0\n","falseMoreThanOne = 0\n","trueZero = 0\n","trueOne = 0\n","trueMoreThanOne = 0\n","for index, row in dataAllAuthorsVeracity.iterrows():\n","    authorsCount = len(row['authors'])\n","    dataAllAuthorsVeracity.at[index, 'authors_count'] = len(row['authors'])\n","    if (authorsCount == 0):\n","        if (row['veracity'] == 1):\n","            trueZero += 1\n","        else:\n","            fakeZero += 1\n","    elif (authorsCount == 1):\n","        if (row['veracity'] == 1):\n","            trueOne += 1\n","        else:\n","            fakeOne += 1\n","    elif (authorsCount > 1):\n","        if (row['veracity'] == 1):\n","            trueMoreThanOne += 1\n","        else:\n","            falseMoreThanOne += 1\n","\n","print(\"trueZeroAuthors=\", trueZero)\n","print(\"fakeZeroAuthors=\", fakeZero)\n","print(\"trueOneAuthors=\", trueOne)\n","print(\"fakeOneAuthors=\", fakeOne)\n","print(\"trueMoreThanOneAuthors=\", trueMoreThanOne)\n","print(\"fakeMoreThanOneAuthors=\", falseMoreThanOne)\n","\n","columnsToRemove = ['authors', 'canonical_link', 'images', 'source','url', 'text', 'title']\n","dataAllAuthorsVeracity = dataAllAuthorsVeracity.drop(columns=columnsToRemove)\n","dataAllAuthorsVeracity.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3H3rv5cnVRkp","colab_type":"code","colab":{}},"source":["dataTrainAuthorsVeracity = dataTrain.copy()\n","dataTestAuthorsVeracity = dataTest.copy()\n","\n","for index, row in dataTrainAuthorsVeracity.iterrows():\n","    dataTrainAuthorsVeracity.at[index, 'authors_count'] = len(row['authors'])\n","\n","for index, row in dataTestAuthorsVeracity.iterrows():\n","    dataTestAuthorsVeracity.at[index, 'authors_count'] = len(row['authors'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLdQb3W6VUIg","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","X_train = dataTrainAuthorsVeracity['authors_count'].values.reshape(-1, 1)\n","Y_train = dataTrainAuthorsVeracity['veracity'].values\n","X_test = dataTestAuthorsVeracity['authors_count'].values.reshape(-1, 1)\n","Y_test = dataTestAuthorsVeracity['veracity'].values.reshape(-1, 1)\n","\n","\n","from sklearn import linear_model\n","logClassifierAuthorsCount = linear_model.LogisticRegression(solver='liblinear', C=1, random_state=111)\n","logClassifierAuthorsCount.fit(X_train, Y_train)\n","predicted = logClassifierAuthorsCount.predict(X_test)\n","\n","from sklearn import metrics\n","print(\"accuracy=\", metrics.accuracy_score(Y_test, predicted))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yA7Bs_0JVY90","colab_type":"code","colab":{}},"source":["def DATAMINERS_getAuthorScore(numAuthors): # return between 0 and 1, being 0 = True,  1 = Fake\n","    x = np.array(numAuthors).reshape(-1, 1)\n","    predicted = logClassifierAuthorsCount.predict(x)\n","    predicedProbTrue = logClassifierAuthorsCount.predict_proba(x)[:,1]\n","    #return int(predicted), float(predicedProb)\n","    return 1 - float(predicedProbTrue)\n","\n","print(DATAMINERS_getAuthorScore(4))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Er_UDTgEVaN7","colab_type":"text"},"source":["### Feature 8 Source Reputation"]},{"cell_type":"code","metadata":{"id":"ZFfut9FmVbh3","colab_type":"code","colab":{}},"source":["import pandas as pd\n","dataFakeNewsSites = pd.read_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/source_reputation/politifact-fakenews-sites.csv\")\n","dataFakeNewsSites.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixli-9SXVdU6","colab_type":"code","colab":{}},"source":["dataFakeNewsSites['type of site'].unique()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"goA0y8y_Ve1v","colab_type":"code","colab":{}},"source":["for index, row in dataFakeNewsSites.iterrows():\n","    score = 1\n","    if (row['type of site'] == 'some fake stories'):\n","        score = 0.5\n","    dataFakeNewsSites.at[index, 'fake_score'] = score\n","\n","dataFakeNewsSites.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOfxII_RVgit","colab_type":"code","colab":{}},"source":["def DATAMINERS_getSourceReputationScore(source): # return between 0 and 1, being 0 = True,  1 = Fake\n","    if (source == \"\"):\n","        return 0\n","    d = dataFakeNewsSites[dataFakeNewsSites['site name'].str.match(source)]\n","    if (d['fake_score'].empty):\n","        return 0\n","    return int(d['fake_score'].values)\n","\n","\n","DATAMINERS_getSourceReputationScore('24wpn')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X4YzlfvRVkNv","colab_type":"text"},"source":["### Feature 9 : Content Length"]},{"cell_type":"code","metadata":{"id":"AB2OmMuGV3t5","colab_type":"code","colab":{}},"source":["dataAllBodyLength = dataAll.copy()\n","for index, row in dataAllBodyLength.iterrows():\n","    textLength = len(row['text'])\n","    dataAllBodyLength.at[index, 'text_length'] = textLength\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.linear_model import LinearRegression\n","linearRegressionBodyLength = LinearRegression(fit_intercept=True)\n","\n","A = np.array(list(dataAllBodyLength.text_length))\n","B = np.array(list(dataAllBodyLength.veracity))\n","\n","linearRegressionBodyLength.fit(A[:, np.newaxis], B)\n","\n","xfit = np.linspace(-1, max(dataAllBodyLength.text_length), 1000)\n","yfit = linearRegressionBodyLength.predict(xfit[:, np.newaxis])\n","\n","plt.scatter(A, B, s=1, c=\"orange\")\n","plt.plot(xfit, yfit);\n","\n","print(\"Model slope:    \", linearRegressionBodyLength.coef_[0])\n","print(\"Model intercept:\", linearRegressionBodyLength.intercept_)\n","print(\"R2 score:\", linearRegressionBodyLength.score(A[:, np.newaxis], B))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jISjrnn2V4QV","colab_type":"code","colab":{}},"source":["for index, row in dataTrain.iterrows():\n","    textLength = len(row['text'])\n","    dataTrain.at[index, 'text_length'] = textLength\n","\n","for index, row in dataTest.iterrows():\n","    textLength = len(row['text'])\n","    dataTest.at[index, 'text_length'] = textLength\n","\n","from sklearn import linear_model\n","# from sklearn import linear_model\n","\n","logClassifierBodyLength = linear_model.LogisticRegression(solver='liblinear', C=17/1000, random_state=111)\n","logClassifierBodyLength.fit(dataTrain['text_length'].values.reshape(-1, 1), dataTrain['veracity'].values)\n","\n","predicted = logClassifierBodyLength.predict(dataTest['text_length'].values.reshape(-1, 1))\n","\n","from sklearn import metrics\n","print(metrics.accuracy_score(dataTest['veracity'].values.reshape(-1, 1), predicted))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHa_tvHgWAwq","colab_type":"code","colab":{}},"source":["def DATAMINERS_getBodyLengthScore(length): # return between 0 and 1, being 0 = True,  1 = Fake\n","    x = np.array(length).reshape(-1, 1)\n","    predicted = logClassifierBodyLength.predict(x)\n","    predicedProb = logClassifierBodyLength.predict_proba(x)[:,1]\n","    #return int(predicted), float(predicedProb)\n","    return 1 - float(predicedProb)\n","\n","print(DATAMINERS_getBodyLengthScore(12000))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZ1vSrvoWINA","colab_type":"text"},"source":["### Feature 10 : Word Frequency"]},{"cell_type":"code","metadata":{"id":"QmxkYKj7WK5a","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem.porter import *\n","from sklearn import metrics\n","\n","class WordFrequency():\n","\n","    def __init__(self):        \n","\n","        columnNames = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n","        dataTrain = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/liar-liar dataset/train.tsv', sep='\\t', header=None, names = columnNames)\n","        dataValidate = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/liar-liar dataset/valid.tsv', sep='\\t', header=None, names = columnNames)\n","        dataTest = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/liar-liar dataset/test.tsv', sep='\\t', header=None, names = columnNames)\n","        \n","        #dropping columns\n","        columnsToRemove = ['id','subject', 'speaker', 'context','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']\n","        dataTrain = dataTrain.drop(columns=columnsToRemove)\n","        dataValidate = dataValidate.drop(columns=columnsToRemove)\n","        dataTest = dataTest.drop(columns=columnsToRemove)\n","\n","        def convertMulticlassToBinaryclass(r):\n","            v = r['label']\n","            if (v == 'true'):\n","                return 'true'\n","            if (v == 'mostly-true'):\n","                return 'true'\n","            if (v == 'half-true'):\n","                return 'true'\n","            if (v == 'barely-true'):\n","                return 'false'\n","            if (v == 'false'):\n","                return 'false'\n","            if (v == 'pants-fire'):\n","                return 'false'\n","        dataTrain['label'] = dataTrain.apply(convertMulticlassToBinaryclass, axis=1)\n","        dataValidate['label'] = dataValidate.apply(convertMulticlassToBinaryclass, axis=1)\n","        dataTest['label'] = dataTest.apply(convertMulticlassToBinaryclass, axis=1)\n","        \n","\n","    \n","        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n","        train_tfidf = tfidfV.fit_transform(dataTrain['statement'].values)\n","        test_tfidf = tfidfV.fit_transform(dataTest['statement'].values)\n","\n","#         print('TF-IDF VECTORIZER')\n","\n","        ## Removing plurals for the tokens using PorterStemmer\n","        stemmer = PorterStemmer()\n","        tfidfVPlurals= tfidfV.get_feature_names()\n","        tfidfVSingles= [stemmer.stem(plural) for plural in tfidfVPlurals]\n","\n","        # Applying Set to remove duplicates\n","        tfidfVTokens = list(set(tfidfVSingles))\n","#         print('TFIDFV Tokens')\n","#         print(tfidfVTokens)\n","\n","        self.logR_pipeline = Pipeline([\n","                ('LogRCV', tfidfV),\n","                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n","                ])\n","\n","        self.logR_pipeline.fit(dataTrain['statement'],dataTrain['label'])\n","        predicted_LogR = self.logR_pipeline.predict(dataTest['statement'])\n","        score = metrics.accuracy_score(dataTest['label'], predicted_LogR)\n","        print(\"Word Frequency Model Trained - accuracy:   %0.6f\" % score)\n","        \n","\n","    def predict(self, text):\n","        predicted = self.logR_pipeline.predict([text])\n","        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n","        return bool(predicted), float(predicedProb)\n","    \n","    \n","# wf = WordFrequency()\n","# wf.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"979tSXxCWRHX","colab_type":"code","colab":{}},"source":["# from ipynb.fs.full.m_wordfrequency import WordFrequency\n","wordFrequency = WordFrequency()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRFfYfhiWSfE","colab_type":"code","colab":{}},"source":["def DATAMINERS_getWordFrequencyScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n","    #print(clickBait.predict(\"Should You bring the money now\"))\n","    binaryValue, probValue = wordFrequency.predict(text)\n","    return (1 - float(probValue))\n","\n","print(DATAMINERS_getWordFrequencyScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCDrWvBEfM05","colab_type":"text"},"source":["### Cred and Reliability"]},{"cell_type":"code","metadata":{"id":"quImnrOBfRs0","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","from sklearn import datasets, linear_model\n","from sklearn.model_selection import train_test_split\n","from matplotlib import pyplot as plt\n","from sklearn.pipeline import Pipeline\n","from sklearn import metrics\n","import json\n","import pickle\n","class Rel_Cred_Feature():\n","    def setup(self): \n","        #load the dataset\n","        columnNames = [\"type\"]\n","        website_data = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/website.csv', sep=',')\n","        y = website_data['type']\n","        website_data = website_data.drop('type', axis=1)\n","        website_data = website_data.drop('domain', axis=1)\n","       \n","        X_train, X_test, y_train, y_test = train_test_split(website_data, y, test_size=0.2)\n","\n","        #countVectorizerHeadlineText = CountVectorizer()\n","        #countVectorizerHeadlineText.fit_transform(website_data['comment'])\n","\n","        self.logR_pipeline = MultinomialNB()\n","\n","        self.logR_pipeline.fit(X_train, y_train)\n","        predicted_LogR = self.logR_pipeline.predict(X_test)\n","        score = metrics.accuracy_score(y_test, predicted_LogR)\n","        print(\"Bias Score Model Trained - accuracy:   %0.6f\" % score)\n","\n","    def predict(self, domain):\n","        API_ENDPOINT = \"https://openpagerank.com/api/v1.0/getPageRank?domains[]=\" + domain.strip()\n","        API_KEY = \"sw0gsosokcwk0go8cgsokoowgk8gcw8gs4ckkswk\"\n","        headers = {'API-OPR':API_KEY}\n","        response = requests.get(url = API_ENDPOINT, headers= headers)\n","        data = json.loads(response.text)\n","        text = data['response'][0]['page_rank_decimal']\n","        text = pd.DataFrame([text], columns=['page_rank_decimal'])\n","        predicted = self.logR_pipeline.predict(text)\n","        predicedProb = self.logR_pipeline.predict_proba(text)[:,1]\n","        return predicted[0], float(predicedProb)\n","    \n","relcred = Rel_Cred_Feature()\n","relcred.setup()\n","relcred.predict(\"www.google.com\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iz0c7ODAf6dM","colab_type":"code","colab":{}},"source":["def DATAMINERS_getCredRelScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n","    #print(clickBait.predict(\"Should You bring the money now\"))\n","    binaryValue, probValue = wordFrequency.predict(text)\n","    return (1 - float(probValue))\n","\n","print(DATAMINERS_getCredRelScore(\"cnn.com\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3KFeQ0lad1Sn","colab_type":"text"},"source":["### Biased"]},{"cell_type":"code","metadata":{"id":"5awopPGceHBz","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import nltk\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import gensim\n","import nltk.sentiment\n","from sklearn.decomposition import LatentDirichletAllocation as LDA\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk \n","from nltk.corpus import wordnet\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('vader_lexicon')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6f1ATDfqeLbl","colab_type":"code","colab":{}},"source":["# ITERATION 3\n","\n","# Load Dataset from drive\n","news = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/articles3.csv', low_memory =False)\n","n = 20\n","\n","news = news.head(500)\n","news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7gR0fToeZmh","colab_type":"text"},"source":["##### Biased - Preprocessing"]},{"cell_type":"code","metadata":{"id":"ySa95glxelhS","colab_type":"code","colab":{}},"source":["news = news.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qV8XezNeoDn","colab_type":"code","colab":{}},"source":["# ITERATION 3\n","import re\n","\n","def cleaning(raw_news):\n","    \n","    # 1. Remove non-letters/Special Characters and Punctuations\n","    # news = re.sub(\"[^a-zA-Z]\", \"\", raw_news)\n","    news = re.sub(\"[,\\.!?]\", \"\", raw_news)\n","    # 2. Convert to lower case.\n","    news =  news.lower()\n","    \n","    # 3. Tokenize.\n","    news_words = nltk.word_tokenize( news)\n","    \n","    # 4. Convert the stopwords list to \"set\" data type.\n","    stops = set(nltk.corpus.stopwords.words(\"english\"))\n","    \n","    # 5. Remove stop words. \n","    words = [w for w in  news_words  if not w in stops]\n","    \n","    # 6. Lemmentize \n","    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n","    \n","    # 7. Stemming\n","    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n","    \n","    # 8. Join the stemmed words back into one string separated by space, and return the result.\n","    return \" \".join(stems)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRtY-86aeqvL","colab_type":"code","colab":{}},"source":["# ITERATION 3\n","import time\n","\n","t1 = time.time()\n","news['clean_content'] = news[\"content\"].apply(cleaning) \n","t2 = time.time()\n","print(\"\\nTime to clean, tokenize and stem title in news: \\n\", len(news), \"news:\", (t2-t1)/60, \"min\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxmTlwnyev1d","colab_type":"text"},"source":["##### Biased - Implementation"]},{"cell_type":"code","metadata":{"id":"cqnAY05ie5En","colab_type":"code","colab":{}},"source":["news.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0cUdKz0Be7Yx","colab_type":"code","colab":{}},"source":["# Iteration 3\n","\n","# Load the LDA model from sk-learn\n","from sklearn.decomposition import LatentDirichletAllocation as LDA\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk \n","from nltk.corpus import wordnet\n","\n","\n","# Helper function\n","def print_topics(model, count_vectorizer, n_top_words):\n","   \n","    words = count_vectorizer.get_feature_names()\n","    num_ant = 0\n","    for topic_idx, topic in enumerate(model.components_):\n","\n","        top_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n","        for word in top_words:\n","          ant = list()\n","          for synset in wordnet.synsets(word):\n","            for lemma in synset.lemmas():\n","                if lemma.antonyms():    #When antonyms are available, add them into the list\n","                  ant.append(lemma.antonyms()[0].name())\n","          \n","          found_ant = False\n","          for a in ant:\n","            if a in top_words:\n","              found_ant = True\n","              \n","          if found_ant:\n","            num_ant = num_ant + 1\n","    return num_ant\n","\n","# Tweak the two parameters below\n","number_topics = 1\n","number_words = 10\n","results = []\n","for index, row in news.iterrows():\n","  \n","  count_vectorizer = CountVectorizer(stop_words='english')\n","  count_data = count_vectorizer.fit_transform([row['clean_content']])\n","\n","  # Create and fit the LDA model\n","  lda = LDA(n_components=number_topics, n_jobs=-1)\n","  lda.fit(count_data)\n","  # Print the topics found by the LDA model\n","  # print(\"Topics found via LDA:\")\n","  results.append(print_topics(lda, count_vectorizer, number_words))\n","print(results)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1_8SPhbe9yG","colab_type":"code","colab":{}},"source":["# Import the libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# matplotlib histogram\n","plt.hist(results, color = 'blue', edgecolor = 'black')\n","\n","# seaborn histogram\n","sns.distplot(results, hist=True, kde=False, color = 'blue',\n","             hist_kws={'edgecolor':'black'})\n","# Add labels\n","plt.title('Histogram of Antonyms')\n","plt.xlabel('# Antonymns')\n","plt.ylabel('Docs')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"blOjZ90UfAmR","colab_type":"code","colab":{}},"source":["model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/GoogleNews-vectors-negative300.bin.gz', binary=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXM6znt5fENm","colab_type":"code","colab":{}},"source":["\n","\"\"\"\n","Get 10 articles that are biased\n","Get 10 articles that are unbiased\n","\tPer document from each set:\n","\t\tDo LDA on that:\n","\t\t\tWord2Vec for each word you get from LDA:\n","\t\t\t\tFind Cosine similarity between those words\n","\t\t\t\t\tif words close together\n","\t\t\t\t\t\tbiased\n","\t\t\t\t\telse\n","\t\t\t\t\t\tunbiased\n","\t\t\t\t\t\n","\t\t\t\t\t(You confirm this with the label of the document)\n","\"\"\"\n","import gensim \n","from gensim.models import Word2Vec \n","\n","# Helper function\n","def print_topics(model, count_vectorizer, n_top_words):\n","    words = count_vectorizer.get_feature_names()\n","    results = []\n","    for topic_idx, topic in enumerate(model.components_):\n","        # print(\"\\nTopic #%d:\" % topic_idx)\n","        results = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n","    print(results)\n","    return results\n","\n","def lda_cs_result(file_name):\n","  number_topics = 1\n","  number_words = 10\n","  \n","  # Loading data\n","  news = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/' + file_name, low_memory =False)\n","  # Data cleaning\n","  news['clean_content'] = news[\"content\"].apply(cleaning) \n","  news = news[:10]\n","\n","  # Keeping track of avg cosine similarity score \n","  all_count = 0\n","  all_value = 0\n","  results_final = []\n","  for index, row in news.iterrows():\n","    if row['clean_content'].strip() != '':\n","      print(\"-----------------------------------------------\")\n","      # Setting up count vectorizer\n","      count_vectorizer = CountVectorizer(stop_words='english')\n","      count_data = count_vectorizer.fit_transform([row['clean_content']])\n","      \n","      # Create and fit the LDA model\n","      lda = LDA(n_components=number_topics, n_jobs=-1)\n","      lda.fit(count_data)\n","\n","      # Print the topics found by the LDA model\n","      temp_results = print_topics(lda, count_vectorizer, number_words)\n","\n","      results = []\n","      # Removing words that don't exist\n","      for tr in temp_results:\n","        if tr in model.wv.vocab:\n","          results.append(tr)\n","      \n","      # Looping through each word, finding cosine similarity against others\n","      for i in range(len(results)):\n","        for j in range(len(results)):\n","          if i != j:\n","            all_value = all_value + model.wv.similarity(results[i], results[j])\n","            all_count = all_count + 1\n","            results_final.append(model.wv.similarity(results[i], results[j]))\n","  return all_value/all_count, results_final\n","\n","result_a_1, result_a_2 = lda_cs_result(\"unbiased.csv\")\n","result_b_1, result_b_2 = lda_cs_result(\"biased.csv\")\n","# print(\"*******************************\")\n","# print(\"*******************************\")\n","\n","# print(\"unbiased\", result_a_1)\n","# print(\"biased\", result_b_1)\n","\n","# print(\"*******************************\")\n","# print(\"*******************************\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vm8rSbh8fHWA","colab_type":"code","colab":{}},"source":["# Import the libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# matplotlib histogram\n","plt.hist(result_a_2, color = 'blue', edgecolor = 'black')\n","\n","# seaborn histogram\n","sns.distplot(result_a_2, hist=True, kde=False, color = 'blue',\n","             hist_kws={'edgecolor':'black'})\n","# Add labels\n","plt.title('Histogram of biased CS Score')\n","plt.xlabel('Cosine Similiarity Score')\n","plt.ylabel('Count')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xOg9STyfJjg","colab_type":"code","colab":{}},"source":["# Import the libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# matplotlib histogram\n","plt.hist(result_b_2, color = 'blue', edgecolor = 'black')\n","\n","# seaborn histogram\n","sns.distplot(result_b_2, hist=True, kde=False, color = 'blue',\n","             hist_kws={'edgecolor':'black'})\n","# Add labels\n","plt.title('Histogram of biased CS Score')\n","plt.xlabel('Cosine Similiarity Score')\n","plt.ylabel('Count')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iIwTItfzfMWG","colab_type":"code","colab":{}},"source":["# This is creating a Model based on the biased/unbiased datasets we have. Very poor results since the previous steps were not successful\n","\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","from sklearn import datasets, linear_model\n","from sklearn.model_selection import train_test_split\n","from matplotlib import pyplot as plt\n","from sklearn.pipeline import Pipeline\n","from sklearn import metrics\n","import pickle\n","class BiasScoreFeature():\n","    def setup(self): \n","        #load the dataset\n","        unbiased_news = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/unbiased.csv', low_memory =False)\n","        biased_news = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/biased.csv' , low_memory =False)\n","        combined_news_features = pd.concat([unbiased_news, biased_news], ignore_index=True)\n","        combined_news_label = combined_news_features['label']\n","        combined_news_features.drop('label', axis=1)\n","       \n","        X_train, X_test, y_train, y_test = train_test_split(combined_news_features, combined_news_label, test_size=0.2)\n","\n","        countVectorizerHeadlineText = CountVectorizer()\n","        countVectorizerHeadlineText.fit_transform(combined_news_features['content'])\n","\n","        self.logR_pipeline = Pipeline([\n","            ('NBCV',countVectorizerHeadlineText),\n","            ('nb_clf',MultinomialNB())])\n","\n","        self.logR_pipeline.fit(X_train['content'], y_train)\n","        predicted_LogR = self.logR_pipeline.predict(X_test['content'])\n","        score = metrics.accuracy_score(y_test, predicted_LogR)\n","        print(\"Bias Score Model Trained - accuracy:   %0.6f\" % score)\n","\n","    def predict(self, text):\n","        predicted = self.logR_pipeline.predict([text])\n","        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n","        return predicted[0], float(predicedProb)\n","    \n","biasscore = BiasScoreFeature()\n","biasscore.setup()\n","biasscore.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gD7N97ntfPnX","colab_type":"code","colab":{}},"source":["pickle.dump(biasscore, open(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Models/biased_unbiased.sav\", 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgYDdhAGiKvQ","colab_type":"code","colab":{}},"source":["def DATAMINERS_getBiasScore(text): \n","    binaryValue, probValue = biasscore.predict(text)\n","    if binaryValue == 'bias':\n","      return probValue\n","    else:\n","      return (1 - float(probValue))\n","\n","print(DATAMINERS_getBiasScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QuT_S3DdHLE","colab_type":"text"},"source":["### Final Model"]},{"cell_type":"code","metadata":{"id":"YNgEFaPXdJ8G","colab_type":"code","colab":{}},"source":["# import warnings\n","# warnings.filterwarnings('ignore')\n","\n","def isFakeNews(text, headline=\"\", numAuthors = 0, source = \"\", party =\"\"):\n","    accur = [0.84, 0.56, 0.95, 0.35,  0.1 ,0.54, 0.98, 0.71, 0.6, 1, 0.60, .01] # using the (normalized) accuracy as weigths\n","    w = [float(i)/sum(accur) for i in accur]\n","    sumW = 0\n","    prob = []\n","    prob.append(w[0] * DATAMINERS_getAuthorScore(numAuthors))\n","    sumW += w[0]\n","    if ( (headline != \"\") & (party != \"\")):\n","        prob.append(w[1] * DATAMINERS_getPartyAffiliationScore(headline, party))\n","        sumW += w[1]\n","    if (headline != \"\"):\n","        prob.append(w[2] * DATAMINERS_getClickbaitScore(headline))\n","        sumW += w[2]\n","    if (headline != \"\"):\n","        prob.append(w[3] * DATAMINERS_getSentimentAnalysisScore(headline))\n","        sumW += w[3]\n","    if (headline != \"\"):\n","        prob.append(w[4] * DATAMINERS_getLDATopicModellingScore(headline))\n","        sumW += w[4]\n","    if (headline != \"\"):\n","        prob.append(w[5] * DATAMINERS_getSensationalismScore(headline))\n","        sumW += w[5]\n","    if (headline != \"\"):\n","        prob.append(w[6] * DATAMINERS_getSpamScore(headline))\n","        sumW += w[6]\n","    prob.append(w[7] * DATAMINERS_getBodyLengthScore(len(text)))\n","    sumW += w[7]\n","    prob.append(w[8] * DATAMINERS_getWordFrequencyScore(text))\n","    sumW += w[8]\n","    if (party != \"\"):\n","        prob.append(w[9] * DATAMINERS_getSourceReputationScore(source))\n","        sumW += w[9]\n","    if (source != \"\"):\n","        prob.append(w[10] * DATAMINERS_getCredRelScore(source))\n","        sumW += w[10]\n","    if (text != \"\"):\n","        prob.append(w[11] * DATAMINERS_getBiasScore(text))\n","        sumW += w[11]\n","\n","    probTotal = sum(prob[0:len(prob)]) / sumW\n","    return probTotal\n","    \n","result = isFakeNews(\"Yesterday, the Brazilian soccer team won the world cup by defeating Argentina\", \"World Cup ends\", 1, \"cnn.com\", \"republican\")\n","print(\"result:\", result)\n","\n","if result > 0.75:\n","  print(\"is Fake NEWS!!!\")\n","elif (result > 0.5 and result <= 0.75):\n","  print(\"is maybe FAKE NEWS!!!\")\n","elif result == 0.5:\n","  print(\"Neutral\")\n","elif (result >= 0.25 and result < 0.5):\n","  print(\"is maybe TRUE NEWS!!!\")\n","else:\n","  print(\"is TRUE NEWS!!!\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUtuSoo0dWzo","colab_type":"code","colab":{}},"source":["truePos = 0\n","trueNeg = 0\n","falsePos = 0\n","falseNeg = 0\n","for index, row in dataTest.iterrows():\n","    text = row['text']\n","    headline= row['title']\n","    numAuthors = len(row['authors'])\n","    source = row['source']\n","    party = \"\"\n","    if 'party' in dataTest.columns:\n","        party = row['party']\n","    pred = isFakeNews(text, headline, numAuthors, source, party)\n","    if ((row['veracity'] == 1) &  (pred < 0.5) ):\n","        truePos += 1\n","    elif ((row['veracity'] == 0) & (pred >= 0.5) ):\n","        trueNeg += 1\n","    elif ((row['veracity'] == 1) &  (pred >= 0.5) ):\n","        falsePos += 1            \n","    elif ((row['veracity'] == 0) &  (pred < 0.5) ):\n","        falseNeg += 1\n","        \n","print(\"truePos=\", truePos)\n","print(\"trueNeg=\", trueNeg)\n","print(\"falsePos=\", falsePos)\n","print(\"falseNeg=\", falseNeg)\n","print(\"accuracy=\", (truePos/(truePos+falseNeg)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qkm9zhM7dgEA","colab_type":"code","colab":{}},"source":["errorMargin = 0.10\n","ignored = 0\n","truePos = 0\n","trueNeg = 0\n","falsePos = 0\n","falseNeg = 0\n","for index, row in dataTest.iterrows():\n","    text = row['text']\n","    headline= row['title']\n","    numAuthors = len(row['authors'])\n","    source = row['source']\n","    party = \"\"\n","    if 'party' in dataTest.columns:\n","        party = row['party']\n","    pred = isFakeNews(text, headline, numAuthors, source, party)\n","\n","    if (abs(0.5 - pred) < errorMargin):\n","        ignored += 1\n","    elif ((row['veracity'] == 1) &  (pred < 0.5) ):\n","        truePos += 1\n","    elif ((row['veracity'] == 0) & (pred >= 0.5) ):\n","        trueNeg += 1\n","    elif ((row['veracity'] == 1) &  (pred >= 0.5) ):\n","        falsePos += 1            \n","    elif ((row['veracity'] == 0) &  (pred < 0.5) ):\n","        falseNeg += 1\n","\n","        \n","print(\"truePos=\", truePos)\n","print(\"trueNeg=\", trueNeg)\n","print(\"falsePos=\", falsePos)\n","print(\"falseNeg=\", falseNeg)\n","print(\"ignored=\", ignored)\n","print(\"accuracy=\", (truePos/(truePos+falseNeg)))"],"execution_count":0,"outputs":[]}]}