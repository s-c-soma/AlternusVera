{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[6-10] Factors_LiarLiar.ipynb","provenance":[],"collapsed_sections":["LGFP9McQ8_y0","NG4cEldS97uq","xkAvpJaC9Tt9","7pSTvfe29Zwh","S4IVMM5D9nXO","4cRRL_caw_H5"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8MBKm94holrZ","colab_type":"text"},"source":["# Project : Alternus Vera Iteration 3\n","\n","#**Team: The Mean Squares**\n","* Jeyasri Subramanian [SJSU ID: 014510132]\n","* Subarna Chowdhury Soma [SJSU ID: 014549587]\n","* Pranav Lodha [SJSU ID: 009468121]\n","* Wasae Qureshi [SJSU ID: 014569880]"]},{"cell_type":"markdown","metadata":{"id":"UK5NfJJvnwRO","colab_type":"text"},"source":["### Team Contributions:\n","\n","|Features  |  Member |\n","|-----|-----|\n","| Bias |  Wasae Qureshi |\n","| Credibility & Reliability |  Pranav Lodha |\n","| News Converage |  Subarna Chowdhury Soma  |\n","| Micro Pattern |  Jeyasri Subramanian  |"]},{"cell_type":"markdown","metadata":{"id":"xwOAV4pSd2G6","colab_type":"text"},"source":["# 10 Factors\n","\n","* Feature 1 : Sentiment Analysis [Done]\n","*Feature 2 : LDA Topic Modelling [Done]\n","*Feature 3 : Sensationalism\n","*Feature 4 : Political Affiliation [Done]\n","*Feature 5 : Clickbait [Done]\n","*Feature 6 : Spam\n","*Feature 7 : Author Credibility\n","*Feature 8 : Source Reputation\n","*Feature 9 : Content Length [Done]\n","*Feature 10 : Word Frequency [Done]"]},{"cell_type":"markdown","metadata":{"id":"W1b-vCSrpN52","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"5KOtyrTIpYT7","colab_type":"code","outputId":"dbe7035a-bf3d-4463-8584-c341061fd7eb","executionInfo":{"status":"ok","timestamp":1588974264026,"user_tz":420,"elapsed":21655,"user":{"displayName":"Subarna Chowdhury Soma","photoUrl":"","userId":"13997249864171991230"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ppqHYYunpOJ_","colab_type":"code","outputId":"07585159-6e0c-464f-c8c3-ca075f3a535f","executionInfo":{"status":"ok","timestamp":1588974272631,"user_tz":420,"elapsed":3827,"user":{"displayName":"Subarna Chowdhury Soma","photoUrl":"","userId":"13997249864171991230"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["import argparse\n","import pandas as pd\n","import numpy as np\n","import operator\n","import matplotlib.pyplot as plt\n","import nltk as nl\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.model_selection import ParameterGrid\n","import statistics\n","import random\n","import string\n","import warnings\n","from gensim.models import word2vec\n","from string import punctuation\n","from matplotlib import pyplot\n","from pandas import Series, datetime\n","from pandas.plotting import scatter_matrix, autocorrelation_plot\n","from sklearn.metrics import precision_recall_fscore_support as score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, TimeSeriesSplit\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.feature_extraction.text import CountVectorizer\n","from xgboost import XGBClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","import nltk\n","import re\n","import io\n","import requests\n","import time\n","import gensim\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from string import punctuation\n","import nltk.sentiment\n","\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('vader_lexicon')\n","from scipy import sparse\n","# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n","# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kb7AJhvuPlRA","colab_type":"code","colab":{}},"source":["senti = nltk.sentiment.vader.SentimentIntensityAnalyzer()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUAK6G3ppf4e","colab_type":"text"},"source":["# Loading Dataset"]},{"cell_type":"markdown","metadata":{"id":"a0ONimeg7mtR","colab_type":"text"},"source":["## Kaggle- Fake News"]},{"cell_type":"code","metadata":{"id":"IoOrJ5d2Nndw","colab_type":"code","colab":{}},"source":["def get_parsed_data(url, sep='\\t', header=None ):\n","  return pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')), sep=sep, header=header )\n","\n","# Download and parse the dataset... Let us first work with 100 articles\n","KAGGLE_DATASET = 'https://github.com/synle/machine-learning-sample-dataset/raw/master/liar_dataset/kaggle'\n","fake_news_data = get_parsed_data('%s/kaggle-fake.csv'% KAGGLE_DATASET, ',' , 'infer' )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fJNzr6jtOLvk","colab_type":"code","outputId":"9ae2bf3b-e275-4202-a158-3f8e6553f890","executionInfo":{"status":"ok","timestamp":1588974294441,"user_tz":420,"elapsed":573,"user":{"displayName":"Subarna Chowdhury Soma","photoUrl":"","userId":"13997249864171991230"}},"colab":{"base_uri":"https://localhost:8080/","height":568}},"source":["fake_news_data.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>uuid</th>\n","      <th>ord_in_thread</th>\n","      <th>author</th>\n","      <th>published</th>\n","      <th>title</th>\n","      <th>text</th>\n","      <th>language</th>\n","      <th>crawled</th>\n","      <th>site_url</th>\n","      <th>country</th>\n","      <th>domain_rank</th>\n","      <th>thread_title</th>\n","      <th>spam_score</th>\n","      <th>main_img_url</th>\n","      <th>replies_count</th>\n","      <th>participants_count</th>\n","      <th>likes</th>\n","      <th>comments</th>\n","      <th>shares</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n","      <td>0</td>\n","      <td>Barracuda Brigade</td>\n","      <td>2016-10-26T21:41:00.000+03:00</td>\n","      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n","      <td>Print They should pay all the back all the mon...</td>\n","      <td>english</td>\n","      <td>2016-10-27T01:49:27.168+03:00</td>\n","      <td>100percentfedup.com</td>\n","      <td>US</td>\n","      <td>25689.0</td>\n","      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n","      <td>0.000</td>\n","      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>bias</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2bdc29d12605ef9cf3f09f9875040a7113be5d5b</td>\n","      <td>0</td>\n","      <td>reasoning with facts</td>\n","      <td>2016-10-29T08:47:11.259+03:00</td>\n","      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n","      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n","      <td>english</td>\n","      <td>2016-10-29T08:47:11.259+03:00</td>\n","      <td>100percentfedup.com</td>\n","      <td>US</td>\n","      <td>25689.0</td>\n","      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n","      <td>0.000</td>\n","      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>bias</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>c70e149fdd53de5e61c29281100b9de0ed268bc3</td>\n","      <td>0</td>\n","      <td>Barracuda Brigade</td>\n","      <td>2016-10-31T01:41:49.479+02:00</td>\n","      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n","      <td>Red State : \\nFox News Sunday reported this mo...</td>\n","      <td>english</td>\n","      <td>2016-10-31T01:41:49.479+02:00</td>\n","      <td>100percentfedup.com</td>\n","      <td>US</td>\n","      <td>25689.0</td>\n","      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n","      <td>0.000</td>\n","      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>bias</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7cf7c15731ac2a116dd7f629bd57ea468ed70284</td>\n","      <td>0</td>\n","      <td>Fed Up</td>\n","      <td>2016-11-01T05:22:00.000+02:00</td>\n","      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n","      <td>Email Kayla Mueller was a prisoner and torture...</td>\n","      <td>english</td>\n","      <td>2016-11-01T15:46:26.304+02:00</td>\n","      <td>100percentfedup.com</td>\n","      <td>US</td>\n","      <td>25689.0</td>\n","      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n","      <td>0.068</td>\n","      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>bias</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0206b54719c7e241ffe0ad4315b808290dbe6c0f</td>\n","      <td>0</td>\n","      <td>Fed Up</td>\n","      <td>2016-11-01T21:56:00.000+02:00</td>\n","      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n","      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n","      <td>english</td>\n","      <td>2016-11-01T23:59:42.266+02:00</td>\n","      <td>100percentfedup.com</td>\n","      <td>US</td>\n","      <td>25689.0</td>\n","      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n","      <td>0.865</td>\n","      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>bias</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       uuid  ord_in_thread  ... shares  type\n","0  6a175f46bcd24d39b3e962ad0f29936721db70db              0  ...      0  bias\n","1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0  ...      0  bias\n","2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0  ...      0  bias\n","3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0  ...      0  bias\n","4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0  ...      0  bias\n","\n","[5 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"ln5GCFwz8BBU","colab_type":"text"},"source":["## Liar Liar Dataset"]},{"cell_type":"code","metadata":{"id":"x_rVxTi0SG3_","colab_type":"code","colab":{}},"source":["colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', \n","            'speakerjobtitle', 'stateinfo','partyaffiliation', \n","            'barelytruecounts', 'falsecounts','halftruecounts',\n","            'mostlytrueocounts','pantsonfirecounts','context']\n","headlinecolname = 'headline_text'\n","labelcolname = 'encoded_label'\n","processedheadlinecolname = 'processed_headline_text'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qtKwTl1B8G18","colab_type":"code","colab":{}},"source":["#method to load data\n","colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo','partyaffiliation', 'barelytruecounts', 'falsecounts','halftruecounts','mostlytrueocunts','pantsonfirecounts','context']\n","def load_train_data():\n","    #load data\n","    train_filename = '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train.tsv'\n","    train_news = pd.read_csv(train_filename, sep='\\t', names = colnames,error_bad_lines=False)\n","    return train_news\n","\n","def load_test_data():\n","    test_filename = '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test.tsv'\n","    test_news = pd.read_csv(test_filename, sep='\\t', names = colnames,error_bad_lines=False)\n","    return test_news\n","\n","def load_valid_data():\n","    test_filename = '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/valid.tsv'\n","    test_news = pd.read_csv(test_filename, sep='\\t', names = colnames,error_bad_lines=False)\n","    return test_news"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O10qC2huqaB0","colab_type":"code","colab":{}},"source":["#X_train, X_test, y_train, y_test = train_test_split(fake_news_data, y, test_size=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ReiO4bko2el","colab_type":"text"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"LGFP9McQ8_y0","colab_type":"text"},"source":["## Kaggle Fake News"]},{"cell_type":"code","metadata":{"id":"_jx3glRGsKEI","colab_type":"code","colab":{}},"source":["fake_news_data = fake_news_data.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TV7JAy_Zo1sy","colab_type":"code","colab":{}},"source":["import re\n","\n","def cleaning(raw_news):\n","    \n","    # 1. Remove non-letters/Special Characters and Punctuations\n","    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n","    \n","    # 2. Convert to lower case.\n","    news =  news.lower()\n","    \n","    # 3. Tokenize.\n","    news_words = nltk.word_tokenize( news)\n","    \n","    # 4. Convert the stopwords list to \"set\" data type.\n","    stops = set(nltk.corpus.stopwords.words(\"english\"))\n","    \n","    # 5. Remove stop words. \n","    words = [w for w in  news_words  if not w in stops]\n","    \n","    # 6. Lemmentize \n","    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n","    \n","    # 7. Stemming\n","    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n","    \n","    # 8. Join the stemmed words back into one string separated by space, and return the result.\n","    return \" \".join(stems)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZRAXLkTq71H","colab_type":"code","outputId":"ed582ae2-6392-4712-d695-d9c5c2509606","executionInfo":{"status":"ok","timestamp":1588974317474,"user_tz":420,"elapsed":6369,"user":{"displayName":"Subarna Chowdhury Soma","photoUrl":"","userId":"13997249864171991230"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["import time\n","\n","t1 = time.time()\n","fake_news_data['clean_title'] = fake_news_data[\"title\"].apply(cleaning) \n","t2 = time.time()\n","print(\"\\nTime to clean, tokenize and stem title in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")\n","\n","t1 = time.time()\n","fake_news_data['clean_thread_title'] = fake_news_data[\"thread_title\"].apply(cleaning) \n","t2 = time.time()\n","print(\"\\nTime to clean, tokenize and stem thread_title in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")\n","\n","# Testing\n","# t1 = time.time()\n","# fake_news_data['clean_text'] = fake_news_data[\"text\"].apply(cleaning) \n","# t2 = time.time()\n","# print(\"\\nTime to clean, tokenize and stem text in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Time to clean, tokenize and stem title in fake_news_data: \n"," 4702 news: 0.061739130814870195 min\n","\n","Time to clean, tokenize and stem thread_title in fake_news_data: \n"," 4702 news: 0.037133399645487467 min\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zhx7YrqLscvL","colab_type":"code","outputId":"2f41347a-c0a1-4898-c3f9-2348118eb457","executionInfo":{"status":"ok","timestamp":1588974446428,"user_tz":420,"elapsed":126571,"user":{"displayName":"Subarna Chowdhury Soma","photoUrl":"","userId":"13997249864171991230"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/Shared drives/CMPE 257: Machine Learning/AlterusVera-Datasets/GoogleNews-vectors-negative300.bin.gz', binary=True)\n","words = model.index2word\n","\n","w_rank = {}\n","for i,word in enumerate(words):\n","    w_rank[word] = i\n","\n","WORDS = w_rank"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NG4cEldS97uq","colab_type":"text"},"source":["#### Spell Checker"]},{"cell_type":"code","metadata":{"id":"KUhOgPegstJ3","colab_type":"code","colab":{}},"source":["import re\n","from collections import Counter\n","\n","def words(text): return re.findall(r'\\w+', text.lower())\n","\n","def P(word, N=sum(WORDS.values())): \n","    \"Probability of `word`.\"\n","    return - WORDS.get(word, 0)\n","\n","def correction(word): \n","    \"Most probable spelling correction for word.\"\n","    return max(candidates(word), key=P)\n","\n","def candidates(word): \n","    \"Generate possible spelling corrections for word.\"\n","    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n","\n","def known(words): \n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\n","    return set(w for w in words if w in WORDS)\n","\n","def edits1(word):\n","    \"All edits that are one edit away from `word`.\"\n","    letters    = 'abcdefghijklmnopqrstuvwxyz'\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    deletes    = [L + R[1:]               for L, R in splits if R]\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","    inserts    = [L + c + R               for L, R in splits for c in letters]\n","    return set(deletes + transposes + replaces + inserts)\n","\n","def edits2(word): \n","    \"All edits that are two edits away from `word`.\"\n","    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dw2kpRvosv51","colab_type":"code","colab":{}},"source":["def spell_checker(text):\n","    all_words = re.findall(r'\\w+', text.lower()) # split sentence to words\n","    spell_checked_text  = []\n","    for i in range(len(all_words)):\n","        spell_checked_text.append(correction(all_words[i]))\n","    return ' '.join(spell_checked_text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxWOSGS8swca","colab_type":"code","colab":{}},"source":["print(\"Before: \\n\", fake_news_data['clean_title'][0] )\n","t1 = time.time()\n","fake_news_data['clean_title'] = fake_news_data['clean_title'].apply(spell_checker)\n","t2 = time.time()\n","print(\"\\nTime to spell check the clean_title in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")\n","\n","print(\"\\nAfter: \\n\",fake_news_data['clean_title'][0] )\n","fake_news_data.head(2)\n","\n","print(\"Before: \\n\", fake_news_data['clean_thread_title'][0] )\n","t1 = time.time()\n","fake_news_data['clean_thread_title'] = fake_news_data['clean_thread_title'].apply(spell_checker)\n","t2 = time.time()\n","print(\"\\nTime to spell check the clean_thread_title in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")\n","\n","print(\"\\nAfter: \\n\",fake_news_data['clean_thread_title'][0] )\n","fake_news_data.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJK4otAUtMFt","colab_type":"code","colab":{}},"source":["fake_news_data['type'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hqqy_Lxy9IkU","colab_type":"text"},"source":["## Liar Liar Dataset"]},{"cell_type":"markdown","metadata":{"id":"xkAvpJaC9Tt9","colab_type":"text"},"source":["#### Label Encoding"]},{"cell_type":"code","metadata":{"id":"CergEAhe9MlH","colab_type":"code","colab":{}},"source":["#label encoding\n","true_labels = ['original','true','mostly-true','half-true']\n","false_labels = ['barely-true','false','pants-fire']\n","\n","def encode_news_type(input_label):\n","    if input_label in true_labels:\n","        return 1\n","    else:\n","        return 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pSTvfe29Zwh","colab_type":"text"},"source":["#### Remove Punctuations"]},{"cell_type":"code","metadata":{"id":"AzOAUrYf9XhS","colab_type":"code","colab":{}},"source":["#method to remove punctuations from textual data\n","def remove_punctuation(text):\n","    translator = str.maketrans('', '', string.punctuation)\n","    return text.translate(translator)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4IVMM5D9nXO","colab_type":"text"},"source":["#### Remove Stopwords"]},{"cell_type":"code","metadata":{"id":"YC_KMNp69mNu","colab_type":"code","colab":{}},"source":["from nltk.stem.snowball import SnowballStemmer\n","sw = stopwords.words('english')\n","stemmer = SnowballStemmer(\"english\")\n","\n","#Remove stop words\n","def remove_stopwords(text):\n","    text = [word.lower() for word in text.split() if word.lower() not in sw]\n","    return \" \".join(text)\n","\n","#Lemmetize and pos tagging\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","#Stemming\n","def stemming(text):    \n","    text = [stemmer.stem(word) for word in text.split()]\n","    return \" \".join(text)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1jxF9fK5-Hon","colab_type":"text"},"source":["#### Distillation on Test and Training Data"]},{"cell_type":"code","metadata":{"id":"uZIxKy4V-MAu","colab_type":"code","colab":{}},"source":["def text_preprocess(df):\n","  ## headline_text\n","    #encode labels\n","    df['encoded_label'] = df.apply(lambda row: encode_news_type(row['label']), axis=1)\n","    #convert to lower case\n","    df['processed_headline_text'] = df['headline_text'].str.lower()\n","    #remove stop words\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(remove_stopwords)\n","    #spell check\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(spell_checker)\n","    #Lemmetize\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(lemmatize_stemming)\n","    #stemming\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(stemming)\n","    #remove punctuation\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(remove_punctuation)\n","    #remove less than 3 letter words\n","    df['processed_headline_text']  = df.processed_headline_text.apply(lambda i: ' '.join(filter(lambda j: len(j) > 3, i.split())))\n","    return df[['headline_text','processed_headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo', 'partyaffiliation', 'context', 'encoded_label']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X2wIl2d5-QUT","colab_type":"code","colab":{}},"source":["#load test and train data\n","train_news = load_train_data()\n","test_news = load_test_data()\n","valid_news = load_valid_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRwgjj0H-UsJ","colab_type":"code","colab":{}},"source":["#pre-processing train dataset\n","df_train = text_preprocess(train_news)\n","df_train.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YpqkUga3-2fa","colab_type":"code","colab":{}},"source":["#pre-processing test dataset\n","df_test = text_preprocess(test_news)\n","df_test.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgR1PheiTWG4","colab_type":"code","colab":{}},"source":["#pre-processing valid dataset\n","df_valid = text_preprocess(valid_news)\n","df_valid.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIfKy8ipT05L","colab_type":"code","colab":{}},"source":["train_news['index'] = train_news.index\n","\n","train_news.head()\n","\n","test_news['index'] = test_news.index\n","\n","test_news.head()\n","\n","valid_news['index'] = valid_news.index\n","\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqGhjjue6iQ6","colab_type":"text"},"source":["# Factor 9 : Content Length"]},{"cell_type":"code","metadata":{"id":"TzWeDijV6s2d","colab_type":"code","colab":{}},"source":["dataAllBodyLength = train_news.copy()\n","for index, row in dataAllBodyLength.iterrows():\n","    textLength = len(row['text'])\n","    dataAllBodyLength.at[index, 'text_length'] = textLength\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.linear_model import LinearRegression\n","linearRegressionBodyLength = LinearRegression(fit_intercept=True)\n","\n","A = np.array(list(dataAllBodyLength.text_length))\n","B = np.array(list(dataAllBodyLength.veracity))\n","\n","linearRegressionBodyLength.fit(A[:, np.newaxis], B)\n","\n","xfit = np.linspace(-1, max(dataAllBodyLength.text_length), 1000)\n","yfit = linearRegressionBodyLength.predict(xfit[:, np.newaxis])\n","\n","plt.scatter(A, B, s=1, c=\"orange\")\n","plt.plot(xfit, yfit);\n","\n","print(\"Model slope:    \", linearRegressionBodyLength.coef_[0])\n","print(\"Model intercept:\", linearRegressionBodyLength.intercept_)\n","print(\"R2 score:\", linearRegressionBodyLength.score(A[:, np.newaxis], B))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Toc-_mzo6xzC","colab_type":"code","colab":{}},"source":["or index, row in dataTrain.iterrows():\n","    textLength = len(row['text'])\n","    dataTrain.at[index, 'text_length'] = textLength\n","\n","for index, row in dataTest.iterrows():\n","    textLength = len(row['text'])\n","    dataTest.at[index, 'text_length'] = textLength\n","\n","from sklearn import linear_model\n","# from sklearn import linear_model\n","\n","logClassifierBodyLength = linear_model.LogisticRegression(solver='liblinear', C=17/1000, random_state=111)\n","logClassifierBodyLength.fit(dataTrain['text_length'].values.reshape(-1, 1), dataTrain['veracity'].values)\n","\n","predicted = logClassifierBodyLength.predict(dataTest['text_length'].values.reshape(-1, 1))\n","\n","from sklearn import metrics\n","print(metrics.accuracy_score(dataTest['veracity'].values.reshape(-1, 1), predicted))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRwQP1j16zk_","colab_type":"code","colab":{}},"source":["def DATAMINERS_getBodyLengthScore(length): # return between 0 and 1, being 0 = True,  1 = Fake\n","    x = np.array(length).reshape(-1, 1)\n","    predicted = logClassifierBodyLength.predict(x)\n","    predicedProb = logClassifierBodyLength.predict_proba(x)[:,1]\n","    #return int(predicted), float(predicedProb)\n","    return 1 - float(predicedProb)\n","\n","print(DATAMINERS_getBodyLengthScore(12000))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2sFD184X4Z8C","colab_type":"text"},"source":["# Factor 10 : Word Frequency"]},{"cell_type":"code","metadata":{"id":"vhu2DtvM4sde","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem.porter import *\n","from sklearn import metrics\n","\n","class WordFrequency():\n","\n","    def __init__(self):        \n","\n","        columnNames = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n","        dataTrain = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train.tsv', sep='\\t', header=None, names = columnNames)\n","        dataValidate = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/valid.tsv', sep='\\t', header=None, names = columnNames)\n","        dataTest = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test.tsv', sep='\\t', header=None, names = columnNames)\n","        \n","        #dropping columns\n","        columnsToRemove = ['id','subject', 'speaker', 'context','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']\n","        dataTrain = dataTrain.drop(columns=columnsToRemove)\n","        dataValidate = dataValidate.drop(columns=columnsToRemove)\n","        dataTest = dataTest.drop(columns=columnsToRemove)\n","\n","        def convertMulticlassToBinaryclass(r):\n","            v = r['label']\n","            if (v == 'true'):\n","                return 'true'\n","            if (v == 'mostly-true'):\n","                return 'true'\n","            if (v == 'half-true'):\n","                return 'true'\n","            if (v == 'barely-true'):\n","                return 'false'\n","            if (v == 'false'):\n","                return 'false'\n","            if (v == 'pants-fire'):\n","                return 'false'\n","        dataTrain['label'] = dataTrain.apply(convertMulticlassToBinaryclass, axis=1)\n","        dataValidate['label'] = dataValidate.apply(convertMulticlassToBinaryclass, axis=1)\n","        dataTest['label'] = dataTest.apply(convertMulticlassToBinaryclass, axis=1)\n","        \n","\n","    \n","        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n","        train_tfidf = tfidfV.fit_transform(dataTrain['statement'].values)\n","        test_tfidf = tfidfV.fit_transform(dataTest['statement'].values)\n","\n","#         print('TF-IDF VECTORIZER')\n","\n","        ## Removing plurals for the tokens using PorterStemmer\n","        stemmer = PorterStemmer()\n","        tfidfVPlurals= tfidfV.get_feature_names()\n","        tfidfVSingles= [stemmer.stem(plural) for plural in tfidfVPlurals]\n","\n","        # Applying Set to remove duplicates\n","        tfidfVTokens = list(set(tfidfVSingles))\n","#         print('TFIDFV Tokens')\n","#         print(tfidfVTokens)\n","\n","        self.logR_pipeline = Pipeline([\n","                ('LogRCV', tfidfV),\n","                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n","                ])\n","\n","        self.logR_pipeline.fit(dataTrain['statement'],dataTrain['label'])\n","        predicted_LogR = self.logR_pipeline.predict(dataTest['statement'])\n","        score = metrics.accuracy_score(dataTest['label'], predicted_LogR)\n","        print(\"Word Frequency Model Trained - accuracy:   %0.6f\" % score)\n","        \n","\n","    def predict(self, text):\n","        predicted = self.logR_pipeline.predict([text])\n","        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n","        return bool(predicted), float(predicedProb)\n","    \n","    \n","# wf = WordFrequency()\n","# wf.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sd7y-xU15_bD","colab_type":"code","colab":{}},"source":["# from ipynb.fs.full.m_wordfrequency import WordFrequency\n","wordFrequency = WordFrequency()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8Fc_UP-6SRp","colab_type":"code","colab":{}},"source":["def DATAMINERS_getWordFrequencyScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n","    #print(clickBait.predict(\"Should You bring the money now\"))\n","    binaryValue, probValue = wordFrequency.predict(text)\n","    return (1 - float(probValue))\n","\n","print(DATAMINERS_getWordFrequencyScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"],"execution_count":0,"outputs":[]}]}