{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10Factors_LiarLiar.ipynb","provenance":[],"collapsed_sections":["LGFP9McQ8_y0","NG4cEldS97uq","xkAvpJaC9Tt9","7pSTvfe29Zwh","S4IVMM5D9nXO","4cRRL_caw_H5"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8MBKm94holrZ","colab_type":"text"},"source":["# Project : Alternus Vera Iteration 3\n","\n","#**Team: The Mean Squares**\n","* Jeyasri Subramanian [SJSU ID: 014510132]\n","* Subarna Chowdhury Soma [SJSU ID: 014549587]\n","* Pranav Lodha [SJSU ID: 009468121]\n","* Wasae Qureshi [SJSU ID: 014569880]"]},{"cell_type":"markdown","metadata":{"id":"UK5NfJJvnwRO","colab_type":"text"},"source":["### Team Contributions:\n","\n","|Features  |  Member |\n","|-----|-----|\n","| Bias |  Wasae Qureshi |\n","| Credibility & Reliability |  Pranav Lodha |\n","| News Converage |  Subarna Chowdhury Soma  |\n","| Micro Pattern |  Jeyasri Subramanian  |"]},{"cell_type":"markdown","metadata":{"id":"xwOAV4pSd2G6","colab_type":"text"},"source":["# 10 Factors\n","\n","* Feature 1 : Sentiment Analysis [Done]\n","*Feature 2 : LDA Topic Modelling [Done]\n","*Feature 3 : Sensationalism\n","*Feature 4 : Political Affiliation [Done]\n","*Feature 5 : Clickbait [Done]\n","*Feature 6 : Spam\n","*Feature 7 : Author Credibility\n","*Feature 8 : Source Reputation\n","*Feature 9 : Content Length [Done]\n","*Feature 10 : Word Frequency [Done]"]},{"cell_type":"markdown","metadata":{"id":"W1b-vCSrpN52","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"ppqHYYunpOJ_","colab_type":"code","outputId":"4e03e3db-cc43-4616-fdef-8f1e11d77679","executionInfo":{"status":"error","timestamp":1588659902581,"user_tz":420,"elapsed":412,"user":{"displayName":"Pranav Lodha","photoUrl":"","userId":"17488211667113804286"}},"colab":{"base_uri":"https://localhost:8080/","height":664}},"source":["import argparse\n","import pandas as pd\n","import numpy as np\n","import operator\n","import matplotlib.pyplot as plt\n","import nltk as nl\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.model_selection import ParameterGrid\n","import statistics\n","import random\n","import string\n","import warnings\n","from gensim.models import word2vec\n","from string import punctuation\n","from matplotlib import pyplot\n","from pandas import Series, datetime\n","from pandas.plotting import scatter_matrix, autocorrelation_plot\n","from sklearn.metrics import precision_recall_fscore_support as score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, TimeSeriesSplit\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.feature_extraction.text import CountVectorizer\n","from xgboost import XGBClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","import nltk\n","import re\n","import io\n","import requests\n","import time\n","import gensim\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from string import punctuation\n","import nltk.sentiment\n","senti = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('vader_lexicon')\n","from scipy import sparse\n","# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n","# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-74449d520dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0msenti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}]},{"cell_type":"code","metadata":{"id":"5KOtyrTIpYT7","colab_type":"code","colab":{}},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUAK6G3ppf4e","colab_type":"text"},"source":["# Loading Dataset"]},{"cell_type":"markdown","metadata":{"id":"a0ONimeg7mtR","colab_type":"text"},"source":["## Kaggle- Fake News"]},{"cell_type":"code","metadata":{"id":"IoOrJ5d2Nndw","colab_type":"code","colab":{}},"source":["def get_parsed_data(url, sep='\\t', header=None ):\n","  return pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')), sep=sep, header=header )\n","\n","# Download and parse the dataset... Let us first work with 100 articles\n","KAGGLE_DATASET = 'https://github.com/synle/machine-learning-sample-dataset/raw/master/liar_dataset/kaggle'\n","fake_news_data = get_parsed_data('%s/kaggle-fake.csv'% KAGGLE_DATASET, ',' , 'infer' )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fJNzr6jtOLvk","colab_type":"code","colab":{}},"source":["fake_news_data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ln5GCFwz8BBU","colab_type":"text"},"source":["## Liar Liar Dataset"]},{"cell_type":"code","metadata":{"id":"x_rVxTi0SG3_","colab_type":"code","colab":{}},"source":["colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', \n","            'speakerjobtitle', 'stateinfo','partyaffiliation', \n","            'barelytruecounts', 'falsecounts','halftruecounts',\n","            'mostlytrueocounts','pantsonfirecounts','context']\n","headlinecolname = 'headline_text'\n","labelcolname = 'encoded_label'\n","processedheadlinecolname = 'processed_headline_text'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qtKwTl1B8G18","colab_type":"code","colab":{}},"source":["#method to load data\n","colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo','partyaffiliation', 'barelytruecounts', 'falsecounts','halftruecounts','mostlytrueocunts','pantsonfirecounts','context']\n","def load_train_data():\n","    #load data\n","    train_filename = '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train.tsv'\n","    train_news = pd.read_csv(train_filename, sep='\\t', names = colnames,error_bad_lines=False)\n","    return train_news\n","\n","def load_test_data():\n","    test_filename = '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test.tsv'\n","    test_news = pd.read_csv(test_filename, sep='\\t', names = colnames,error_bad_lines=False)\n","    return test_news\n","\n","def load_valid_data():\n","    test_filename = '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/valid.tsv'\n","    test_news = pd.read_csv(test_filename, sep='\\t', names = colnames,error_bad_lines=False)\n","    return test_news"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O10qC2huqaB0","colab_type":"code","colab":{}},"source":["#X_train, X_test, y_train, y_test = train_test_split(fake_news_data, y, test_size=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ReiO4bko2el","colab_type":"text"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"LGFP9McQ8_y0","colab_type":"text"},"source":["## Kaggle Fake News"]},{"cell_type":"code","metadata":{"id":"_jx3glRGsKEI","colab_type":"code","colab":{}},"source":["fake_news_data = fake_news_data.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TV7JAy_Zo1sy","colab_type":"code","colab":{}},"source":["import re\n","\n","def cleaning(raw_news):\n","    \n","    # 1. Remove non-letters/Special Characters and Punctuations\n","    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n","    \n","    # 2. Convert to lower case.\n","    news =  news.lower()\n","    \n","    # 3. Tokenize.\n","    news_words = nltk.word_tokenize( news)\n","    \n","    # 4. Convert the stopwords list to \"set\" data type.\n","    stops = set(nltk.corpus.stopwords.words(\"english\"))\n","    \n","    # 5. Remove stop words. \n","    words = [w for w in  news_words  if not w in stops]\n","    \n","    # 6. Lemmentize \n","    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n","    \n","    # 7. Stemming\n","    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n","    \n","    # 8. Join the stemmed words back into one string separated by space, and return the result.\n","    return \" \".join(stems)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZRAXLkTq71H","colab_type":"code","colab":{}},"source":["import time\n","\n","t1 = time.time()\n","fake_news_data['clean_title'] = fake_news_data[\"title\"].apply(cleaning) \n","t2 = time.time()\n","print(\"\\nTime to clean, tokenize and stem title in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")\n","\n","t1 = time.time()\n","fake_news_data['clean_thread_title'] = fake_news_data[\"thread_title\"].apply(cleaning) \n","t2 = time.time()\n","print(\"\\nTime to clean, tokenize and stem thread_title in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")\n","\n","# Testing\n","# t1 = time.time()\n","# fake_news_data['clean_text'] = fake_news_data[\"text\"].apply(cleaning) \n","# t2 = time.time()\n","# print(\"\\nTime to clean, tokenize and stem text in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zhx7YrqLscvL","colab_type":"code","colab":{}},"source":["model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/Shared drives/CMPE 257: Machine Learning/AlterusVera-Datasets/GoogleNews-vectors-negative300.bin.gz', binary=True)\n","words = model.index2word\n","\n","w_rank = {}\n","for i,word in enumerate(words):\n","    w_rank[word] = i\n","\n","WORDS = w_rank"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NG4cEldS97uq","colab_type":"text"},"source":["#### Spell Checker"]},{"cell_type":"code","metadata":{"id":"KUhOgPegstJ3","colab_type":"code","colab":{}},"source":["import re\n","from collections import Counter\n","\n","def words(text): return re.findall(r'\\w+', text.lower())\n","\n","def P(word, N=sum(WORDS.values())): \n","    \"Probability of `word`.\"\n","    return - WORDS.get(word, 0)\n","\n","def correction(word): \n","    \"Most probable spelling correction for word.\"\n","    return max(candidates(word), key=P)\n","\n","def candidates(word): \n","    \"Generate possible spelling corrections for word.\"\n","    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n","\n","def known(words): \n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\n","    return set(w for w in words if w in WORDS)\n","\n","def edits1(word):\n","    \"All edits that are one edit away from `word`.\"\n","    letters    = 'abcdefghijklmnopqrstuvwxyz'\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    deletes    = [L + R[1:]               for L, R in splits if R]\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","    inserts    = [L + c + R               for L, R in splits for c in letters]\n","    return set(deletes + transposes + replaces + inserts)\n","\n","def edits2(word): \n","    \"All edits that are two edits away from `word`.\"\n","    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dw2kpRvosv51","colab_type":"code","colab":{}},"source":["def spell_checker(text):\n","    all_words = re.findall(r'\\w+', text.lower()) # split sentence to words\n","    spell_checked_text  = []\n","    for i in range(len(all_words)):\n","        spell_checked_text.append(correction(all_words[i]))\n","    return ' '.join(spell_checked_text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxWOSGS8swca","colab_type":"code","colab":{}},"source":["print(\"Before: \\n\", fake_news_data['clean_title'][0] )\n","t1 = time.time()\n","fake_news_data['clean_title'] = fake_news_data['clean_title'].apply(spell_checker)\n","t2 = time.time()\n","print(\"\\nTime to spell check the clean_title in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")\n","\n","print(\"\\nAfter: \\n\",fake_news_data['clean_title'][0] )\n","fake_news_data.head(2)\n","\n","print(\"Before: \\n\", fake_news_data['clean_thread_title'][0] )\n","t1 = time.time()\n","fake_news_data['clean_thread_title'] = fake_news_data['clean_thread_title'].apply(spell_checker)\n","t2 = time.time()\n","print(\"\\nTime to spell check the clean_thread_title in fake_news_data: \\n\", len(fake_news_data), \"news:\", (t2-t1)/60, \"min\")\n","\n","print(\"\\nAfter: \\n\",fake_news_data['clean_thread_title'][0] )\n","fake_news_data.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJK4otAUtMFt","colab_type":"code","colab":{}},"source":["fake_news_data['type'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hqqy_Lxy9IkU","colab_type":"text"},"source":["## Liar Liar Dataset"]},{"cell_type":"markdown","metadata":{"id":"xkAvpJaC9Tt9","colab_type":"text"},"source":["#### Label Encoding"]},{"cell_type":"code","metadata":{"id":"CergEAhe9MlH","colab_type":"code","colab":{}},"source":["#label encoding\n","true_labels = ['original','true','mostly-true','half-true']\n","false_labels = ['barely-true','false','pants-fire']\n","\n","def encode_news_type(input_label):\n","    if input_label in true_labels:\n","        return 1\n","    else:\n","        return 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pSTvfe29Zwh","colab_type":"text"},"source":["#### Remove Punctuations"]},{"cell_type":"code","metadata":{"id":"AzOAUrYf9XhS","colab_type":"code","colab":{}},"source":["#method to remove punctuations from textual data\n","def remove_punctuation(text):\n","    translator = str.maketrans('', '', string.punctuation)\n","    return text.translate(translator)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4IVMM5D9nXO","colab_type":"text"},"source":["#### Remove Stopwords"]},{"cell_type":"code","metadata":{"id":"YC_KMNp69mNu","colab_type":"code","colab":{}},"source":["from nltk.stem.snowball import SnowballStemmer\n","sw = stopwords.words('english')\n","stemmer = SnowballStemmer(\"english\")\n","\n","#Remove stop words\n","def remove_stopwords(text):\n","    text = [word.lower() for word in text.split() if word.lower() not in sw]\n","    return \" \".join(text)\n","\n","#Lemmetize and pos tagging\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","#Stemming\n","def stemming(text):    \n","    text = [stemmer.stem(word) for word in text.split()]\n","    return \" \".join(text)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1jxF9fK5-Hon","colab_type":"text"},"source":["#### Distillation on Test and Training Data"]},{"cell_type":"code","metadata":{"id":"uZIxKy4V-MAu","colab_type":"code","colab":{}},"source":["def text_preprocess(df):\n","  ## headline_text\n","    #encode labels\n","    df['encoded_label'] = df.apply(lambda row: encode_news_type(row['label']), axis=1)\n","    #convert to lower case\n","    df['processed_headline_text'] = df['headline_text'].str.lower()\n","    #remove stop words\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(remove_stopwords)\n","    #spell check\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(spell_checker)\n","    #Lemmetize\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(lemmatize_stemming)\n","    #stemming\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(stemming)\n","    #remove punctuation\n","    df['processed_headline_text'] = df['processed_headline_text'].apply(remove_punctuation)\n","    #remove less than 3 letter words\n","    df['processed_headline_text']  = df.processed_headline_text.apply(lambda i: ' '.join(filter(lambda j: len(j) > 3, i.split())))\n","    return df[['headline_text','processed_headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo', 'partyaffiliation', 'context', 'encoded_label']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X2wIl2d5-QUT","colab_type":"code","colab":{}},"source":["#load test and train data\n","train_news = load_train_data()\n","test_news = load_test_data()\n","valid_news = load_valid_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRwgjj0H-UsJ","colab_type":"code","colab":{}},"source":["#pre-processing train dataset\n","df_train = text_preprocess(train_news)\n","df_train.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YpqkUga3-2fa","colab_type":"code","colab":{}},"source":["#pre-processing test dataset\n","df_test = text_preprocess(test_news)\n","df_test.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgR1PheiTWG4","colab_type":"code","colab":{}},"source":["#pre-processing valid dataset\n","df_valid = text_preprocess(valid_news)\n","df_valid.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIfKy8ipT05L","colab_type":"code","colab":{}},"source":["train_news['index'] = train_news.index\n","\n","train_news.head()\n","\n","test_news['index'] = test_news.index\n","\n","test_news.head()\n","\n","valid_news['index'] = valid_news.index\n","\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0s_yBd_T6Lo","colab_type":"code","colab":{}},"source":["\n","train_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train_cleantext.csv\", sep=',')\n","test_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test_cleantext.csv\", sep=',')\n","valid_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/valid_cleantext.csv\", sep=',')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPqELfb7tq0R","colab_type":"text"},"source":["# Visualizations"]},{"cell_type":"markdown","metadata":{"id":"2mpukeztB0My","colab_type":"text"},"source":["## Kaggle Fake News"]},{"cell_type":"code","metadata":{"id":"dJZc7cPRtsnl","colab_type":"code","colab":{}},"source":["from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpjlZhebt3-0","colab_type":"code","colab":{}},"source":["def cloud(data,backgroundcolor = 'white', width = 800, height = 600):\n","    wordcloud = WordCloud(stopwords = STOPWORDS, background_color = backgroundcolor,\n","                         width = width, height = height).generate(data)\n","    plt.figure(figsize = (15, 10))\n","    plt.imshow(wordcloud)\n","    plt.axis(\"off\")\n","    plt.show()\n","    \n","cloud(' '.join(fake_news_data['clean_title']))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmcOce0oB6Uz","colab_type":"text"},"source":["## Liar Liar Dataset"]},{"cell_type":"code","metadata":{"id":"kwLS52niB-dc","colab_type":"code","colab":{}},"source":["#text length matrix\n","def word_length_visualisation(df):\n","    text_len = []\n","\n","    for i in range(len(df.headline_text)):\n","        text_len.append(len(str(df.headline_text[i])))\n","    plt.figure(figsize=(10,6))\n","    pd.DataFrame(text_len).hist(bins = 100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TerVTUR5CD26","colab_type":"code","colab":{}},"source":["print('Train Data Word Length Visualization')\n","word_length_visualisation(df_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5ORhNVQCH84","colab_type":"code","colab":{}},"source":["print('Test Data Word Length Visualization')\n","word_length_visualisation(df_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndP8kQimCQDn","colab_type":"code","colab":{}},"source":["def fake_news_distribution(df):\n","    #Training data set with text, domain ranking and type.\n","    X = df[['processed_headline_text', 'encoded_label']]\n","\n","    plt.title('Category Vs Count')\n","    X.groupby(['encoded_label']).size().plot(kind='barh', color='blue')\n","    plt.xlabel('count')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fg8cyfq1CT27","colab_type":"code","colab":{}},"source":["\n","print('Train Data Fake News Encoded Label Visualization')\n","fake_news_distribution(df_train)\n","\n","print('Test Data Fake News Encoded Label  Visualization')\n","fake_news_distribution(df_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAM_aIEjD2WP","colab_type":"text"},"source":["### Word cloud"]},{"cell_type":"code","metadata":{"id":"_5BMWVGmD_xP","colab_type":"code","colab":{}},"source":["#Word Cloud to Visualize important text\n","# lower max_font_size, change the maximum number of word and lighten the background\n","\n","def word_cloud_visualization(df):\n","    text = \" \".join(ht for ht in df.processed_headline_text)\n","    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n","    plt.figure()\n","    plt.imshow(wordcloud, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oRel3pKD6Pi","colab_type":"code","colab":{}},"source":["print('Train Data Word Cloud Visualization')\n","word_cloud_visualization(df_train)\n","\n","print('Test Data Word Cloud Visualization')\n","word_cloud_visualization(df_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z395m8coEL53","colab_type":"text"},"source":["### Word2Vec and t-SNE"]},{"cell_type":"code","metadata":{"id":"2CL00oLDEHQZ","colab_type":"code","colab":{}},"source":["\n","num_features = 15      # Word vector dimensionality                      \n","min_word_count = 40   # Minimum word count                        \n","num_workers = 4       # Number of threads to run in parallel\n","context = 10          # Context window size                                                                                    \n","downsampling = 1e-3   # Downsample setting for frequent words\n","\n","#Create Word2Vec Model. Get Vector from the model\n","word_tokens = []\n","for i in range(len(df_train)):\n","    words = df_train['processed_headline_text'][i].split()\n","    word_tokens.append(words)\n","    \n","word2vec_model = word2vec.Word2Vec(word_tokens, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3RRUh6GFFMuU","colab_type":"code","colab":{}},"source":["#\"Creates and TSNE model and plots it\"\n","from sklearn.manifold import TSNE\n","def tsne_plot(model):\n","    labels = []\n","    tokens = []\n","\n","    for word in model.wv.vocab:\n","        tokens.append(model[word])\n","        labels.append(word)\n","    \n","    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n","    new_values = tsne_model.fit_transform(tokens)\n","\n","    x = []\n","    y = []\n","    for value in new_values:\n","        x.append(value[0])\n","        y.append(value[1])\n","        \n","    plt.figure(figsize=(16, 16)) \n","    for i in range(len(x)):\n","        plt.scatter(x[i],y[i])\n","        plt.annotate(labels[i],\n","                     xy=(x[i], y[i]),\n","                     xytext=(5, 2),\n","                     textcoords='offset points',\n","                     ha='right',\n","                     va='bottom')\n","    plt.show()\n","\n","tsne_plot(word2vec_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y6XufHh2t-_U","colab_type":"text"},"source":["#Factor 1: Sentiment Analysis"]},{"cell_type":"code","metadata":{"id":"3-JIxmYV_fgj","colab_type":"code","colab":{}},"source":["snt = senti.polarity_scores(train_news[processedheadlinecolname][0])\n","print(\"{:-<40} \\n{}\".format([processedheadlinecolname][0], str(snt)))\n","print ([snt['neg'],snt['neu'],snt['pos']])\n","print (snt['compound'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbVVgvKQ_igB","colab_type":"code","colab":{}},"source":["\n","def identify_sentiment_on_text(text):\n","    snt = senti.polarity_scores(text)\n","    # print(\"{:-<40} \\n{}\".format(text, str(snt)))\n","    # print ([snt['neg'],snt['neu'],snt['pos']])\n","    # print (snt['compound'])\n","    return pd.Series([snt['compound'], [snt['neg'],snt['neu'],snt['pos']]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"18_AK6qz_ot_","colab_type":"code","colab":{}},"source":["def update_sentiment_results_to_dataset(dataframe,sentimentcolnames,coltoapplysentiment):\n","    dataframe[sentimentcolnames] = dataframe.apply(\n","    lambda row: identify_sentiment_on_text(\n","        row[coltoapplysentiment]), axis=1)\n","    return dataframe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVAJ6gmJ_r-7","colab_type":"code","colab":{}},"source":["sentimentcolnames = ['headline_sentiment_compound','headline_sentiment_polarity_vector']\n","train_news = update_sentiment_results_to_dataset(train_news,sentimentcolnames,processedheadlinecolname)\n","train_news.head()\n","\n","test_news = update_sentiment_results_to_dataset(test_news,sentimentcolnames,processedheadlinecolname)\n","test_news.head()\n","\n","valid_news = update_sentiment_results_to_dataset(valid_news,sentimentcolnames,processedheadlinecolname)\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PESSq9P7u0pn","colab_type":"text"},"source":["#Factor 2: LDA Topic Modelling"]},{"cell_type":"markdown","metadata":{"id":"lJF4566E2Y2P","colab_type":"text"},"source":["function to convert text to word tokens from cleaned dataset"]},{"cell_type":"code","metadata":{"id":"4Enx7eYK2VpI","colab_type":"code","colab":{}},"source":["def get_word_tokens(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if len(token) > 3:\n","            result.append(token)\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MHy2I7qZ2dme","colab_type":"code","colab":{}},"source":["class ProcessedDocuments(object):\n","    def __init__(self, *arrays):\n","        self.arrays = arrays\n"," \n","    def __iter__(self):\n","        for array in self.arrays:\n","            for document in array:\n","                for sent in nltk.sent_tokenize(document):\n","                    yield nltk.word_tokenize(sent)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gU9SBmzR2k-G","colab_type":"text"},"source":["Function to create a word2vec model"]},{"cell_type":"code","metadata":{"id":"sIWNaZZz2hs2","colab_type":"code","colab":{}},"source":["def get_word2vec(sentences, location):\n","    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n","    print('Model done training. Saving to disk as ' + 'models/' + location)\n","    model.save('models/' + location)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyH-XkH52rbO","colab_type":"text"},"source":["Function to tokenize docs"]},{"cell_type":"code","metadata":{"id":"CY4yRBqh2qnK","colab_type":"code","colab":{}},"source":["def get_tokenized_docs(dataframe,colname):\n","    documents_locale = dataframe[[colname]]\n","    tokenized_docs_local = documents_locale[colname].map(get_word_tokens)\n","    return tokenized_docs_local"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENqBPFBe21lf","colab_type":"text"},"source":["function to build the dictionary and tokenized docs for given features"]},{"cell_type":"code","metadata":{"id":"zcmHluX12yj_","colab_type":"code","colab":{}},"source":["def get_dictionary_print_words(dataframe,colname):\n","    tokenized_docs_local = get_tokenized_docs(dataframe,colname)\n","    dictionary_gensim = gensim.corpora.Dictionary(tokenized_docs_local)\n","    count = 0\n","    print('######## DICTIONARY Words and occurences ########')\n","    for k, v in dictionary_gensim.iteritems():\n","        print(k, v)\n","        count += 1\n","        if count > 10:\n","            break\n","    dictionary_gensim.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n","    return dictionary_gensim,tokenized_docs_local"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Hv4Z_sg2-w2","colab_type":"text"},"source":["function to build bow_corpus from dictionary and tokenized_docs"]},{"cell_type":"code","metadata":{"id":"N6T4K53l279H","colab_type":"code","colab":{}},"source":["def get_bow_corpus_print_sample(dataframe,colname):\n","    dictionary_gensim,tokenized_docs_local = get_dictionary_print_words(dataframe, colname)\n","    bow_corpus_local = [dictionary_gensim.doc2bow(doc) for doc in tokenized_docs_local]\n","    bow_doc_local_0 = bow_corpus_local[0]\n","    print('\\n ######## BOW VECTOR FIRST ITEM ########')\n","    print(bow_doc_local_0)\n","    print('\\n ######## PREVIEW BOW ########')\n","    for i in range(len(bow_doc_local_0)):\n","        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_local_0[i][0], \n","                                               dictionary_gensim[bow_doc_local_0[i][0]], bow_doc_local_0[i][1]))\n","    return bow_corpus_local,dictionary_gensim"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pttdas4i3HNe","colab_type":"text"},"source":["function to build tfidf_corpus from bow_corpus"]},{"cell_type":"code","metadata":{"id":"uNTJ9HfN3DHI","colab_type":"code","colab":{}},"source":["def get_tfidf_corpus_print_sample(bow_corpus_local):\n","    from gensim import corpora, models\n","    tfidf = models.TfidfModel(bow_corpus_local)\n","    tfidf_corpus_local = tfidf[bow_corpus_local]\n","    print('\\n ######## TFIDF VECTOR FIRST ITEM ########')\n","    \n","    from pprint import pprint\n","    for doc in tfidf_corpus_local:\n","        pprint(doc)\n","        break\n","    return tfidf_corpus_local"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMr1tRyz3OuZ","colab_type":"text"},"source":["function to run ldamodel and print top 10 topics"]},{"cell_type":"code","metadata":{"id":"Ky6lqZT23LXY","colab_type":"code","colab":{}},"source":["def get_lda_model_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n","    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2)\n","    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n","    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n","\n","    #Below Code Prints Topics and Words\n","    for topic,words in lda_topics_words:\n","        print(str(topic)+ \"::\"+ str(words))\n","    return lda_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"13QjfHRn5X9V","colab_type":"code","colab":{}},"source":["def get_lda_model_topics_topwords_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n","    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2, random_state=1)\n","    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n","    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n","\n","    #Below Code Prints Topics and Words\n","    for topic,words in lda_topics_words:\n","        print(str(topic)+ \"::\"+ str(words))\n","    return lda_model,lda_topics_words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"StN6Y1S_5gow","colab_type":"text"},"source":[" Function to enrich data with lda topics, lda topics score, top words"]},{"cell_type":"code","metadata":{"id":"aDmdG1Lf5bVN","colab_type":"code","colab":{}},"source":["def identify_topic_number_score_label_topwords(text,dictionary_local,lda_model_local,lda_topics_top_words_local):\n","    bow_vector_local = dictionary_local.doc2bow(get_word_tokens(text))\n","    topic_number_local, topic_score_local = sorted(\n","        lda_model_local[bow_vector_local], key=lambda tup: -1*tup[1])[0]\n","    #print (topic_number_local, topic_score_local)\n","    return pd.Series([topic_number_local, topic_score_local,\" \".join(lda_topics_top_words_local[int(topic_number_local)][1])])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lw0K3kxg5ryk","colab_type":"text"},"source":["function that can enrich topic data to dataframe"]},{"cell_type":"code","metadata":{"id":"UEFoH-rY5oUf","colab_type":"code","colab":{}},"source":["def update_lda_results_to_dataset(dataframe,topiccolnames,coltoapplylda,colnamedictionary,colnameldamodel, colnameldatopwords):\n","    dataframe[topiccolnames] = dataframe.apply(\n","    lambda row: identify_topic_number_score_label_topwords(\n","        row[coltoapplylda],colnamedictionary,colnameldamodel,\n","        colnameldatopwords), axis=1)\n","    return dataframe"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHfmSsse5xQC","colab_type":"text"},"source":["## Bag of Words"]},{"cell_type":"code","metadata":{"id":"SlDbQrJb51_n","colab_type":"code","colab":{}},"source":["bow_corpus_headline,dictionary_headline = get_bow_corpus_print_sample(train_news,processedheadlinecolname)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5I39cTs58ub","colab_type":"text"},"source":["## LDA using Bag of Words"]},{"cell_type":"code","metadata":{"id":"T_yUe-9X551w","colab_type":"code","colab":{}},"source":["lda_model_headline,lda_headline_topic_words = get_lda_model_topics_topwords_print_top_topics(\n","    bow_corpus_headline,10,dictionary_headline)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SdY7AWsS6FEg","colab_type":"text"},"source":["## LDA using TF-IDF"]},{"cell_type":"code","metadata":{"id":"yAzWRKOK6G4N","colab_type":"code","colab":{}},"source":["tfidf_corpus_headline = get_tfidf_corpus_print_sample(bow_corpus_headline)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTeM5bTX6McW","colab_type":"code","colab":{}},"source":["lda_tfidf_model_headline = get_lda_model_print_top_topics(tfidf_corpus_headline,10,dictionary_headline)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6Kaje866jHH","colab_type":"text"},"source":["Creating a Word2Vec model with the list of headline_text"]},{"cell_type":"code","metadata":{"id":"qDXWJYO-9hE8","colab_type":"code","colab":{}},"source":["# check this code later\n","'''\n","w2vec = get_word2vec(\n","    ProcessedDocuments(\n","        train_news[processedheadlinecolname].values, \n","        test_news[processedheadlinecolname].values, \n","        valid_news[processedheadlinecolname].values\n","    ),\n","    '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/w2vmodel'\n",")\n","\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nqsLm10f9M34","colab_type":"code","colab":{}},"source":["semisupervised_headline_topic_labels = ['american_clinton_vote','trump_republic_vote',\n","                                        'obama_immigr_support','health_care_job',\n","                                        'employe_percent_secur','school_spend_democrat',\n","                                        'tax_rais_debt','american_peopl_spend',\n","                                        'creat_job_million','feder_cost_colleg']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_DKTTHvE9T74","colab_type":"text"},"source":["Assigning Topic details to Train, Test and Valid Data Set"]},{"cell_type":"code","metadata":{"id":"sWVtnfrL9QQ_","colab_type":"code","colab":{}},"source":["headlinetopiccolnames = ['headline_lda_topic_number','headline_lda_topic_score','headline_lda_topic_topwords']\n","train_news = update_lda_results_to_dataset(\n","    train_news,headlinetopiccolnames,headlinecolname,\n","    dictionary_headline,lda_model_headline,lda_headline_topic_words)\n","train_news.head()\n","\n","test_news = update_lda_results_to_dataset(\n","    test_news,headlinetopiccolnames,headlinecolname,\n","    dictionary_headline,lda_model_headline,lda_headline_topic_words)\n","test_news.head()\n","\n","valid_news = update_lda_results_to_dataset(\n","    valid_news,headlinetopiccolnames,headlinecolname,\n","    dictionary_headline,lda_model_headline,lda_headline_topic_words)\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ba7NAUW492B3","colab_type":"text"},"source":["## LDA2VEC"]},{"cell_type":"code","metadata":{"id":"NSU_FkUQ9-KL","colab_type":"code","colab":{}},"source":["# check this code later\n","'''\n","lda2vec = get_word2vec(\n","    ProcessedDocuments(\n","        train_news['headline_lda_topic_topwords'].values, \n","        test_news['headline_lda_topic_topwords'].values, \n","        valid_news['headline_lda_topic_topwords'].values\n","    ),\n","    'lda2vecmodel'\n",")\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s062OvtV-FYY","colab_type":"code","colab":{}},"source":["# tsne_plot(lda2vec,8,8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xn0hoeMUCKVN","colab_type":"text"},"source":["## Adding Feature- Context"]},{"cell_type":"code","metadata":{"id":"_eLxA9Q0CQ42","colab_type":"code","colab":{}},"source":["contextcolname = 'context'\n","processedcontextcolname = 'processed_context'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZ51THrnCUwZ","colab_type":"code","colab":{}},"source":["def preprocess_context(text):\n","    text = lemmatize_stemming(text)\n","    #print('Lemmatized :: ' + text)\n","    text = remove_punctuation(text)\n","    #print('Punctuation removed :: ' + text)\n","    return text\n","preprocess_context(str('a floor speech.'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PHdgu8UyClkm","colab_type":"text"},"source":["Apply Preprocessing for all datasets on Context"]},{"cell_type":"code","metadata":{"id":"NIjvBVaxCfM8","colab_type":"code","colab":{}},"source":["def apply_context_preprocess_dataframe(dflocal):\n","    dflocal[contextcolname] = dflocal[contextcolname].fillna('others')\n","    dflocal[processedcontextcolname] = dflocal.apply(\n","        lambda row: preprocess_context(str(row[contextcolname])), axis=1)\n","    return dflocal\n","\n","train_news = apply_context_preprocess_dataframe(train_news)\n","test_news = apply_context_preprocess_dataframe(test_news)\n","valid_news = apply_context_preprocess_dataframe(valid_news)\n","\n","dfcontextunique = train_news\n","dfcontextunique = dfcontextunique.append([test_news, valid_news])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2so67D8ZC55T","colab_type":"code","colab":{}},"source":["\n","len(dfcontextunique.context.unique())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YheVlfnQDDuE","colab_type":"code","colab":{}},"source":["bow_corpus_context,dictionary_context = get_bow_corpus_print_sample(dfcontextunique,contextcolname)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUhdCcyEDU-6","colab_type":"code","colab":{}},"source":["semisupervised_context_topics = ['interview_campaign_appearance','pressrelease_media',\n","                                 'book_newsletter','presidential_debate',\n","                                 'townhall_meeting','senate_house_floor',\n","                                 'news_release_nation','twitter_online_campaign'\n","                                'interview_media','public_comments']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2T9Zo7hDd5d","colab_type":"text"},"source":["Assign context topics back to dataset"]},{"cell_type":"code","metadata":{"id":"xSy2ORDqDaWE","colab_type":"code","colab":{}},"source":["contexttopiccolnames = ['context_lda_topic_number','context_lda_topic_score','context_lda_topic_topwords']\n","contexttopicnamecol = 'context_lda_topic_name'\n","train_news = update_lda_results_to_dataset(\n","    train_news,contexttopiccolnames,contextcolname,\n","    dictionary_context,lda_model_context,lda_context_topic_words)\n","train_news[contexttopicnamecol] = train_news[processedcontextcolname] = train_news.apply(\n","        lambda row: semisupervised_context_topics[int(row['context_lda_topic_number'])-1], axis=1)\n","train_news.head()\n","\n","test_news = update_lda_results_to_dataset(\n","    test_news,contexttopiccolnames,contextcolname,\n","    dictionary_headline,lda_model_headline,lda_headline_topic_words)\n","test_news[contexttopicnamecol] = test_news[processedcontextcolname] = test_news.apply(\n","        lambda row: semisupervised_context_topics[int(row['context_lda_topic_number'])-1], axis=1)                                                 \n","test_news.head()\n","\n","valid_news = update_lda_results_to_dataset(\n","    valid_news,contexttopiccolnames,contextcolname,\n","    dictionary_headline,lda_model_headline,lda_headline_topic_words)\n","valid_news[contexttopicnamecol] = valid_news[processedcontextcolname] = valid_news.apply(\n","        lambda row: semisupervised_context_topics[int(row['context_lda_topic_number'])-1], axis=1)\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WGv5bjGDOfp","colab_type":"text"},"source":["##  LDA to get top topics for context"]},{"cell_type":"code","metadata":{"id":"N8Kj9pATDIXN","colab_type":"code","colab":{}},"source":["lda_model_context,lda_context_topic_words = get_lda_model_topics_topwords_print_top_topics(\n","    bow_corpus_context,10,dictionary_context)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4gjlivUDjyG","colab_type":"text"},"source":["Saving data for future reference"]},{"cell_type":"code","metadata":{"id":"R_AB3iV4Di9I","colab_type":"code","colab":{}},"source":["train_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train_contextlda.csv\", sep=',')\n","test_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test_contextlda.csv\", sep=',')\n","valid_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/valid_contextlda.csv\", sep=',')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzwtc3QDEIuP","colab_type":"text"},"source":["Train News Distribution against context"]},{"cell_type":"code","metadata":{"id":"Wy0nVJAIEEw6","colab_type":"code","colab":{}},"source":["create_distribution(train_news, contexttopicnamecol)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMLzS_erFoB4","colab_type":"text"},"source":["## Tagging Documents against context Train, Test and Valid"]},{"cell_type":"code","metadata":{"id":"SvANcYyYFtjV","colab_type":"code","colab":{}},"source":["from gensim.models.doc2vec import TaggedDocument\n","from gensim.models import Doc2Vec\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0g1CTH2Fyw_","colab_type":"text"},"source":["Function to tag documents"]},{"cell_type":"code","metadata":{"id":"kFZfc0XKFxnC","colab_type":"code","colab":{}},"source":["#tag each headline text with the label found in previous step\n","def tag_headline(data, coltotag, colusedtotag):\n","    tagged_data = []\n","    for index, row in data.iterrows():\n","        tagged_data.append(TaggedDocument(words=get_word_tokens(row[coltotag]), tags=[row[colusedtotag]]))\n","    return tagged_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ve5WYjmOF3aZ","colab_type":"code","colab":{}},"source":["#tag train data and run doc2vec model\n","context_labelled_headlines_train = tag_headline(train_news, processedheadlinecolname, contexttopicnamecol)\n","print(context_labelled_headlines_train[:3])\n","\n","#tag train data and run doc2vec model\n","context_labelled_headlines_test = tag_headline(test_news, processedheadlinecolname, contexttopicnamecol)\n","print(context_labelled_headlines_test[:3])\n","\n","#tag train data and run doc2vec model\n","context_labelled_headlines_valid = tag_headline(valid_news, processedheadlinecolname, contexttopicnamecol)\n","print(context_labelled_headlines_valid[:3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1v1pnoXGAuJ","colab_type":"text"},"source":["Apply Doc2Vec on Tagged Documents to created doc2vec models"]},{"cell_type":"code","metadata":{"id":"Uwmu5H1tF8Uy","colab_type":"code","colab":{}},"source":["doc2vec_model_train = Doc2Vec(documents = context_labelled_headlines_train,\n","                              dm=0, num_features=500, min_count=2, size=21, window=4)\n","doc2vec_model_test= Doc2Vec(documents = context_labelled_headlines_test,\n","                              dm=0, num_features=500, min_count=2, size=21, window=4)\n","doc2vec_model_valid= Doc2Vec(documents = context_labelled_headlines_valid,\n","                              dm=0, num_features=500, min_count=2, size=21, window=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UxJTpn_MGHZ8","colab_type":"code","colab":{}},"source":["#Method to convert doc2vec model and tagged documents into vectors training and testing\n","def create_vector_for_learning(model, tagged_docs):\n","    #documents = tagged_docs.values\n","    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in tagged_docs])\n","    return targets, regressors"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Not13cMZGOM4","colab_type":"text"},"source":["Doc2Vec Model to Vector converter"]},{"cell_type":"code","metadata":{"id":"yaL0vn5SGK3b","colab_type":"code","colab":{}},"source":["def create_docvector(doc2vec_model,label):\n","    return doc2vec_model.docvecs[label]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6Ah09WMGVYr","colab_type":"text"},"source":["Adding vector for every text in the dataset"]},{"cell_type":"code","metadata":{"id":"QaM_ccHwGSoA","colab_type":"code","colab":{}},"source":["def apply_context_doc2vec(dflocal,doc2vec_model, vectorcolname, label):\n","    dflocal[vectorcolname] = dflocal.apply(\n","        lambda row: create_docvector(doc2vec_model,str(row[label])), axis=1)\n","    return dflocal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBEony-aGfY9","colab_type":"text"},"source":["Updating Dataset with vectors identified by Doc2Vec applied on context"]},{"cell_type":"code","metadata":{"id":"sUaiMKm_GaWE","colab_type":"code","colab":{}},"source":["contextdoc2veccolumn = 'content_doc2vec_vector'\n","train_news = apply_context_doc2vec(train_news, doc2vec_model_train,contextdoc2veccolumn,contexttopicnamecol)\n","train_news.head()\n","\n","test_news = apply_context_doc2vec(test_news, doc2vec_model_test,contextdoc2veccolumn,contexttopicnamecol)\n","test_news.head()\n","\n","valid_news = apply_context_doc2vec(valid_news, doc2vec_model_valid,contextdoc2veccolumn,contexttopicnamecol)\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0jsJ8riGlBR","colab_type":"code","colab":{}},"source":["create_docvector(doc2vec_model_train,train_news[contexttopicnamecol][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujTCXPZlGo_x","colab_type":"code","colab":{}},"source":["[train_news['headline_lda_topic_score'][0],\n","           train_news['headline_sentiment_compound'][0],\n","            create_docvector(doc2vec_model_train,train_news[contexttopicnamecol][0])]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vdvExmzIGswI","colab_type":"code","colab":{}},"source":["\n","print (type(train_news[contextdoc2veccolumn][0]))\n","np.append(train_news[contextdoc2veccolumn][0],\n","          (train_news['headline_lda_topic_score'][0],train_news['headline_sentiment_compound'][0]))\n","\n","##pd.concat([train_news['headline_lda_topic_score'],\n","           ##train_news['headline_sentiment_compound'],train_news[contextvectorcolumn][0]],axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHmF3XiXGy-J","colab_type":"code","colab":{}},"source":["#### Append LDA, Sentiment, Feature Vector\n","def apply_feature_distill_vector(dflocal, vectorcolname, doc2veccolname):\n","    dflocal[vectorcolname] = dflocal.apply(\n","        lambda row: np.append(row[doc2veccolname],\n","          (row['headline_lda_topic_score'],row['headline_sentiment_compound'])), axis=1)\n","    return dflocal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jBsxDy8hG5mZ","colab_type":"text"},"source":["## Creating Feature Vector for Context\n","\n","* Append Context Feature Doc2Vec column\n","*Append LDA topic retrieved through distillation\n","*Append Sentiment Analysis Compount retrieved through distillation"]},{"cell_type":"code","metadata":{"id":"ESKLR118G2gp","colab_type":"code","colab":{}},"source":["contextfeaturevector = 'contextfeaturevector'\n","\n","train_news = apply_feature_distill_vector(train_news, contextfeaturevector, contextdoc2veccolumn)\n","test_news = apply_feature_distill_vector(test_news,contextfeaturevector, contextdoc2veccolumn)\n","valid_news = apply_feature_distill_vector(valid_news,contextfeaturevector, contextdoc2veccolumn)\n","\n","train_news.head(3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XiMn4nCNHVwZ","colab_type":"code","colab":{}},"source":["def getcontextfeaturetrainvector():\n","    sentiment = train_news.headline_sentiment_compound.reset_index()['headline_sentiment_compound']\n","    topic = train_news.headline_lda_topic_score.reset_index()['headline_lda_topic_score']\n","    context_doc2vec = []\n","    for i in range(len(train_news[contexttopicnamecol])):\n","        context_value = train_news[contexttopicnamecol][i]\n","        context = doc2vec_model_train[context_value]\n","        context_doc2vec.append(context)\n","    \n","    context_vector = pd.concat([sentiment,topic, pd.DataFrame(context_doc2vec)],axis=1)\n","    return context_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLvtwwvYHY9z","colab_type":"code","colab":{}},"source":["def getcontextfeaturetestvector():\n","    sentiment = test_news.headline_sentiment_compound.reset_index()['headline_sentiment_compound']\n","    topic = test_news.headline_lda_topic_score.reset_index()['headline_lda_topic_score']\n","    context_doc2vec = []\n","    for i in range(len(test_news[contexttopicnamecol])):\n","        context_value = test_news[contexttopicnamecol][i]\n","        context = doc2vec_model_test[context_value]\n","        context_doc2vec.append(context)\n","    \n","    context_vector = pd.concat([sentiment,topic, pd.DataFrame(context_doc2vec)],axis=1)\n","    return context_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t84TX0RTHj02","colab_type":"code","colab":{}},"source":["context_vector = getcontextfeaturetrainvector()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeHeRWgBHuTB","colab_type":"code","colab":{}},"source":["context_vector_test = getcontextfeaturetestvector()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgGp48_HHx2S","colab_type":"code","colab":{}},"source":["#Get train and test data for classification\n","X_train = getcontextfeaturetrainvector()\n","y_train = train_news['encoded_label']\n","\n","X_test = getcontextfeaturetestvector()\n","y_test = test_news['encoded_label']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IrrKREKH2W6","colab_type":"code","colab":{}},"source":["\n","#perform vector classification using Logistic Regression\n","from sklearn.linear_model import  LogisticRegression\n","fake_news_classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n","fake_news_classifier.fit(X_train, y_train)\n","y_pred = fake_news_classifier.predict(X_test)\n","estimate_score(y_test, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NvVOcJcmH-y-","colab_type":"code","colab":{}},"source":["feature = []\n","for index,row in X_test.iterrows():\n","    feature.append(row.tolist())\n","    \n","final_data = []\n","for i in range(len(test_news)):\n","    data = {}\n","    headline = test_news['headline_text'][i]\n","    encoded_label = test_news['encoded_label'][i]\n","    context_vector = feature[i]\n","    \n","    data = {'headline_text':headline, 'encoded_label':encoded_label, 'context_vector':context_vector}\n","    final_data.append(data)\n","\n","test_feature = pd.DataFrame(final_data)\n","test_feature.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test_contextfeature.csv\", sep=',')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DsZp7vZ0IF3i","colab_type":"code","colab":{}},"source":["feature = []\n","for index,row in X_train.iterrows():\n","    feature.append(row.tolist())\n","    \n","final_data = []\n","for i in range(len(train_news)):\n","    data = {}\n","    headline = train_news['headline_text'][i]\n","    encoded_label = train_news['encoded_label'][i]\n","    context_vector = feature[i]\n","    \n","    data = {'headline_text':headline, 'encoded_label':encoded_label, 'context_vector':context_vector}\n","    final_data.append(data)\n","\n","train_feature = pd.DataFrame(final_data)\n","train_feature.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train_contextfeature.csv\", sep=',')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRRJtF_TIKUE","colab_type":"code","colab":{}},"source":["train_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train_contextfeature.csv\", sep=',')\n","test_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test_contextfeature.csv\", sep=',')\n","valid_news.to_csv(\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/valid_contextfeature.csv\", sep=',')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4cRRL_caw_H5","colab_type":"text"},"source":["#Factor 3: Sensational Feature Analysis"]},{"cell_type":"markdown","metadata":{"id":"DbsbJ6f0Stnn","colab_type":"text"},"source":["# Factor 4: Political Affiliation Model"]},{"cell_type":"markdown","metadata":{"id":"l-2A0s0PF-Dy","colab_type":"text"},"source":["## Analysis on Political Affiliation Distribution"]},{"cell_type":"code","metadata":{"id":"I262y0nKFva4","colab_type":"code","colab":{}},"source":["\n","def political_affiliation_distribution(df):\n","    #Training data set with text, domain ranking and type.\n","    X = df[['processed_headline_text', 'partyaffiliation']]\n","\n","    plt.title('Category Vs Count')\n","    X.groupby(['partyaffiliation']).size().plot(kind='barh', color='blue')\n","    plt.xlabel('count')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdXUQY0yF4xi","colab_type":"code","colab":{}},"source":["def political_affiliation_distribution(df):\n","    #Training data set with text, domain ranking and type.\n","    X = df[['processed_headline_text', 'partyaffiliation']]\n","\n","    plt.title('Category Vs Count')\n","    X.groupby(['partyaffiliation']).size().plot(kind='barh', color='blue')\n","    plt.xlabel('count')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a5MxNowAF0Ms","colab_type":"text"},"source":["From the Distribution graphs below we notice that data is distributed highly between 4 labels Democrat, Republican, Independent and None. For understanding the classification based on political affiliation, I will consider only these 4 labels. I will also encode all the other tags as Other"]},{"cell_type":"code","metadata":{"id":"13-BDsUeGHwu","colab_type":"code","colab":{}},"source":["print('Train Data Party Affiliation Label Distribution Visualization')\n","political_affiliation_distribution(df_train)\n","\n","print('Test Data Party Affiliation Label Distribution Visualization')\n","political_affiliation_distribution(df_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KqhoSmrGU0l","colab_type":"text"},"source":["## Label encoding : Political Party"]},{"cell_type":"code","metadata":{"id":"uIZN1tm5GTFO","colab_type":"code","colab":{}},"source":["#label encoding\n","labels = ['democrat','republican','independent']\n","\n","def encode_party_affiliation_type(input_label):\n","    if input_label not in labels:\n","        return str('other')\n","    else:\n","        return input_label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQgx3Ea1Gcrv","colab_type":"code","colab":{}},"source":["df_train['partyaffiliation'] = df_train.apply(lambda row: encode_party_affiliation_type(row['partyaffiliation']), axis=1)\n","df_test['partyaffiliation'] = df_test.apply(lambda row: encode_party_affiliation_type(row['partyaffiliation']), axis=1)\n","\n","print('Train Data Party Affiliation Encoded Label Distribution Visualization')\n","political_affiliation_distribution(df_train)\n","\n","print('Test Data Party Affiliation Encoded Label Distribution Visualization')\n","political_affiliation_distribution(df_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NU5M2lQpG0mG","colab_type":"text"},"source":["Party affilition labels are encoded as follows: 1: Republican, 2: Democrat, 3:Other, 4:Independent. I have considered the distribution of labels for encoding. As more records are Republican I have assigned the least values label to avoid bias due to more data being present."]},{"cell_type":"code","metadata":{"id":"U-Efcl5XG3eI","colab_type":"code","colab":{}},"source":["#encode party affiliation labels\n","def convert_partyaffiliation_category(df):\n","    partyaffiliation_dict = {'independent':4, 'other':3, 'democrat':2, 'republican':1}\n","    pa = []\n","    for index,row in df.iterrows():\n","        pa.append(partyaffiliation_dict[row['partyaffiliation']])\n","    return pa\n","\n","pa_encode_train = pd.DataFrame(convert_partyaffiliation_category(df_train))\n","df_train['partyaffiliation_encode'] = pa_encode_train\n","\n","pa_encode_test = pd.DataFrame(convert_partyaffiliation_category(df_test))\n","df_test['partyaffiliation_encode'] = pa_encode_test\n","\n","df_train.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nOLypTNqHFz_","colab_type":"text"},"source":["## TF-IDF\n","\n","* Based on party affiliaition feature present in the Liar Liar data set, I have identified the top most occuring words for each of the category.\n","* Use this as the dictionary and perform a manual TF-IDF evaluation.\n","* Classify TF-IDF vectors into different political affiliation."]},{"cell_type":"code","metadata":{"id":"W_RVydYHHIBt","colab_type":"code","colab":{}},"source":["#Method to find the frequency of words per party affiliation tag\n","def find_freq_dist_per_party(df, label):\n","    frequencty_dist = []\n","    data = df[df['partyaffiliation'] == str(label)]\n","    text_str = ' '.join(data['processed_headline_text'])\n","    allWords = nltk.tokenize.word_tokenize(text_str)\n","    allWordsDist = nltk.FreqDist(w.lower() for w in allWords)\n","    for word, frequency in allWordsDist.most_common(10):\n","        dist = {\"word\":word, 'frequency':frequency, 'label':label}\n","        frequencty_dist.append(dist)\n","    return frequencty_dist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-U_8_pjHVcc","colab_type":"code","colab":{}},"source":["frequency_dist = find_freq_dist_per_party(df_train, 'democrat')\n","frequency_dist.extend(find_freq_dist_per_party(df_train, 'republican'))\n","frequency_dist.extend(find_freq_dist_per_party(df_train, 'independent'))\n","frequency_dist.extend(find_freq_dist_per_party(df_train, 'other'))\n","\n","party_affiliation_dict = pd.DataFrame(frequency_dist)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_q7TwkqcHYaQ","colab_type":"code","colab":{}},"source":["\n","#Find frequency of dictionary in each doc\n","#Divide by total number of words in dictionary\n","def computeTF(df, dictionary):\n","    TF = []\n","    dict_words = dictionary['word'].unique()\n","    for index, row in df.iterrows():\n","        row_freq = []\n","        words = row['processed_headline_text'].split()\n","        for i in range(len(dict_words)):\n","            frequency = float(words.count(dict_words[i])/len(dict_words))\n","            row_freq.append(frequency)\n","        TF.append(row_freq)\n","    return TF"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKB863s4HczW","colab_type":"code","colab":{}},"source":["#Calculate IDF for the dictionary\n","import math\n","def computeIDF(df, dictionary):\n","    IDF = []\n","    dict_words = dictionary['word'].unique()\n","    num_of_docs = len(df)\n","    for i in range(len(dict_words)):\n","        count = 0\n","        for index,row in df.iterrows():\n","            if dict_words[i] in row['processed_headline_text']:\n","                count += 1\n","        IDF.append(math.log(num_of_docs/count))\n","    return IDF"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8Dl7AVvHgOB","colab_type":"code","colab":{}},"source":["\n","#Calculate TF-IDF for each headline text based on the dictionary created\n","def computeTFIDF(TF, IDF):\n","    TFIDF = []\n","    IDF = np.asarray(IDF)\n","    for j in TF:\n","        tfidf = np.asarray(j) * IDF.T\n","        TFIDF.append(tfidf)\n","    return TFIDF"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6_5aw63HlPU","colab_type":"code","colab":{}},"source":["\n","\n","#Method to calculate model accuracy, precision\n","def estimate_score(y_test, y_pred):\n","    score(y_test, y_pred, average='macro')\n","    precision, recall, fscore, support = score(y_test, y_pred)\n","    \n","    print('accuracy: {}'.format(accuracy_score(y_test,y_pred)*100))\n","    print('precision: {}'.format(precision))\n","    print('recall: {}'.format(recall))\n","    print('fscore: {}'.format(fscore))\n","    print('support: {}'.format(support))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQ-El2IZHyEh","colab_type":"code","colab":{}},"source":["#TF-IDF for train data\n","TF_scores = computeTF(df_train, party_affiliation_dict)\n","IDF_scores = computeIDF(df_train, party_affiliation_dict)\n","TFIDF_scores = computeTFIDF(TF_scores, IDF_scores)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qkN3zVAH27B","colab_type":"code","colab":{}},"source":["#TF-IDF for test data\n","TF_test_scores = computeTF(df_test, party_affiliation_dict)\n","IDF_test_scores = computeIDF(df_test, party_affiliation_dict)\n","TFIDF_stest_cores = computeTFIDF(TF_test_scores, IDF_test_scores)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F2KQhI4tJqwI","colab_type":"text"},"source":["## Classification based on TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"w0kM2ScEK1vj","colab_type":"text"},"source":["### Logistic Refression"]},{"cell_type":"code","metadata":{"id":"ahxGVXooJokg","colab_type":"code","colab":{}},"source":["#Political affiliation classification based on TF-IDF\n","from sklearn.linear_model import LogisticRegression\n","X, y = TFIDF_scores, df_train['partyaffiliation_encode']\n","logistic_regression = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n","clf = logistic_regression.fit(X, y)\n","y_pred = logistic_regression.predict(TFIDF_stest_cores)\n","estimate_score(df_test['partyaffiliation_encode'], y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrK0YovgK61i","colab_type":"text"},"source":["### Random Forest"]},{"cell_type":"code","metadata":{"id":"CSSrl2tNKaER","colab_type":"code","colab":{}},"source":["f_cov = RandomForestClassifier(n_estimators=10)\n","f_cov.fit(X, y)\n","y_pred = f_cov.predict(TFIDF_stest_cores)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xb2xP5upKlBK","colab_type":"code","colab":{}},"source":["from sklearn import metrics\n","\n","'Accuracy', metrics.accuracy_score(df_test['partyaffiliation_encode'], y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Otgmkf7gLJ7Y","colab_type":"text"},"source":["# Factor 5: Clickbait\n","\n","Steps to accomplish\n","\n","* Identify ways to increase clickbait\n","\n","> *Length of Headline\n","\n","> *Question mark\n","\n","> *Exclamation\n","\n","> *Caps Ratio\n","\n","* Enrich dataset with new topic labels as defined above\n","* If any of the above is present then it is marked as clickbait\n","* Tag the documents using gensim tagging\n","* Apply doc2vec on the tagged documents\n","* Distill it with LDA topics on headline\n","* Distill it with Sentiment Analysis on headline\n","* New vector formed will be part of our polynomial equation"]},{"cell_type":"markdown","metadata":{"id":"S_8Ter8RP_qP","colab_type":"text"},"source":["## Identify Exclamation"]},{"cell_type":"code","metadata":{"id":"MY-uh2hDP-vD","colab_type":"code","colab":{}},"source":["def get_exclamation(text):\n","    if '!' in text:\n","        return 1\n","    else:\n","        return 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myNYWZRvQI23","colab_type":"text"},"source":["## Identify Questionmark"]},{"cell_type":"code","metadata":{"id":"mNop_ZuOQIFP","colab_type":"code","colab":{}},"source":["def get_question(text):\n","    if '?' in text:\n","        return 1\n","    else:\n","        return 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EYrzIJMQTzw","colab_type":"text"},"source":["## Length of CAPS_RATIO"]},{"cell_type":"code","metadata":{"id":"fmjCqJhCQRlS","colab_type":"code","colab":{}},"source":["\n","def get_caps_ratio(text):\n","    temp_np_title = text.strip(string.punctuation).split()\n","    np_title = [word.strip(string.punctuation) for word in temp_np_title]\n","    final_title = [word for word in np_title if word not in set(stopwords.words('english'))]\n","    final_title = \" \".join(final_title)\n","    num_caps = len([elem for elem in final_title if elem.isupper()])\n","    num_words = len([elem for elem in final_title if elem == ' ']) + 1\n","    ratio = num_caps / num_words\n","    return ratio"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3a4-QSe6QhFf","colab_type":"text"},"source":["## Enrich dataset \n","\n","with presence of exclamation,caps_ratio,questionmark and length"]},{"cell_type":"code","metadata":{"id":"D5pySI5AQd-w","colab_type":"code","colab":{}},"source":["clickbait_features = ['question','exclamation','caps_ratio','length_of_text']\n","def update_data_set_for_clickbait_features(dataframe):\n","    dataframe[clickbait_features] = dataframe.apply(lambda row: pd.Series([get_question(row[headlinecolname]), get_exclamation(row[headlinecolname]),get_caps_ratio(row[headlinecolname]),str(len(row[headlinecolname]))]),axis=1)\n","    return dataframe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZjCX-unRzaR","colab_type":"code","colab":{}},"source":["train_news.head(3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WqhuBUXQoW3","colab_type":"code","colab":{}},"source":["train_news = update_data_set_for_clickbait_features(train_news)\n","train_news.head()\n","\n","test_news = update_data_set_for_clickbait_features(test_news)\n","test_news.head()\n","\n","valid_news = update_data_set_for_clickbait_features(valid_news)\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5bTP-G0eYJbp","colab_type":"text"},"source":["Adding clickbait label with yes or no"]},{"cell_type":"code","metadata":{"id":"v3xpCUAQYFiM","colab_type":"code","colab":{}},"source":["def clickbait(question,exclamation,caps_ratio, text_length):\n","    if any([question==1, exclamation==1 , caps_ratio > 1]):\n","        return 'yes'\n","    return 'no'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBAIZ9MIYSgl","colab_type":"code","colab":{}},"source":["clickbaitfeature = 'clickbait_feature'\n","def update_data_set_for_clickbait_label(dataframe):\n","    dataframe['clickbait_feature'] = dataframe.apply(lambda row: clickbait(row['question'],\n","                                                                            row['exclamation'],\n","                                                                            row['caps_ratio'],\n","                                                                           int(row['length_of_text'])), axis=1)\n","    return dataframe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXBrVraxYYXB","colab_type":"code","colab":{}},"source":["train_news = update_data_set_for_clickbait_label(train_news)\n","train_news.head()\n","\n","test_news = update_data_set_for_clickbait_label(test_news)\n","test_news.head()\n","\n","valid_news = update_data_set_for_clickbait_label(valid_news)\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ja72MvGfYrTL","colab_type":"code","colab":{}},"source":["def create_distribution(dataFile):\n","    return sb.countplot(x=labelcolname, data=dataFile, palette='hls')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUNnJdTsYytS","colab_type":"code","colab":{}},"source":["import seaborn as sb\n","def create_distribution(dataFile, colname):\n","    g = sb.countplot(x=colname, data=dataFile, palette='hls')\n","    g.set_xticklabels(g.get_xticklabels(),rotation=90)\n","\n","    return g"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iaZDnEscYfca","colab_type":"code","colab":{}},"source":["create_distribution(train_news,'clickbait_feature')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ls3Ce15FY9pZ","colab_type":"code","colab":{}},"source":["create_distribution(test_news,'clickbait_feature')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PR83iRctZMha","colab_type":"text"},"source":["## Tagging\n","Tag Documents against context Train, Test and Valid"]},{"cell_type":"code","metadata":{"id":"D0xalbngZXhC","colab_type":"code","colab":{}},"source":["#tag each headline text with the label found in previous step\n","from gensim.models.doc2vec import TaggedDocument\n","def tag_headline(data, coltotag, colusedtotag):\n","    tagged_data = []\n","    for index, row in data.iterrows():\n","        tagged_data.append(TaggedDocument(words=get_word_tokens(row[coltotag]), tags=[row[colusedtotag]]))\n","    return tagged_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3xbvVTiZDLB","colab_type":"code","colab":{}},"source":["#tag train data and run doc2vec model\n","clickbait_labelled_headlines_train = tag_headline(train_news, processedheadlinecolname, clickbaitfeature)\n","print(clickbait_labelled_headlines_train[:3])\n","\n","#tag train data and run doc2vec model\n","clickbait_labelled_headlines_test = tag_headline(test_news, processedheadlinecolname, clickbaitfeature)\n","print(clickbait_labelled_headlines_test[:3])\n","\n","#tag train data and run doc2vec model\n","clickbait_labelled_headlines_valid = tag_headline(valid_news, processedheadlinecolname, clickbaitfeature)\n","print(clickbait_labelled_headlines_valid[:3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WzNqiz5haCtH","colab_type":"text"},"source":["## Apply Doc2Vec on Tagged Documents to created doc2vec models"]},{"cell_type":"code","metadata":{"id":"dGx_eJb7Z6q2","colab_type":"code","colab":{}},"source":["from gensim.models import Doc2Vec\n","doc2vec_model_clickbait_train = Doc2Vec(documents = clickbait_labelled_headlines_train,\n","                              dm=0, num_features=500, min_count=2, size=21, window=4)\n","doc2vec_model_clickbait_test= Doc2Vec(documents = clickbait_labelled_headlines_test,\n","                              dm=0, num_features=500, min_count=2, size=21, window=4)\n","doc2vec_model_clickbait_valid= Doc2Vec(documents = clickbait_labelled_headlines_valid,\n","                              dm=0, num_features=500, min_count=2, size=21, window=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2uRqV7wuyCa","colab_type":"text"},"source":["Add vector for every text in the dataset"]},{"cell_type":"code","metadata":{"id":"Hy23Br78uVtx","colab_type":"code","colab":{}},"source":["def create_docvector(doc2vec_model,label):\n","    return doc2vec_model.docvecs[label]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiysxKwMuZOD","colab_type":"code","colab":{}},"source":["\n","def apply_clickbait_doc2vec(dflocal,doc2vec_model, vectorcolname, label):\n","    dflocal[vectorcolname] = dflocal.apply(\n","        lambda row: create_docvector(doc2vec_model,str(row[label])), axis=1)\n","    return dflocal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KhR9BbGauqyN","colab_type":"text"},"source":["Update Dataset with vectors identified by Doc2Vec applied on context"]},{"cell_type":"code","metadata":{"id":"m5_irAc0udT8","colab_type":"code","colab":{}},"source":["\n","clickbaitdoc2veccolumn = 'clickbait_doc2vec_vector'\n","train_news = apply_clickbait_doc2vec(train_news, doc2vec_model_clickbait_train,clickbaitdoc2veccolumn,'clickbait_feature')\n","train_news.head()\n","\n","test_news = apply_clickbait_doc2vec(test_news, doc2vec_model_clickbait_test,clickbaitdoc2veccolumn,'clickbait_feature')\n","test_news.head()\n","\n","valid_news = apply_clickbait_doc2vec(valid_news, doc2vec_model_clickbait_valid,clickbaitdoc2veccolumn,'clickbait_feature')\n","valid_news.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijSZ5J35umGu","colab_type":"code","colab":{}},"source":["\n","create_docvector(doc2vec_model_clickbait_train,train_news['clickbait_feature'][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWrFGkDWu5eg","colab_type":"code","colab":{}},"source":["[train_news['headline_lda_topic_score'][0],\n","           train_news['headline_sentiment_compound'][0],\n","            create_docvector(doc2vec_model_train,train_news[contexttopicnamecol][0])]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfEXMClqIxMu","colab_type":"code","colab":{}},"source":["def getclickbaitfeaturetrainvector():\n","    sentiment = train_news.headline_sentiment_compound.reset_index()['headline_sentiment_compound']\n","    topic = train_news.headline_lda_topic_score.reset_index()['headline_lda_topic_score']\n","    clickbait_doc2vec = []\n","    for i in range(len(train_news['clickbait_feature'])):\n","        clickbait_value = train_news['clickbait_feature'][i]\n","        clickbait = doc2vec_model_clickbait_train[clickbait_value]\n","        clickbait_doc2vec.append(clickbait)\n","    \n","    clickbait_vector = pd.concat([sentiment,topic, pd.DataFrame(clickbait_doc2vec)],axis=1)\n","    return clickbait_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQtrXO90I87a","colab_type":"code","colab":{}},"source":["def getclickbaitfeaturetestvector():\n","    sentiment = test_news.headline_sentiment_compound.reset_index()['headline_sentiment_compound']\n","    topic = test_news.headline_lda_topic_score.reset_index()['headline_lda_topic_score']\n","    clickbait_doc2vec = []\n","    for i in range(len(test_news['clickbait_feature'])):\n","        clickbait_value = test_news['clickbait_feature'][i]\n","        clickbait = doc2vec_model_clickbait_test[clickbait_value]\n","        clickbait_doc2vec.append(clickbait)\n","    \n","    clickbait_vector = pd.concat([sentiment,topic, pd.DataFrame(clickbait_doc2vec)],axis=1)\n","    return clickbait_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cq7dtpVJMGx","colab_type":"code","colab":{}},"source":["clickbait_vector = getclickbaitfeaturetrainvector()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nsRzYDJ9JOpl","colab_type":"code","colab":{}},"source":["clickbait_vector_test = getclickbaitfeaturetestvector()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCBen3S6JR4y","colab_type":"code","colab":{}},"source":["#Get train and test data for classification\n","X_train = clickbait_vector\n","y_train = train_news['encoded_label']\n","\n","X_test = clickbait_vector_test\n","y_test = test_news['encoded_label']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oEcFNpaVJZdW","colab_type":"text"},"source":["## Classification using Logistic Regression"]},{"cell_type":"code","metadata":{"id":"oh4v7SXPJXDp","colab_type":"code","colab":{}},"source":["#perform vector classification using Logistic Regression\n","from sklearn.linear_model import  LogisticRegression\n","fake_news_classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n","fake_news_classifier.fit(X_train, y_train)\n","y_pred = fake_news_classifier.predict(X_test)\n","estimate_score(y_test, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qamfmCpHJi_m","colab_type":"code","colab":{}},"source":["estimate_score(y_test, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1SC8g6izJqSh","colab_type":"code","colab":{}},"source":["\n","def write_to_csv(df, vector_data, filename):\n","    feature = []\n","    for index,row in vector_data.iterrows():\n","        feature.append(row.tolist())\n","    \n","    final_feature = []\n","    for i in range(len(df)):\n","        data = {}\n","        headline = df['headline_text'][i]\n","        encoded_label = df['encoded_label'][i]\n","        clickbait_vector = feature[i]\n","        data = {'headline_text':headline, 'encoded_label':encoded_label, 'clickbait_vector':clickbait_vector}\n","        final_feature.append(data)\n","\n","    df = pd.DataFrame(final_feature)\n","    df.to_csv(filename, sep=',')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMAGWyL0JuBT","colab_type":"code","colab":{}},"source":["write_to_csv(train_news, X_train,\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train_clickbaitfeature.csv\" )\n","\n","write_to_csv(test_news, X_test,\"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test_clickbaitfeature.csv\" )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqGhjjue6iQ6","colab_type":"text"},"source":["# Factor 9 : Content Length"]},{"cell_type":"code","metadata":{"id":"TzWeDijV6s2d","colab_type":"code","colab":{}},"source":["dataAllBodyLength = train_news.copy()\n","for index, row in dataAllBodyLength.iterrows():\n","    textLength = len(row['text'])\n","    dataAllBodyLength.at[index, 'text_length'] = textLength\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.linear_model import LinearRegression\n","linearRegressionBodyLength = LinearRegression(fit_intercept=True)\n","\n","A = np.array(list(dataAllBodyLength.text_length))\n","B = np.array(list(dataAllBodyLength.veracity))\n","\n","linearRegressionBodyLength.fit(A[:, np.newaxis], B)\n","\n","xfit = np.linspace(-1, max(dataAllBodyLength.text_length), 1000)\n","yfit = linearRegressionBodyLength.predict(xfit[:, np.newaxis])\n","\n","plt.scatter(A, B, s=1, c=\"orange\")\n","plt.plot(xfit, yfit);\n","\n","print(\"Model slope:    \", linearRegressionBodyLength.coef_[0])\n","print(\"Model intercept:\", linearRegressionBodyLength.intercept_)\n","print(\"R2 score:\", linearRegressionBodyLength.score(A[:, np.newaxis], B))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Toc-_mzo6xzC","colab_type":"code","colab":{}},"source":["or index, row in dataTrain.iterrows():\n","    textLength = len(row['text'])\n","    dataTrain.at[index, 'text_length'] = textLength\n","\n","for index, row in dataTest.iterrows():\n","    textLength = len(row['text'])\n","    dataTest.at[index, 'text_length'] = textLength\n","\n","from sklearn import linear_model\n","# from sklearn import linear_model\n","\n","logClassifierBodyLength = linear_model.LogisticRegression(solver='liblinear', C=17/1000, random_state=111)\n","logClassifierBodyLength.fit(dataTrain['text_length'].values.reshape(-1, 1), dataTrain['veracity'].values)\n","\n","predicted = logClassifierBodyLength.predict(dataTest['text_length'].values.reshape(-1, 1))\n","\n","from sklearn import metrics\n","print(metrics.accuracy_score(dataTest['veracity'].values.reshape(-1, 1), predicted))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRwQP1j16zk_","colab_type":"code","colab":{}},"source":["def DATAMINERS_getBodyLengthScore(length): # return between 0 and 1, being 0 = True,  1 = Fake\n","    x = np.array(length).reshape(-1, 1)\n","    predicted = logClassifierBodyLength.predict(x)\n","    predicedProb = logClassifierBodyLength.predict_proba(x)[:,1]\n","    #return int(predicted), float(predicedProb)\n","    return 1 - float(predicedProb)\n","\n","print(DATAMINERS_getBodyLengthScore(12000))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2sFD184X4Z8C","colab_type":"text"},"source":["# Factor 10 : Word Frequency"]},{"cell_type":"code","metadata":{"id":"vhu2DtvM4sde","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem.porter import *\n","from sklearn import metrics\n","\n","class WordFrequency():\n","\n","    def __init__(self):        \n","\n","        columnNames = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n","        dataTrain = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/train.tsv', sep='\\t', header=None, names = columnNames)\n","        dataValidate = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/valid.tsv', sep='\\t', header=None, names = columnNames)\n","        dataTest = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 3/Datasets/liar_dataset/test.tsv', sep='\\t', header=None, names = columnNames)\n","        \n","        #dropping columns\n","        columnsToRemove = ['id','subject', 'speaker', 'context','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']\n","        dataTrain = dataTrain.drop(columns=columnsToRemove)\n","        dataValidate = dataValidate.drop(columns=columnsToRemove)\n","        dataTest = dataTest.drop(columns=columnsToRemove)\n","\n","        def convertMulticlassToBinaryclass(r):\n","            v = r['label']\n","            if (v == 'true'):\n","                return 'true'\n","            if (v == 'mostly-true'):\n","                return 'true'\n","            if (v == 'half-true'):\n","                return 'true'\n","            if (v == 'barely-true'):\n","                return 'false'\n","            if (v == 'false'):\n","                return 'false'\n","            if (v == 'pants-fire'):\n","                return 'false'\n","        dataTrain['label'] = dataTrain.apply(convertMulticlassToBinaryclass, axis=1)\n","        dataValidate['label'] = dataValidate.apply(convertMulticlassToBinaryclass, axis=1)\n","        dataTest['label'] = dataTest.apply(convertMulticlassToBinaryclass, axis=1)\n","        \n","\n","    \n","        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n","        train_tfidf = tfidfV.fit_transform(dataTrain['statement'].values)\n","        test_tfidf = tfidfV.fit_transform(dataTest['statement'].values)\n","\n","#         print('TF-IDF VECTORIZER')\n","\n","        ## Removing plurals for the tokens using PorterStemmer\n","        stemmer = PorterStemmer()\n","        tfidfVPlurals= tfidfV.get_feature_names()\n","        tfidfVSingles= [stemmer.stem(plural) for plural in tfidfVPlurals]\n","\n","        # Applying Set to remove duplicates\n","        tfidfVTokens = list(set(tfidfVSingles))\n","#         print('TFIDFV Tokens')\n","#         print(tfidfVTokens)\n","\n","        self.logR_pipeline = Pipeline([\n","                ('LogRCV', tfidfV),\n","                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n","                ])\n","\n","        self.logR_pipeline.fit(dataTrain['statement'],dataTrain['label'])\n","        predicted_LogR = self.logR_pipeline.predict(dataTest['statement'])\n","        score = metrics.accuracy_score(dataTest['label'], predicted_LogR)\n","        print(\"Word Frequency Model Trained - accuracy:   %0.6f\" % score)\n","        \n","\n","    def predict(self, text):\n","        predicted = self.logR_pipeline.predict([text])\n","        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n","        return bool(predicted), float(predicedProb)\n","    \n","    \n","# wf = WordFrequency()\n","# wf.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sd7y-xU15_bD","colab_type":"code","colab":{}},"source":["# from ipynb.fs.full.m_wordfrequency import WordFrequency\n","wordFrequency = WordFrequency()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8Fc_UP-6SRp","colab_type":"code","colab":{}},"source":["def DATAMINERS_getWordFrequencyScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n","    #print(clickBait.predict(\"Should You bring the money now\"))\n","    binaryValue, probValue = wordFrequency.predict(text)\n","    return (1 - float(probValue))\n","\n","print(DATAMINERS_getWordFrequencyScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"],"execution_count":0,"outputs":[]}]}