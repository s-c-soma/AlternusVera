{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "AlternusVeraNLP_TheMeanSquares_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-c-soma/AlternusVera/blob/master/TheMeanSquares_AlternusVera/Team/Final/AlternusVeraNLP_TheMeanSquares_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VmsH6itUFZy"
      },
      "source": [
        "# Project : Alternus Vera \n",
        "\n",
        "\n",
        "Course code : **CMPE-257** \n",
        "-Final Submission\n",
        "\n",
        "#**Team: The Mean Squares**\n",
        "* Jeyasri Subramanian [SJSU ID: 014510132]\n",
        "* Subarna Chowdhury Soma [SJSU ID: 014549587]\n",
        "* Pranav Lodha [SJSU ID: 009468121]\n",
        "* Wasae Qureshi [SJSU ID: 014569880]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxHexh6g8A2w"
      },
      "source": [
        "### Liar Liar Pants on Fire Dataset Description \n",
        "- It has 3 files test, training and valid.\n",
        "- Each file has 14 columns\n",
        "    \n",
        "    Column 1: the ID of the statement ([ID].json).\n",
        "    \n",
        "    Column 2: the label.\n",
        "    \n",
        "    Column 3: the statement.\n",
        "    \n",
        "    Column 4: the subject(s).\n",
        "    \n",
        "    Column 5: the speaker.\n",
        "    \n",
        "    Column 6: the speaker's job title.\n",
        "    \n",
        "    Column 7: the state info.\n",
        "    \n",
        "    Column 8: the party affiliation.\n",
        "    \n",
        "    Column 9-13: the total credit history count, including the current\n",
        "    statement.\n",
        "    \n",
        "        9: barely true counts.\n",
        "        \n",
        "        10: false counts.\n",
        "        \n",
        "        11: half true counts.\n",
        "        \n",
        "        12: mostly true counts.\n",
        "        \n",
        "        13: pants on fire counts.\n",
        "        \n",
        "    Column 14: the context (venue / location of the speech or statement).\n",
        "\n",
        "\n",
        "\n",
        "## Business Problem\n",
        "\n",
        "Fake news problem is too important to ignore especially after recent election of Donald Trump. These news are like malignant tumor that causes moral treats. It is nothing but a nassault on truth. Being impartial about the real news are same as that of a deliberate lies.\n",
        "\n",
        "The main purpose of this project is to identify essential features that can be trusted to predict if a news is fake or not. These features are realted to the news content. Our key classification is to predict if a news is fake or not based on these features. In addition, we are also intended to learn various deep learning and neural networking techniques and compare their performances.\n",
        "\n",
        "\n",
        "We initially as a team performend literature survey on list of features that has so far played major role in popularising a fake news. Each member of the team took one major feature and performed distillation process. We vectorized and came up with embedding vectors. Computed polynomial equation and classified the news articles based on all the below features. \n",
        "\n",
        "\n",
        "### Process Steps: \n",
        "- Load the Data\n",
        "- Distillation Process\n",
        "    - Data Cleaning and Text Preprocessing\n",
        "    - Visualization\n",
        "    - Sentiment Analysis  \n",
        "    - LDA Topic Modelling\n",
        "    - word2Vec Word Embedding \n",
        "    - TF-IDF Vectorization\n",
        "    - Doc2Vec using individual feature\n",
        "- Vector Classification Modeling \n",
        "- Ranking and Importance\n",
        "- Compute Polynomial Equation **[(0.67 *MicroPattern) + (0.88 *News Coverage) + (0.6 *Creditibility and Reliability) + (0.5 * Bias)]**\n",
        "- Perform Classification Prediction \n",
        "\n",
        "\n",
        "### Feature Selection\n",
        "**Top Features Selected based on research articles**\n",
        "\n",
        "1. News Coverage \n",
        "2. Neural Micro-patterns-of-Misinformation\n",
        "3. Biases \n",
        "4. Credibility-and-Reliability \n",
        "\n",
        "*Other simple features as a part of distillation:*\n",
        "6. Sentiment Analysis \n",
        "7. LDA Topic Modeling  \n",
        "\n",
        "\n",
        "### Contributions:\n",
        "\n",
        "\n",
        "|Features |  Member |\n",
        "|-----|-----|\n",
        "| News Coverage               |  Subarna Chowdhury Soma| \n",
        "| Neural Micro-patterns-of-Misinformation|  Jeyasri Subramanian |  \n",
        "| Biases                         |  Wasae Qureshi   | \n",
        "| Credibility-and-Reliability                      |  Pranav Lodha | \n",
        "| Sentiment Analysis                              |  All | \n",
        "\n",
        "As a team, we decided on the importance of the factors presented in this paper. We brainstormed on the general pre-processing techniques we did want to use. We also had common visualization methods and similar techniques for evaluating the classification model accuracy. Each of us enriched the dataset with individual features and persisted it in a csv file. Each feature vector is persisted on csv (which is distilled with LDA, sentiment scores). We also came up with a polynomial equation based on the factors and the accuracy scores we received by classification. The polynomial equation is then used to build a model for fake news classification. The **polynomial equation that we have used is [(0.67 *MicroPattern) + (0.88 *News Coverage) + (0.6 *Creditibility and Reliability) + (0.5 * Bias)]**. The final model that we built is a variation of the stack ensemble technique. Stacked generalization is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combined machine learning algorithm. The combined model is then used to predict the fakeness in the corpus.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdb6KHQb9E2-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71af2c47-60fd-4944-cc44-79fd6b53a287"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxrkA11eH8L5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "1501bfe3-c0db-416a-fe5d-fbb0a8627e75"
      },
      "source": [
        "!ls '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Model'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "biased_unbiased.pkl\n",
            "gnb_micropattern.pkl\n",
            "newscoverage_detect_fake_logistic.pkl\n",
            "newscoverage_detect_fake_rf.pkl\n",
            "newscoverage_detect_fake_svd_transformer.pkl\n",
            "newscoverage_score_computation_rf.pkl\n",
            "Rel_Cred_Feature.pkl\n",
            "svc_micropattern.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZOHobXk8A2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "91559cfe-2daa-4dd0-d2e9-f8fcba1a41e0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "## ignore warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78K1gQ7W9a4L"
      },
      "source": [
        "# Loading Liar Liar Dataset preprocessed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7NT8AXa9jlx"
      },
      "source": [
        "We did preprocess the headline text in Liar Liar dataset\n",
        "We have identifed LDA topic "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF04Yy5jDBII",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "c8a8a662-3334-47cf-d966-f02142c72aa4"
      },
      "source": [
        "! ls 'drive/Shared drives/CMPE 257: Machine Learning/AlterusVera-Datasets/liar_dataset'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed_LiarLiar.csv\t   test.tsv\t\t       valid_cleantext.csv\n",
            "README\t\t\t   train_cleantext.csv\t       valid_contextfeature.csv\n",
            "test_cleantext.csv\t   train_clickbaitfeature.csv  valid_contextlda.csv\n",
            "test_clickbaitfeature.csv  train_contextfeature.csv    valid.tsv\n",
            "test_contextfeature.csv    train_contextlda.csv\n",
            "test_contextlda.csv\t   train.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16c-SZBP8A24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "42025b2e-e3ba-4433-e562-2ea262b445ec"
      },
      "source": [
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Dataset/liar-liar processed/processed_LiarLiar.csv',low_memory=False)\n",
        "data.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline_text</th>\n",
              "      <th>processed_headline_text</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>speakerjobtitle</th>\n",
              "      <th>stateinfo</th>\n",
              "      <th>partyaffiliation</th>\n",
              "      <th>context</th>\n",
              "      <th>encoded_label</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lda_topics</th>\n",
              "      <th>negative</th>\n",
              "      <th>neutral</th>\n",
              "      <th>positive</th>\n",
              "      <th>compound</th>\n",
              "      <th>subject_codes</th>\n",
              "      <th>speaker_codes</th>\n",
              "      <th>partyaffiliation_codes</th>\n",
              "      <th>speakerjobtitle_codes</th>\n",
              "      <th>context_codes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>nanni list polit group support third trimest a...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>a mailer</td>\n",
              "      <td>5</td>\n",
              "      <td>['nanni', 'list', 'polit', 'group', 'support',...</td>\n",
              "      <td>nanni group administr trimest third support po...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>31</td>\n",
              "      <td>868</td>\n",
              "      <td>20</td>\n",
              "      <td>930</td>\n",
              "      <td>1430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>declin coal start start natur took start begin...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>a floor speech.</td>\n",
              "      <td>3</td>\n",
              "      <td>['declin', 'coal', 'start', 'start', 'natur', ...</td>\n",
              "      <td>list demand third polit administr trimest supp...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.0772</td>\n",
              "      <td>2810</td>\n",
              "      <td>2652</td>\n",
              "      <td>6</td>\n",
              "      <td>928</td>\n",
              "      <td>1257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>hillari clinton agre john mccain vote give geo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>Denver</td>\n",
              "      <td>2</td>\n",
              "      <td>['hillari', 'clinton', 'agr', 'john', 'mccain'...</td>\n",
              "      <td>trimest third polit administr support nanni li...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>3139</td>\n",
              "      <td>197</td>\n",
              "      <td>6</td>\n",
              "      <td>686</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health care reform legisl like mandat free cha...</td>\n",
              "      <td>health-care</td>\n",
              "      <td>blog-posting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>a news release</td>\n",
              "      <td>5</td>\n",
              "      <td>['health', 'care', 'reform', 'legisl', 'like',...</td>\n",
              "      <td>group trimest support administr third polit na...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.722</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>3369</td>\n",
              "      <td>290</td>\n",
              "      <td>17</td>\n",
              "      <td>-1</td>\n",
              "      <td>1529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>econom turnaround start term</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>charlie-crist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Florida</td>\n",
              "      <td>democrat</td>\n",
              "      <td>an interview on CNN</td>\n",
              "      <td>3</td>\n",
              "      <td>['econom', 'turnaround', 'start', 'term']</td>\n",
              "      <td>demand third administr trimest support polit n...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>2392</td>\n",
              "      <td>459</td>\n",
              "      <td>6</td>\n",
              "      <td>-1</td>\n",
              "      <td>3141</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       headline_text  ... context_codes\n",
              "0  Says the Annies List political group supports ...  ...          1430\n",
              "1  When did the decline of coal start? It started...  ...          1257\n",
              "2  Hillary Clinton agrees with John McCain \"by vo...  ...            87\n",
              "3  Health care reform legislation is likely to ma...  ...          1529\n",
              "4  The economic turnaround started at the end of ...  ...          3141\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxf4TtNk8A29"
      },
      "source": [
        "# News Coverage ( By: Subarna )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WZFpbEEE19U"
      },
      "source": [
        "News coverage means an important factor for fake news detection. Definition wise this factor investigates coverage of the same story in different media outlets. To detect 'News Coverage' we have to measure and compare news coverage of the same story published in different news sources within the same timeframe. For this we have used Kaggle all news data sets. There is no label in this data set to group articles with the same story. So, we have clustered the data into the same group based on their story. To clarify some terms: \n",
        "\n",
        " ARTICLE - a single article printed by one news publication \n",
        "\n",
        " STORY - the underlying event that an article is in reference to \n",
        "\n",
        " For details factor computation, refer to colab: https://colab.research.google.com/drive/1gFKq-swxsVMXQn2adAB5y2v7ldiwBxaM?authuser=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-0MLzfpWqy7"
      },
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from string import punctuation\n",
        "from nltk import PorterStemmer\n",
        "import copy\n",
        "import pickle as pkl \n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import LabelEncoder,  MaxAbsScaler #Imputer,\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "#nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ0mufTvVfv0"
      },
      "source": [
        "location = '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Model'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMCBXp6k7fCF"
      },
      "source": [
        "def tokenize2(text):\n",
        "    cachedStopWords = set(stopwords.words('english') + list(punctuation))\n",
        "    min_length = 3\n",
        "    # tokenize\n",
        "    # convert to lower case\n",
        "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
        "    # remove stop words\n",
        "    words = [word for word in words if word not in cachedStopWords]\n",
        "    # steming\n",
        "    #tokens = list(map(lambda token: PorterStemmer().stem(token), words))\n",
        "    # lemmatize\n",
        "    lemmas = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
        "    # only focus on alphabetic words\n",
        "    p = re.compile('[a-zA-Z]+')\n",
        "    \n",
        "    filtered_lemmas = list(filter(lambda lemma: p.match(lemma) and len(lemma) >= min_length, lemmas))\n",
        "    return filtered_lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMWiM3kv4_sc"
      },
      "source": [
        "def encode_news_label(input_label):\n",
        "    if input_label == 'true':\n",
        "        return 1\n",
        "    elif input_label == 'mostly-true':\n",
        "        return 2\n",
        "    elif input_label == 'half-true':\n",
        "        return 3\n",
        "    elif input_label == 'barely-true':\n",
        "        return 4\n",
        "    elif input_label == 'false':\n",
        "        return 5\n",
        "    elif input_label == 'pants-on-fire':\n",
        "        return 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4_DECLsEai7"
      },
      "source": [
        "# To be filled by Subarna\n",
        "def TheMeanSquares_NewsCoverage(text):\n",
        "  # load the model from disk\n",
        "  filename = location+'/newscoverage_detect_fake_svd_transformer.pkl'\n",
        "  svd_transformer_model = pkl.load(open(filename, 'rb'))\n",
        "\n",
        "  filename = location+'/newscoverage_detect_fake_logistic.pkl'\n",
        "  logistic_model = pkl.load(open(filename, 'rb'))\n",
        "\n",
        "  data = np.array([\"Had President Donald Trump been successful in launching prosecutions against Hillary Clinton and James Comey, it could have spelled the end of his presidency, as a clear-cut abuse of power.\\\n",
        "  It never happened, apparently thwarted by then-White House Counsel Don McGahn and other senior officials. But that does not mean this is a crisis dodged for Trump and he is now free from fresh legal and political jeopardy. Quite the reverse.\\\n",
        "  RELATED: Trump raised prosecuting Clinton with top White House, Justice officials\\\n",
        "  At the very least, the latest developments underline how Trump's senior subordinates may have shielded a President unschooled in constitutional norms from disastrous steps that could have put his presidency in peril.\\\n",
        "  And it leaves anyone on the outside wondering what other potential disasters top officials like McGahn, former Attorney General Jeff Sessions and current Deputy Attorney General Rod Rosenstein might have prevented.\\\n",
        "  They also raise questions about the capacity of a now-understaffed White House and legal counsel's operation to protect the President from current or future transgressions.\\\n",
        "  It will be impossible to confirm, given the habitual silence from the special counsel's office, but the revelations hint at the possibility that Robert Mueller knows much more about what went on in the corridors of the West Wing than has been publicly revealed.\\\n",
        "  That will play into rising tensions in Washington amid expectations that the endgame of Mueller's probe is in sight and speculation about possible indictments targeting Trump world and the content of his final report.\\\n",
        "  Bombshell reports by CNN and The New York Times about the President's intentions emerged on another surreal day in Washington that saw shocking disclosures about Ivanka Trump's emails and a huge foreign policy pivot over Saudi Arabia.\"])\n",
        "  data = np.array([text])\n",
        "  s = pd.Series(data)\n",
        "\n",
        "  vectorised_test_documents = svd_transformer_model.transform(s)\n",
        "  y_pred=logistic_model.predict(vectorised_test_documents)\n",
        "  print(y_pred)\n",
        "  if y_pred == 'non-bs':\n",
        "      return [1,0,0,0,0,0]\n",
        "  elif y_pred == 'bs':\n",
        "      return [0,0,0,0,0,0]\n",
        "  elif y_pred == 'true':\n",
        "      return [0,1,0,0,0,0]\n",
        "  elif y_pred == 'mostly-true':\n",
        "      return [0,0,1,0,0,0]\n",
        "  elif y_pred == 'half-true':\n",
        "      return [0,0,0,1,0,0]\n",
        "  elif y_pred == 'barely-true':\n",
        "      return [0,0,0,0,1,0]\n",
        "  elif y_pred == 'false':\n",
        "      return [0,0,0,0,0,1]\n",
        "  elif y_pred == 'pants-fire':\n",
        "      return [0,0,1,0,0,0]\n",
        "  else:\n",
        "      return [0,0,1,0,0,0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-LznnQ77Lkf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5d9d998a-e102-45de-f8db-c93b3cc53256"
      },
      "source": [
        "TheMeanSquares_NewsCoverage(\"Says the Annies List political group supports\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['non-bs']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjF3WJdJ8enM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "be4259d2-c604-4a5e-d763-f2ee82eb44c7"
      },
      "source": [
        "TheMeanSquares_NewsCoverage(\"Had President Donald Trump been successful in launching prosecutions against Hillary Clinton and James Comey, it could have spelled the end of his presidency, as a clear-cut abuse of power.\\\n",
        "  It never happened, apparently thwarted by then-White House Counsel Don McGahn and other senior officials. But that does not mean this is a crisis dodged for Trump and he is now free from fresh legal and political jeopardy. Quite the reverse.\\\n",
        "  RELATED: Trump raised prosecuting Clinton with top White House, Justice officials\\\n",
        "  At the very least, the latest developments underline how Trump's senior subordinates may have shielded a President unschooled in constitutional norms from disastrous steps that could have put his presidency in peril.\\\n",
        "  And it leaves anyone on the outside wondering what other potential disasters top officials like McGahn, former Attorney General Jeff Sessions and current Deputy Attorney General Rod Rosenstein might have prevented.\\\n",
        "  They also raise questions about the capacity of a now-understaffed White House and legal counsel's operation to protect the President from current or future transgressions.\\\n",
        "  It will be impossible to confirm, given the habitual silence from the special counsel's office, but the revelations hint at the possibility that Robert Mueller knows much more about what went on in the corridors of the West Wing than has been publicly revealed.\\\n",
        "  That will play into rising tensions in Washington amid expectations that the endgame of Mueller's probe is in sight and speculation about possible indictments targeting Trump world and the content of his final report.\\\n",
        "  Bombshell reports by CNN and The New York Times about the President's intentions emerged on another surreal day in Washington that saw shocking disclosures about Ivanka Trump's emails and a huge foreign policy pivot over Saudi Arabia.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['non-bs']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG6qA-vL8A3Q"
      },
      "source": [
        "# **Neural Micro-patterns-of-Misinformation (by: Jeyasri)**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCI6Z2FgXkFM"
      },
      "source": [
        "MicroPatterns of misinformation refers to micro-information present in news article which convey the anomaly for fake data. \n",
        "\n",
        "This problem is address by LDA topic modeling, Doc2Vec (AutoEncoder) and sentiment analysis vectors from ISOT dataset. The exploratory analysis is done with t-SNE, LDA-frequent topic from real and fake news.\n",
        "The performance of the model is 0.67 for binary classification and 0.18 for Multi-class classification of LiarLiar Dataset labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buFYdzN3P8DP"
      },
      "source": [
        "def encode_news_type(input_label):\n",
        "    if input_label == 'true':\n",
        "        return 1\n",
        "    elif input_label == 'mostly-true':\n",
        "        return 2\n",
        "    elif input_label == 'half-true':\n",
        "        return 3\n",
        "    elif input_label == 'barely-true':\n",
        "        return 4\n",
        "    elif input_label == 'false':\n",
        "        return 5\n",
        "    elif input_label == 'pants-on-fire':\n",
        "        return 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW3rADi6Oe_D"
      },
      "source": [
        "def encode_labelType(input_label):\n",
        "    if input_label == 1:\n",
        "        return 'true'\n",
        "    elif input_label == 2:\n",
        "        return 'mostly-true'\n",
        "    elif input_label == 3:\n",
        "        return 'half-true'\n",
        "    elif input_label == 4:\n",
        "        return 'barely-true'\n",
        "    elif input_label == 5:\n",
        "        return 'false'\n",
        "    elif input_label == 6:\n",
        "        return  'pants-on-fire'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFIompT2P785"
      },
      "source": [
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn import utils as skl_utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "\n",
        "class Doc2VecTransformer(BaseEstimator):\n",
        "\n",
        "    def __init__(self, vector_size=100, learning_rate=0.02, epochs=20,column='clean_text'):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self._model = None\n",
        "        self.vector_size = vector_size\n",
        "        self.workers = multiprocessing.cpu_count() - 1\n",
        "        self.column = column\n",
        "\n",
        "    def fit(self, df_x, df_y=None):\n",
        "        tagged_x = [TaggedDocument(str(row[self.column]).split(), [index]) for index, row in df_x.iterrows()]\n",
        "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            model.train(skl_utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
        "            model.alpha -= self.learning_rate\n",
        "            model.min_alpha = model.alpha\n",
        "\n",
        "        self._model = model\n",
        "        return self\n",
        "\n",
        "    def transform(self, df_x):\n",
        "        return np.asmatrix(np.array([self._model.infer_vector(str(row[self.column]).split())\n",
        "                                     for index, row in df_x.iterrows()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h61G6i0P7z0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "475f2e85-f0ac-4120-b324-023422b3ce07"
      },
      "source": [
        "#doc2vec_tr = Doc2VecTransformer(vector_size=300)\n",
        "doc2vec_tr = Doc2VecTransformer(vector_size=10, learning_rate=0.001, epochs=10,column='processed_headline_text')\n",
        "doc2vec_tr.fit(data)\n",
        "doc2vec_vectors = doc2vec_tr.transform(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11507/11507 [00:00<00:00, 1180045.38it/s]\n",
            "100%|██████████| 11507/11507 [00:00<00:00, 2776657.24it/s]\n",
            "100%|██████████| 11507/11507 [00:00<00:00, 2839718.53it/s]\n",
            "100%|██████████| 11507/11507 [00:00<00:00, 2756674.44it/s]\n",
            "100%|██████████| 11507/11507 [00:00<00:00, 2709473.76it/s]\n",
            "100%|██████████| 11507/11507 [00:00<00:00, 1518925.45it/s]\n",
            "100%|██████████| 11507/11507 [00:00<00:00, 1610674.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dAL5dQhQJ0C"
      },
      "source": [
        "#doc2vec_tr = Doc2VecTransformer(vector_size=300)\n",
        "doc2vector2 = Doc2VecTransformer(vector_size=10, learning_rate=0.001, epochs=10,column='lda_topics')\n",
        "doc2vector2.fit(data)\n",
        "doc2vec_vectors2 = doc2vector2.transform(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9OxuElBQRlM"
      },
      "source": [
        "sentiment_values = data[['compound','negative','neutral','positive']].values\n",
        "doc2vec_vectors3 = np.concatenate([doc2vec_vectors, doc2vec_vectors2,sentiment_values],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LnGA-p4QZB6"
      },
      "source": [
        "data['binary_label'] = data['encoded_label'].apply(lambda x: 1 if (int(x) ==5 or int(x) == 6)  else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytbVrN3hQhpb"
      },
      "source": [
        "labels = data['binary_label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEmkeadUQnZp"
      },
      "source": [
        "!ls 'drive/Shared drives/CMPE 257: Machine Learning/AlternusVera-Model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f--WFmwUsRA"
      },
      "source": [
        "loc = '/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 4_Final Submission/Model'\n",
        "doc2vec_vectors3[20].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDkaZLeFalKu"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "import pickle as pkl\n",
        "\n",
        "def TheMeanSquares_MicroPattern(data, index):\n",
        "  \n",
        "    #model = GaussianNB(priors=None)\n",
        "    filepath = loc+'/svc_micropattern.pkl'\n",
        "    with open(filepath, \"rb\") as f:\n",
        "      model = pkl.load(f)\n",
        "     \n",
        "      labels_pred = model.predict(doc2vec_vectors3[index])\n",
        "      \n",
        "      \n",
        "      prob_vectors = model.predict_proba(doc2vec_vectors3[index])\n",
        "      \n",
        "      \n",
        "      return prob_vectors[0][0],prob_vectors[0][1],prob_vectors[0][2],prob_vectors[0][3],prob_vectors[0][4],prob_vectors[0][5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4Ru_C86ElYm"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "import pickle as pkl\n",
        "\n",
        "def TheMeanSquares_MicroPatternBinary(data, index):\n",
        "  \n",
        "    #model = GaussianNB(priors=None)\n",
        "    filepath = loc+'/gnb_micropattern.pkl'\n",
        "    with open(filepath, \"rb\") as f:\n",
        "      model = pkl.load(f)\n",
        "    \n",
        "      labels_pred = model.predict(doc2vec_vectors3[index])\n",
        "      \n",
        "      prob_vectors = model.predict_proba(doc2vec_vectors3[index])\n",
        "      \n",
        "      for i in range(prob_vectors.shape[0]):\n",
        "        true_prob = prob_vectors[i][0]\n",
        "        false_prob = prob_vectors[i][1]\n",
        "        \n",
        "        if(true_prob >= 0.89):\n",
        "            label = encode_news_type('true');\n",
        "            return [1,0,0,0,0,0]\n",
        "        elif(true_prob < 0.89 and true_prob > 0.75):\n",
        "            label = encode_news_type('mostly-true');\n",
        "            return [0,1,0,0,0,0]\n",
        "        elif(true_prob <= 0.75 and true_prob >= 0.55):\n",
        "            label = encode_news_type('half-true');\n",
        "            return [0,0,1,0,0,0]\n",
        "        elif(true_prob <= 0.6 and true_prob >= 0.5):\n",
        "            label = encode_news_type('barely-true');\n",
        "            return [0,0,0,1,0,0]\n",
        "        elif(false_prob >= 0.9):\n",
        "            label = encode_news_type('pants-on-fire');\n",
        "            return [0,0,0,0,0,1]\n",
        "        elif(false_prob < 0.9 and false_prob > 0.75):\n",
        "            label = encode_news_type('false'); \n",
        "            return [0,0,0,0,1,0]\n",
        "        else:\n",
        "            label = encode_news_type('false'); \n",
        "            return [0,0,0,0,1,0]   \n",
        "  \n",
        "\n",
        "      return [0,0,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gApBzk8rTFJz"
      },
      "source": [
        "TheMeanSquares_MicroPatternBinary(data, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIYWKFUnax54"
      },
      "source": [
        "TheMeanSquares_MicroPattern(data,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tQNZ3r4D0m_"
      },
      "source": [
        "# Bias (by Wasae Qureshi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkeU-ekQSPTB"
      },
      "source": [
        "# This is creating a Model based on the biased/unbiased datasets we have. Very poor results since the previous steps were not successful\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "class BiasScoreFeature():\n",
        "    def setup(self): \n",
        "        #load the dataset\n",
        "        unbiased_news = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/unbiased.csv', low_memory =False)\n",
        "        biased_news = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/biased.csv' , low_memory =False)\n",
        "        combined_news_features = pd.concat([unbiased_news, biased_news], ignore_index=True)\n",
        "        combined_news_label = combined_news_features['label']\n",
        "        combined_news_features.drop('label', axis=1)\n",
        "       \n",
        "        X_train, X_test, y_train, y_test = train_test_split(combined_news_features, combined_news_label, test_size=0.2)\n",
        "\n",
        "        countVectorizerHeadlineText = CountVectorizer()\n",
        "        countVectorizerHeadlineText.fit_transform(combined_news_features['content'])\n",
        "\n",
        "        self.logR_pipeline = Pipeline([\n",
        "            ('NBCV',countVectorizerHeadlineText),\n",
        "            ('nb_clf',MultinomialNB())])\n",
        "\n",
        "        self.logR_pipeline.fit(X_train['content'], y_train)\n",
        "        predicted_LogR = self.logR_pipeline.predict(X_test['content'])\n",
        "        score = metrics.accuracy_score(y_test, predicted_LogR)\n",
        "        # print(\"Bias Score Model Trained - accuracy:   %0.6f\" % score)\n",
        "\n",
        "    def predict(self, text):\n",
        "        predicted = self.logR_pipeline.predict([text])\n",
        "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
        "        return predicted[0], float(predicedProb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Rd4MUBo0Fg"
      },
      "source": [
        "This was done to have some sort of result for our model so that our group could run all our factors together to make a prediction. There won't be much weight to bias since we are just using the key words from biased and unbiased articles from my earlier research to distinguish between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R192TUeD4Bh"
      },
      "source": [
        "def TheMeanSquares_Bias(statement):\n",
        "  # load the model from disk\n",
        "  filename = \"/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Models/biased_unbiased.sav\"\n",
        "  bias_model = pkl.load(open(filename, 'rb'))\n",
        "  bias_model.setup()\n",
        "  y_pred_label, y_pred_prob = bias_model.predict(statement)\n",
        "\n",
        "  if y_pred_label == 'unbiased':\n",
        "      return [1,0,0,0,0,0]\n",
        "  elif y_pred_label == 'bias':\n",
        "      return [0,0,0,0,0,1]\n",
        "  else:\n",
        "      return [0,0,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKed5zahN49E"
      },
      "source": [
        "TheMeanSquares_Bias(\"Had President Donald Trump been successful in launching prosecutions against Hillary Clinton and James Comey, it could have spelled the end of his presidency, as a clear-cut abuse of power.\\\n",
        "  It never happened, apparently thwarted by then-White House Counsel Don McGahn and other senior officials. But that does not mean this is a crisis dodged for Trump and he is now free from fresh legal and political jeopardy. Quite the reverse.\\\n",
        "  RELATED: Trump raised prosecuting Clinton with top White House, Justice officials\\\n",
        "  At the very least, the latest developments underline how Trump's senior subordinates may have shielded a President unschooled in constitutional norms from disastrous steps that could have put his presidency in peril.\\\n",
        "  And it leaves anyone on the outside wondering what other potential disasters top officials like McGahn, former Attorney General Jeff Sessions and current Deputy Attorney General Rod Rosenstein might have prevented.\\\n",
        "  They also raise questions about the capacity of a now-understaffed White House and legal counsel's operation to protect the President from current or future transgressions.\\\n",
        "  It will be impossible to confirm, given the habitual silence from the special counsel's office, but the revelations hint at the possibility that Robert Mueller knows much more about what went on in the corridors of the West Wing than has been publicly revealed.\\\n",
        "  That will play into rising tensions in Washington amid expectations that the endgame of Mueller's probe is in sight and speculation about possible indictments targeting Trump world and the content of his final report.\\\n",
        "  Bombshell reports by CNN and The New York Times about the President's intentions emerged on another surreal day in Washington that saw shocking disclosures about Ivanka Trump's emails and a huge foreign policy pivot over Saudi Arabia.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY_nVzn5D5BA"
      },
      "source": [
        "# Credibility and Reliability (by : Pranav Lodha)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quImnrOBfRs0"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "class Rel_Cred_Feature():\n",
        "    def setup(self): \n",
        "        #load the dataset\n",
        "        columnNames = [\"type\"]\n",
        "        website_data = pd.read_csv('/content/drive/My Drive/MLSpring2020/TheMeanSquares-StockPrediction/Alternus-Vera TheMeanSquares/Iteration 1/Datasets/website.csv', sep=',')\n",
        "        y = website_data['type']\n",
        "        website_data = website_data.drop('type', axis=1)\n",
        "        website_data = website_data.drop('domain', axis=1)\n",
        "       \n",
        "        X_train, X_test, y_train, y_test = train_test_split(website_data, y, test_size=0.2)\n",
        "\n",
        "        #countVectorizerHeadlineText = CountVectorizer()\n",
        "        #countVectorizerHeadlineText.fit_transform(website_data['comment'])\n",
        "\n",
        "        self.logR_pipeline = MultinomialNB()\n",
        "\n",
        "        self.logR_pipeline.fit(X_train, y_train)\n",
        "        predicted_LogR = self.logR_pipeline.predict(X_test)\n",
        "        score = metrics.accuracy_score(y_test, predicted_LogR)\n",
        "        # print(\"Bias Score Model Trained - accuracy:   %0.6f\" % score)\n",
        "\n",
        "    def predict(self, domain):\n",
        "        API_ENDPOINT = \"https://openpagerank.com/api/v1.0/getPageRank?domains[]=\" + domain.strip()\n",
        "        API_KEY = \"sw0gsosokcwk0go8cgsokoowgk8gcw8gs4ckkswk\"\n",
        "        headers = {'API-OPR':API_KEY}\n",
        "        response = requests.get(url = API_ENDPOINT, headers= headers)\n",
        "        data = json.loads(response.text)\n",
        "        text = data['response'][0]['page_rank_decimal']\n",
        "        text = pd.DataFrame([text], columns=['page_rank_decimal'])\n",
        "        predicted = self.logR_pipeline.predict(text)\n",
        "        predicedProb = self.logR_pipeline.predict_proba(text)[:,1]\n",
        "        return predicted[0], float(predicedProb)\n",
        "    \n",
        "relcred = Rel_Cred_Feature()\n",
        "relcred.setup()\n",
        "relcred.predict(\"www.google.com\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz0c7ODAf6dM"
      },
      "source": [
        "def TheMeanSquares_Credibility(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    #print(clickBait.predict(\"Should You bring the money now\"))\n",
        "    binaryValue, probValue = relcred.predict(text)\n",
        "    if probValue >= 0.5:\n",
        "      return [1,0,0,0,0,0]\n",
        "    else:\n",
        "      return[0,1,0,0,0,0]\n",
        "\n",
        "print(TheMeanSquares_Credibility(\"cnn.com\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GTpFvSL8A34"
      },
      "source": [
        "# Get polynomial weighted vectors for the fake news classifier\n",
        "\n",
        "Equation: (0.88 * New Coverage) + (0.67 * Micro Pattern) + (0.5* Bias Feature) + (0.6*Credibility & Reliability)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz74Lb5bEVnO"
      },
      "source": [
        "def detectFakeNews(statement,source, index=100,show_log=False):\n",
        "  accur = [0.88,0.67,0.5,0.6] # using the (normalized) accuracy as weigths\n",
        "  w = [float(i)/sum(accur) for i in accur]\n",
        "  sumW = 0\n",
        "  prob = []\n",
        "  if  (statement != \"\") :\n",
        "    a1,b1,c1,d1,e1,f1 =  TheMeanSquares_NewsCoverage(statement)\n",
        "\n",
        "  if (index >= 0):\n",
        "    a2,b2,c2,d2,e2,f2 = TheMeanSquares_MicroPatternBinary(data, index=index) \n",
        "    \n",
        "    \n",
        "  if (statement!=''):\n",
        "    a3,b3,c3,d3,e3,f3 =  TheMeanSquares_Bias(statement)\n",
        "    \n",
        "  if (source != \"\"):\n",
        "    a4,b4,c4,d4,e4,f4 =  TheMeanSquares_Credibility(source)\n",
        "\n",
        "  if show_log:\n",
        "    print('predictions from news coverage : ',a1,b1,c2,d1,e1,f1)\n",
        "    print('predictions from micro pattern : ',a2,b2,c2,d2,e2,f2)\n",
        "    print('predictions from Bias : ', a3,b3,c3,d3,e3,f3)\n",
        "    print('predictions from Credibility : ',a4,b4,c4,d4,e4,f4)\n",
        "\n",
        "  A1=a1*w[0]+a2*w[1]+a3*w[2]+a4*w[3] /sum(accur)\n",
        "  A2=b1*w[0]+b2*w[1]+b3*w[2]+b4*w[3] /sum(accur)\n",
        "  A3=c1*w[0]+c2*w[1]+c3*w[2]+c4*w[3] /sum(accur)\n",
        "  A4=d1*w[0]+d2*w[1]+d3*w[2]+d4*w[3] /sum(accur)\n",
        "  A5=e1*w[0]+e2*w[1]+e3*w[2]+e4*w[3] /sum(accur)\n",
        "  A6=f1*w[0]+f2*w[1]+f3*w[2]+f4*w[3] /sum(accur)\n",
        "\n",
        "  prob_list = [A1,A2,A3,A4,A5,A6]\n",
        "  prob_max = max(prob_list)\n",
        " \n",
        "  label = prob_list.index(prob_max)\n",
        "  result = label+1\n",
        "  return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lxietAHKtbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cd29684-09eb-4816-ab35-ddf9e5253604"
      },
      "source": [
        "data.loc[24]['processed_headline_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mitt romney want plan parenthood'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r9mb4eAUi4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "514b0aee-df32-4ef6-eeb9-cf0c9de76a16"
      },
      "source": [
        "df_train, df_test = train_test_split(data, test_size=0.1, random_state=0)\n",
        "print(df_test.shape)\n",
        "actual_labels = df_test['encoded_label'].values\n",
        "predicted_labels = []\n",
        "for i in range(df_test.shape[0]):\n",
        "  result = detectFakeNews(data.loc[i]['processed_headline_text'], \"cnn.com\", index= i)\n",
        "  predicted_labels.append(result)\n",
        "  if i % 50 == 0: print('completed ',i,' of ',df_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1151, 21)\n",
            "completed  0  of  1151\n",
            "completed  50  of  1151\n",
            "completed  100  of  1151\n",
            "completed  150  of  1151\n",
            "completed  200  of  1151\n",
            "completed  250  of  1151\n",
            "completed  300  of  1151\n",
            "completed  350  of  1151\n",
            "completed  400  of  1151\n",
            "completed  450  of  1151\n",
            "completed  500  of  1151\n",
            "completed  550  of  1151\n",
            "completed  600  of  1151\n",
            "completed  650  of  1151\n",
            "completed  700  of  1151\n",
            "completed  750  of  1151\n",
            "completed  800  of  1151\n",
            "completed  850  of  1151\n",
            "completed  900  of  1151\n",
            "completed  950  of  1151\n",
            "completed  1000  of  1151\n",
            "completed  1050  of  1151\n",
            "completed  1100  of  1151\n",
            "completed  1150  of  1151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCIihXC-VjuA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "96b6f60d-13e4-48a6-da96-320355978e96"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "target_names = ['true','mostly-true','half-true','barely-true','false','pants-on-fire']\n",
        "print(classification_report(y_true=actual_labels, y_pred=predicted_labels,target_names = target_names, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "         true      0.149     0.994     0.260       172\n",
            "  mostly-true      0.000     0.000     0.000       223\n",
            "    half-true      0.000     0.000     0.000       254\n",
            "  barely-true      0.000     0.000     0.000       180\n",
            "        false      0.000     0.000     0.000       230\n",
            "pants-on-fire      0.000     0.000     0.000        92\n",
            "\n",
            "     accuracy                          0.149      1151\n",
            "    macro avg      0.025     0.166     0.043      1151\n",
            " weighted avg      0.022     0.149     0.039      1151\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRfOLjkKl089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f54fa37f-7315-4cf6-f13d-fca892bd0ad5"
      },
      "source": [
        "print(\"Test Accuracy: {}\".format(accuracy_score(predicted_labels,actual_labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.14856646394439618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S62irXXEPYvo"
      },
      "source": [
        "# Some sample prediction results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht9lv7GDMlgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0d950572-20c9-4301-8dcd-acddda124675"
      },
      "source": [
        "result = detectFakeNews(data.loc[14]['processed_headline_text'], \"cnn.com\", index= 14,show_log=True)\n",
        "print('actual label: ',encode_labelType(data.loc[14]['encoded_label']))\n",
        "print('Predicted Label Result : ',encode_labelType(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions from news coverage :  0 0 1 0 0 0\n",
            "predictions from micro pattern :  0 0 1 0 0 0\n",
            "predictions from Bias :  1 0 0 0 0 0\n",
            "predictions from Credibility :  1 0 0 0 0 0\n",
            "actual label:  barely-true\n",
            "Predicted Label Result :  true\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xctc1IiUEVky",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "529bae3f-7b6e-4d4d-cd5a-0dd7d117c939"
      },
      "source": [
        "result = detectFakeNews(data.loc[21]['processed_headline_text'], \"cnn.com\", index= 21,show_log=True)\n",
        "print('actual label: ',encode_labelType(data.loc[21]['encoded_label']))\n",
        "print('Predicted Label Result : ',encode_labelType(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions from news coverage :  0 0 1 0 0 0\n",
            "predictions from micro pattern :  0 0 1 0 0 0\n",
            "predictions from Bias :  0 0 0 0 0 1\n",
            "predictions from Credibility :  1 0 0 0 0 0\n",
            "actual label:  mostly-true\n",
            "Predicted Label Result :  true\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}